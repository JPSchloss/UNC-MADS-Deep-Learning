{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02cfcc4c",
   "metadata": {},
   "source": [
    "# Encoder-Only Models\n",
    "\n",
    "Encoder-only architectures are a subset of transformer models designed solely for understanding or encoding input data without generating new text. These models are used when the goal is to analyze or classify text rather than generate responses.\n",
    "\n",
    "### Common Applications: \n",
    "  - **Text Classification:**  \n",
    "    Ideal for tasks like sentiment analysis, topic categorization, and spam detection, where the model assigns a label or score to the input text.\n",
    "  - **Sequence Labeling:**  \n",
    "    Useful for tasks where each token in the text must be labeled (e.g., Named Entity Recognition, Part-of-Speech Tagging). The model outputs a label for every token based on its contextualized representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d97c17",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/v2/resize:fit:680/1*fNQ9_cJ0Jo78U01nYz-Hcg.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4096a1fb",
   "metadata": {},
   "source": [
    "## How They Work\n",
    "\n",
    "### Transformers as Feature Extractors\n",
    "\n",
    "#### **Tokenization and Embedding:**  \n",
    "  Input text is first tokenized into individual tokens (words or subwords). Each token is then mapped to a dense vector using an embedding layer. These embeddings serve as the starting point, capturing basic semantic information about the tokens.\n",
    "\n",
    "#### **Stack of Transformer Encoder Layers:**  \n",
    "  The embedded tokens are passed through a series of transformer encoder layers. Each layer consists of several key components:\n",
    "  - **Self-Attention Mechanism:**  \n",
    "    Every token in the sequence is allowed to attend to every other token. This mechanism computes attention scores that determine how much influence each token should have on the others, thereby integrating context from the entire sequence.\n",
    "  - **Feed-Forward Networks:**  \n",
    "    After the self-attention step, the output is processed by a fully-connected feed-forward network. This introduces non-linear transformations and helps in learning higher-level representations.\n",
    "  - **Normalization and Residual Connections:**  \n",
    "    Layer normalization and residual connections are applied after both the self-attention and feed-forward steps. These techniques help stabilize training, maintain information flow, and prevent the vanishing gradient problem.\n",
    "\n",
    "#### **Contextualized Representations:**  \n",
    "  As tokens pass through successive encoder layers, their representations become highly contextualized. This means each token’s final high-dimensional embedding encodes not only its own meaning but also its relationships and dependencies with every other token in the sequence.\n",
    "\n",
    "### Classification via the CLS Token\n",
    "\n",
    "#### **Introducing the CLS Token:**  \n",
    "  For tasks that require a single output for the entire sequence (such as text classification), a special token known as the **CLS token** is inserted at the beginning of the input sequence. This token does not represent actual content but serves as a placeholder for aggregated information.\n",
    "\n",
    "#### **Aggregating Information:**  \n",
    "  As the sequence flows through the transformer layers, the CLS token interacts with all other tokens via self-attention. Its final hidden state effectively becomes a summary representation of the entire input sequence.\n",
    "\n",
    "#### **Classifier Head:**  \n",
    "  The summary representation captured by the CLS token is then fed into a classifier—often a simple feed-forward network or a couple of layers followed by a softmax activation—to produce the final prediction, such as a sentiment label or topic category.\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:1400/0*O4yM2Kis_k2S5M40.png)\n",
    "\n",
    "### Sequence Labeling\n",
    "\n",
    "#### **Token-Level Predictions:**  \n",
    "  For tasks requiring predictions for each token (like Named Entity Recognition or Part-of-Speech Tagging), the model uses the final hidden state of each token instead of just the CLS token. Each token’s contextualized embedding is passed through a classification layer to predict labels at the token level.\n",
    "\n",
    "#### **Enhancing Predictions with Structured Techniques:**  \n",
    "  In some sequence labeling tasks, additional structured prediction layers such as Conditional Random Fields (CRFs) are applied on top of the token-level outputs. This helps capture the dependencies between labels, ensuring that the sequence of predictions is coherent and contextually valid.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bc9dce",
   "metadata": {},
   "source": [
    "# BERT (Bidirectional Encoder Representation Transformer)\n",
    "\n",
    "BERT is one of the most well-known encoder-only transformer models. It leverages a deep bidirectional architecture to generate powerful language representations. \n",
    "\n",
    "![](BERT.png)\n",
    "\n",
    "\n",
    "## BERT Architecture\n",
    "\n",
    "- **Encoder-Only Design:**  \n",
    "  BERT is built entirely using transformer encoder layers. Unlike models that include both an encoder and a decoder (e.g., GPT), BERT is solely focused on understanding input text. This specialization makes it highly effective for tasks like text classification, sentiment analysis, and question answering, where the goal is to derive meaning from the input rather than generate new text.\n",
    "\n",
    "#### **Bidirectionality:**  \n",
    "  A hallmark of BERT is its bidirectional self-attention mechanism.  \n",
    "  - **How It Works:**  \n",
    "    During training, every token in the input sequence attends to all other tokens simultaneously, both to its left and right. This allows the model to capture context from both directions.  \n",
    "  - **Significance:**  \n",
    "    This bidirectional approach enables BERT to develop a richer understanding of language, as it can capture nuances that arise from the full context of a sentence. It contrasts with traditional language models that only process text in one direction (left-to-right or right-to-left).\n",
    "\n",
    "#### **Input Representations:**  \n",
    "  BERT creates a comprehensive input representation by summing three types of embeddings:\n",
    "  - **Token Embeddings:**  \n",
    "    Each token (word or subword) is mapped to a dense vector that encodes its basic semantic properties.\n",
    "  - **Position Embeddings:**  \n",
    "    Since transformers lack an inherent sense of order, position embeddings are added to each token to encode its position in the sequence, ensuring that the order of words is preserved.\n",
    "  - **Segment Embeddings:**  \n",
    "    When processing tasks that involve pairs of sentences (e.g., next sentence prediction), segment embeddings are used to distinguish tokens belonging to different sentences. This helps the model understand the relationship between segments within the same input.\n",
    "\n",
    "\n",
    "## BERT Pre-Training\n",
    "\n",
    "BERT is first pre-trained on large volumes of unlabeled text which enables it to learn robust and generalizable language representations. There are two common approaches taken here:\n",
    "\n",
    "#### **Masked Language Modeling (MLM):**  \n",
    "  - **Concept:**  \n",
    "    A percentage (typically about 15%) of the input tokens is randomly selected and replaced with a special `[MASK]` token.\n",
    "  - **Task:**  \n",
    "    The model must predict the original token for each masked position using the context provided by the surrounding tokens.\n",
    "  - **Purpose:**  \n",
    "    This encourages the model to learn deep, bidirectional representations of language, as it must infer missing words based on both preceding and following context.\n",
    "\n",
    "#### **Next Sentence Prediction (NSP):**  \n",
    "  - **Concept:**  \n",
    "    BERT is presented with pairs of sentences, where some pairs are consecutive sentences from the original text, while others are randomly paired.\n",
    "  - **Task:**  \n",
    "    The model must predict whether the second sentence logically follows the first.\n",
    "  - **Purpose:**  \n",
    "    NSP helps BERT capture inter-sentence relationships, which is beneficial for tasks that involve understanding the coherence and logical flow of text, such as question answering and natural language inference.\n",
    "\n",
    "#### **Outcome of Pre-Training:**  \n",
    "  Through MLM and NSP, BERT develops a rich understanding of language. It learns syntax, semantics, and even some world knowledge, which forms a foundation that can be fine-tuned for various tasks with minimal additional data.\n",
    "\n",
    "\n",
    "## BERT Fine-Tuning\n",
    "\n",
    "After pre-training, BERT is adapted to specific downstream tasks via fine-tuning, where the model's general language understanding is tailored to the particular requirements of the task.\n",
    "\n",
    "#### **Task-Specific Adaptation:**  \n",
    "  Fine-tuning involves adding a small, task-specific layer on top of the pre-trained BERT model.  \n",
    "  - **Example:**  \n",
    "    For a text classification task, the representation corresponding to the `[CLS]` token is passed through a softmax layer to produce a probability distribution over class labels.\n",
    "\n",
    "#### **Minimal Data Requirements:**  \n",
    "  Because BERT has already learned rich representations during pre-training, fine-tuning on downstream tasks often requires only a small amount of labeled data. This makes BERT highly efficient in scenarios where labeled data is limited.\n",
    "\n",
    "#### **Training Process:**  \n",
    "  - **Input Format:**  \n",
    "    The format remains similar to the pre-training phase, with inputs starting with a `[CLS]` token (and using `[SEP]` tokens to delimit sentences when needed).\n",
    "  - **Optimization:**  \n",
    "    During fine-tuning, the entire model—including the pre-trained layers and the new task-specific layers—is trained jointly. Optimization is typically performed using gradient descent-based methods like Adam or AdamW.\n",
    "  - **Learning Rate and Epochs:**  \n",
    "    Fine-tuning generally employs a smaller learning rate to prevent large updates that could erase the valuable language representations acquired during pre-training. The process usually runs for only a few epochs, ensuring the model adapts to the task without overfitting.\n",
    "\n",
    "This comprehensive process—from encoder-only architecture and bidirectional context understanding to sophisticated pre-training and efficient fine-tuning—forms the backbone of BERT’s success in a wide range of natural language processing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e2ae62",
   "metadata": {},
   "source": [
    "![](BERT_Comps.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e4971f",
   "metadata": {},
   "source": [
    "## Building a BERT Architecture from Scratch \n",
    "\n",
    "Below is a simplified implementation of a BERT-like model using PyTorch. \n",
    "\n",
    "This includes:\n",
    "- Embedding layers (token, position, and segment embeddings)\n",
    "- A stack of Transformer encoder layers to create contextualized representations\n",
    "- A classification head that uses the [CLS] token representation for sequence classification\n",
    "\n",
    "**Note:** This is a minimal example for educational purposes and does not include all optimizations of the original BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d0021f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits from custom BERT model: tensor([[-0.0095, -0.2699],\n",
      "        [-0.2570, -0.0590]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# Define the embedding layer for BERT\n",
    "class BertEmbeddings(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, max_position_embeddings, type_vocab_size):\n",
    "        super(BertEmbeddings, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.position_embeddings = nn.Embedding(max_position_embeddings, hidden_size)\n",
    "        self.token_type_embeddings = nn.Embedding(type_vocab_size, hidden_size)\n",
    "        \n",
    "        self.LayerNorm = nn.LayerNorm(hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, input_ids, token_type_ids=None):\n",
    "        seq_length = input_ids.size(1)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "        # Create position ids from 0 to sequence length - 1\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "        \n",
    "        words_embeddings = self.word_embeddings(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "        \n",
    "        # Sum all embeddings\n",
    "        embeddings = words_embeddings + position_embeddings + token_type_embeddings\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "# Define the BERT model using Transformer encoder layers\n",
    "class BertModel(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size=768, num_hidden_layers=12, \n",
    "                 num_attention_heads=12, intermediate_size=3072, max_position_embeddings=512, \n",
    "                 type_vocab_size=2):\n",
    "        super(BertModel, self).__init__()\n",
    "        self.embeddings = BertEmbeddings(vocab_size, hidden_size, max_position_embeddings, type_vocab_size)\n",
    "        \n",
    "        # Create a stack of transformer encoder layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_size, \n",
    "            nhead=num_attention_heads, \n",
    "            dim_feedforward=intermediate_size, \n",
    "            dropout=0.1,\n",
    "            activation='gelu'\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_hidden_layers)\n",
    "    \n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None):\n",
    "        # Get the embeddings for the input tokens\n",
    "        embeddings = self.embeddings(input_ids, token_type_ids)  # shape: (batch_size, seq_length, hidden_size)\n",
    "        \n",
    "        # Transformer encoder expects input of shape (seq_length, batch_size, hidden_size)\n",
    "        encoder_input = embeddings.transpose(0, 1)\n",
    "        \n",
    "        # Create src_key_padding_mask from attention_mask if provided.\n",
    "        # The mask shape should be (batch_size, seq_length) where positions with value 0 are masked.\n",
    "        if attention_mask is not None:\n",
    "            encoder_output = self.encoder(encoder_input, src_key_padding_mask=(attention_mask==0))\n",
    "        else:\n",
    "            encoder_output = self.encoder(encoder_input)\n",
    "        \n",
    "        # Transpose back to (batch_size, seq_length, hidden_size)\n",
    "        encoder_output = encoder_output.transpose(0, 1)\n",
    "        return encoder_output\n",
    "\n",
    "# Define a classification head that uses the [CLS] token representation (first token)\n",
    "class BertForSequenceClassification(nn.Module):\n",
    "    def __init__(self, bert_model, num_labels):\n",
    "        super(BertForSequenceClassification, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.classifier = nn.Linear(bert_model.embeddings.word_embeddings.embedding_dim, num_labels)\n",
    "    \n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None):\n",
    "        # Get the sequence output from the BERT model\n",
    "        sequence_output = self.bert(input_ids, token_type_ids, attention_mask)\n",
    "        # Use the first token ([CLS]) for classification\n",
    "        cls_token = sequence_output[:, 0, :]  # shape: (batch_size, hidden_size)\n",
    "        logits = self.classifier(cls_token)\n",
    "        return logits\n",
    "\n",
    "# Example usage of the custom BERT model from scratch\n",
    "if __name__ == \"__main__\":\n",
    "    # Hyperparameters for our minimal example\n",
    "    VOCAB_SIZE = 30522  # common vocab size for BERT\n",
    "    HIDDEN_SIZE = 768\n",
    "    MAX_POSITION_EMBEDDINGS = 512\n",
    "    TYPE_VOCAB_SIZE = 2\n",
    "    NUM_LABELS = 2  # e.g., binary classification\n",
    "    BATCH_SIZE = 2\n",
    "    SEQ_LENGTH = 16  # example sequence length\n",
    "\n",
    "    # Create dummy input data (random integers simulating token ids)\n",
    "    input_ids = torch.randint(0, VOCAB_SIZE, (BATCH_SIZE, SEQ_LENGTH))\n",
    "    token_type_ids = torch.zeros_like(input_ids)\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "\n",
    "    # Instantiate the model\n",
    "    bert_model = BertModel(vocab_size=VOCAB_SIZE, hidden_size=HIDDEN_SIZE, \n",
    "                           num_hidden_layers=2, num_attention_heads=12, \n",
    "                           intermediate_size=3072, max_position_embeddings=MAX_POSITION_EMBEDDINGS, \n",
    "                           type_vocab_size=TYPE_VOCAB_SIZE)\n",
    "    model = BertForSequenceClassification(bert_model, num_labels=NUM_LABELS)\n",
    "    \n",
    "    # Forward pass\n",
    "    logits = model(input_ids, token_type_ids, attention_mask)\n",
    "    print(\"Logits from custom BERT model:\", logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1154f13",
   "metadata": {},
   "source": [
    "## Using a Pre-Trained BERT Model Off The Shelf \n",
    "\n",
    "The below example demonstrates how to load a pre-trained BERT model for sequence classification using the Hugging Face Transformers library.\n",
    "We tokenize an example sentence and then perform a forward pass through the model to obtain classification logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4726c9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathanschlosser/anaconda3/envs/pytorch_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/jonathanschlosser/anaconda3/envs/pytorch_env/lib/python3.11/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class from pre-trained BERT: 1\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Load a pre-trained BERT tokenizer and model from Hugging Face\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Example input text\n",
    "text = \"This is a sample sentence for classification.\"\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "# Perform inference with the model\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "\n",
    "# Get the predicted class (for binary classification, 0 or 1)\n",
    "predicted_class = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "print(\"Predicted class from pre-trained BERT:\", predicted_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6366dbde",
   "metadata": {},
   "source": [
    "# Zero-Shot, Few-Shot, and Full Training\n",
    "\n",
    "These approaches describe how pre-trained models can be adapted for specific downstream tasks. Depending on the amount of labeled data available and the complexity of the task, you can choose from different strategies: zero-shot, few-shot, and full training. Each method leverages the rich language representations learned during pre-training in different ways.\n",
    "\n",
    "## Zero-Shot Learning\n",
    "\n",
    "Zero-shot learning involves using a pre-trained model to make predictions on a new task without any additional fine-tuning. In this scenario, the model relies solely on the general language understanding it gained during pre-training. \n",
    "\n",
    "**Definition:**  \n",
    "  The model is directly applied to a downstream task without any task-specific training.\n",
    "\n",
    "**Use Case:**  \n",
    "  This approach is particularly useful when no labeled data is available for the new task. For instance, you might want to classify text in a domain that was not present during the model's pre-training.\n",
    "\n",
    "**Limitations:**  \n",
    "  Since the model hasn't been tailored to the specific nuances of the new task, zero-shot predictions may be less accurate, especially if the task is very different from the data used during pre-training.\n",
    "\n",
    "\n",
    "## Few-Shot Learning\n",
    "\n",
    "Few-shot learning strikes a balance between leveraging pre-trained knowledge and adapting to a new task with a minimal amount of labeled data. Here, the model is fine-tuned on a small number of examples, which allows it to quickly learn the characteristics of the target task.\n",
    "\n",
    "**Definition:**  \n",
    "  The pre-trained model is fine-tuned using a very small number of examples from the target task.\n",
    "\n",
    "**Use Case:**  \n",
    "  This method is beneficial when labeled data is scarce or expensive to obtain. It allows the model to quickly adapt to a new task without the need for a large training set.\n",
    "\n",
    "**Benefits:**  \n",
    "  Due to the strong representations learned during pre-training, even a few examples can be enough to achieve reasonable performance on the target task.\n",
    "\n",
    "**Challenges:**  \n",
    "  The success of few-shot learning can be highly sensitive to the quality and representativeness of the few examples provided. If the examples are not well-chosen, the model might not generalize effectively to unseen data.\n",
    "\n",
    "\n",
    "## Full Training (Fine-Tuning with Extensive Data)\n",
    "\n",
    "Full training involves fine-tuning the pre-trained model on a large, task-specific dataset. This approach is typically used when abundant labeled data is available and the goal is to achieve the highest possible performance on the target task.\n",
    "\n",
    "**Definition:**  \n",
    "  The pre-trained model is further trained (fine-tuned) on a comprehensive dataset specific to the task at hand.\n",
    "\n",
    "**Use Case:**  \n",
    "  When ample labeled data is available, full training allows the model to capture the fine-grained nuances and patterns that are particular to the task, often resulting in state-of-the-art performance.\n",
    "\n",
    "**Benefits:**  \n",
    "  Extensive fine-tuning can significantly improve accuracy by aligning the model's representations more closely with the target task's requirements.\n",
    "\n",
    "**Trade-Off:**  \n",
    "  This approach demands more computational resources and carries a higher risk of overfitting if the training process is not carefully managed. It requires a balance between leveraging the pre-trained knowledge and adapting to the specific patterns in the labeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecad8f48",
   "metadata": {},
   "source": [
    "# Decoder-Only Transformers\n",
    "\n",
    "Decoder-only transformers are designed specifically for generating output sequences. They are the backbone of models used for text generation and text completion. Unlike encoder-only models, which focus on understanding and processing input text, decoder-only architectures are autoregressive, meaning they generate text one token at a time based solely on the tokens that have come before. A well-known example of this type of architecture is the GPT (Generative Pre-Trained Transformer) series.\n",
    "\n",
    "![](Decoder_Only.png)\n",
    "\n",
    "## Core Characteristics and Training\n",
    "\n",
    "At the heart of a decoder-only transformer is its ability to generate text by predicting the next token in a sequence. This process involves a few key modifications compared to the bidirectional models like BERT:\n",
    "\n",
    "- **Autoregressive Generation:**  \n",
    "  In decoder-only models, the prediction of each token depends only on the tokens that have already been generated. This is in contrast to models like BERT, which use bidirectional context and can attend to tokens both before and after a given position.\n",
    "\n",
    "- **Causal Masking:**  \n",
    "  A crucial component of training a decoder-only transformer is the implementation of causal masking in the self-attention mechanism.  \n",
    "  - **How It Works:**  \n",
    "    During training, the model computes attention scores between all pairs of tokens. To ensure that a token only considers previous tokens (and not any future tokens), the upper triangular portion of the attention score matrix is set to negative infinity.  \n",
    "  - **Impact of Masking:**  \n",
    "    When the softmax function is applied to these scores, the masked positions effectively contribute a weight of zero. This prevents the model from \"cheating\" by looking ahead in the sequence, thereby enforcing the autoregressive property.\n",
    "\n",
    "- **Parallelization:**  \n",
    "  Despite being autoregressive, the transformer architecture allows for parallel processing during training. Unlike RNNs, which process tokens sequentially, all tokens in a sequence can be processed simultaneously because the causal mask is applied uniformly. This parallelized computation reduces training time and makes it feasible to train very large models on extensive datasets.\n",
    "  \n",
    "\n",
    "## Advantages Over RNN-Based Models\n",
    "\n",
    "Decoder-only transformers offer several significant benefits compared to traditional RNN-based language models:\n",
    "\n",
    "- **Parallel Training:**  \n",
    "  The self-attention mechanism allows the model to compute representations for all tokens concurrently, which dramatically speeds up training and makes it scalable to larger datasets and models.\n",
    "  \n",
    "- **Stable Training Dynamics:**  \n",
    "  With features like layer normalization and residual connections, transformer architectures avoid the common pitfalls of RNNs such as vanishing or exploding gradients.\n",
    "  \n",
    "- **Short Effective Path Length:**  \n",
    "  In transformer models, every token in the sequence is directly connected to every other token within a single layer, meaning that the effective path length—the number of computational steps needed for information to travel from one token to any other—is always 1. \n",
    "  \n",
    "\n",
    "## GPT Models\n",
    "\n",
    "The GPT family is the most prominent example of decoder-only transformers. Generative Pre-Trained Transformers (GPT) are specifically designed for tasks that require text generation, such as:\n",
    "\n",
    "- **Text Generation and Completion:**  \n",
    "  GPT models generate coherent and contextually relevant text by predicting one token at a time.\n",
    "  \n",
    "- **Conversational AI and Chatbots:**  \n",
    "  Their ability to generate human-like text makes them suitable for building interactive conversational systems.\n",
    "\n",
    "- **Language Modeling:**  \n",
    "  They serve as powerful language models capable of understanding and generating natural language.\n",
    "\n",
    "Examples in the GPT series include GPT-1, GPT-2, and GPT-3. More recent models like GPT-3.5 and GPT-4 are proprietary and may incorporate ensemble techniques or additional refinements, but they continue to build upon the core principles of the autoregressive, decoder-only approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffe5cce",
   "metadata": {},
   "source": [
    "## GPT Model with Hugging Face\n",
    "\n",
    "In this example, we use GPT-2 (a fully open-source GPT model) to generate text. This snippet loads the model and tokenizer, encodes a prompt, and generates text with some sampling options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "845286d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      " Once upon a time in a land far away, where the sea has come out of the sea, and the mountains have gone down to earth for a long time, where the air has not breathed. The seas and the land are like the wind; and the air is the wind. I say to you that my dream has not an end, and I will come again. Let them come to me, and be glad to hear you.\n",
      "\n",
      "CHAPTER XIII\n",
      "\n",
      "\n",
      "O n the sea, I\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"gpt2\"  # You can also experiment with other models like 'EleutherAI/gpt-neo-125M'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Define a prompt for text generation\n",
    "prompt = \"Once upon a time in a land far away\"\n",
    "inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate text using sampling (do_sample=True) for diversity\n",
    "output = model.generate(\n",
    "    inputs,\n",
    "    max_length=100,         # maximum length of the generated sequence\n",
    "    do_sample=True,         # use sampling instead of greedy decoding\n",
    "    top_k=50,               # limit the number of highest probability tokens to consider\n",
    "    top_p=0.95,             # use nucleus sampling to consider tokens with cumulative probability 0.95\n",
    "    num_return_sequences=1  # generate one sequence\n",
    ")\n",
    "\n",
    "# Decode and print the generated text\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Generated Text:\\n\", generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3b6071",
   "metadata": {},
   "source": [
    "## Chat-GPT 4o-mini with OpenAI\n",
    "\n",
    "This example demonstrates how to use the OpenAI Python API to interact with ChatGPT-4. The example sends a conversation to the model and prints out the response. You will need an API key for this to work, which can be obtained from https://platform.openai.com/docs/overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "117b6def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the scarecrow win an award?\n",
      "\n",
      "Because he was outstanding in his field!\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# Make a request to GPT-4\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini-2024-07-18\",\n",
    "    messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "              {\"role\": \"user\", \"content\": \"Tell me a joke.\"}]\n",
    ")\n",
    "\n",
    "# Print the response\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9daf731f",
   "metadata": {},
   "source": [
    "# Encoder-Decoder Transformers\n",
    "\n",
    "Encoder-decoder transformers, originally introduced in \"Attention is All You Need\", are designed to handle tasks where the input is transformed into a different output sequence. This architecture is particularly effective for sequence-to-sequence tasks such as machine translation, summarization, and text generation. In these models, the encoder processes the input text into a rich, context-aware representation, while the decoder generates the output text based on this representation and the previously generated tokens.\n",
    "\n",
    "The overall process is as follows:\n",
    "- **Encoder:**  \n",
    "  The encoder takes the input text and processes it through several layers of self-attention and feed-forward networks. This results in a set of high-dimensional representations that capture the contextual relationships between tokens.\n",
    "  \n",
    "- **Decoder:**  \n",
    "  The decoder generates the output text in an autoregressive manner. It uses a combination of masked self-attention (to ensure that each token is generated only based on preceding tokens) and cross-attention (to integrate information from the encoder's output). This allows the model to produce coherent and contextually relevant output sequences.\n",
    "\n",
    "\n",
    "## T5 (Text-to-Text Transfer Transformer)\n",
    "\n",
    "T5 takes a unified approach by recasting all NLP tasks as text-to-text problems. This means that no matter what the original task is — be it translation, summarization, or classification — both the input and output are treated as text. This unified framework simplifies the training and fine-tuning process considerably.\n",
    "\n",
    "- **Unified Framework:**  \n",
    "  Every task is converted into a format where the model receives text as input and generates text as output. For example, a classification task might involve formatting the input as \"Classify: [text]\" and expecting a textual label as the output.\n",
    "  \n",
    "- **Pre-training and Fine-Tuning:**  \n",
    "  T5 is pre-trained on a large corpus with various unsupervised tasks, learning a broad understanding of language. It is then fine-tuned on specific tasks, which allows it to achieve state-of-the-art performance across a wide range of benchmarks.\n",
    "  \n",
    "- **Benefits:**  \n",
    "  The text-to-text approach unifies the model architecture across tasks, making it easier to transfer learning from one task to another and reducing the need for separate, task-specific models.\n",
    "  \n",
    "![](https://miro.medium.com/v2/resize:fit:1400/0*-MxKkmD7pRHnc0gx.png)\n",
    "\n",
    "\n",
    "## BART (Bidirectional and Auto-Regressive Transformers)\n",
    "\n",
    "BART is designed to leverage the strengths of both bidirectional and autoregressive models. It combines a bidirectional encoder, similar to BERT, with an autoregressive decoder, similar to GPT, which makes it highly effective for tasks that require both understanding and generating text.\n",
    "\n",
    "- **Hybrid Architecture:**  \n",
    "  BART uses an encoder-decoder structure where the encoder is capable of capturing context from the entire input sequence (bidirectional attention) and the decoder generates output in an autoregressive manner (ensuring each generated token is based only on previous tokens).\n",
    "  \n",
    "- **Denoising Autoencoder Objective:**  \n",
    "  During training, BART is tasked with reconstructing the original text from a deliberately corrupted version. This denoising objective helps the model learn robust representations of text and improves its performance in generating coherent and fluent outputs.\n",
    "  \n",
    "- **Applications:**  \n",
    "  The dual nature of BART—being good at both understanding and generating text—makes it a versatile model for a variety of tasks including summarization, translation, and general text generation.\n",
    "\n",
    "![](BART.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5337e8b",
   "metadata": {},
   "source": [
    "# Challenges and Future Directions\n",
    "\n",
    "Despite their impressive performance, transformer models face several key challenges that continue to drive research in the field. These challenges include computational issues and scalability, interpretability, and domain adaptation. \n",
    "\n",
    "## Scalability\n",
    "\n",
    "One of the primary computational challenges with transformers is the quadratic growth in the computation of attention scores relative to the sequence length. Since every token attends to every other token, the self-attention mechanism has a time complexity of O(n²), where n is the number of tokens. This becomes a significant bottleneck when processing long sequences.\n",
    "\n",
    "- **Computational Bottleneck:**  \n",
    "  As the sequence length increases, the number of pairwise comparisons grows rapidly, leading to high memory usage and slower computation. This limits the practicality of applying standard transformers to very long texts or sequences.\n",
    "\n",
    "- **Approximating Attention Scores:**  \n",
    "  Researchers are exploring various methods to reduce this computational load:\n",
    "  - **Sparse Attention Mechanisms:**  \n",
    "    Instead of calculating attention scores for every token pair, sparse attention restricts the connections to a subset of tokens. This creates a sparser graph of attention relationships, which reduces the overall number of computations.\n",
    "  - **Efficient Attention Approximations:**  \n",
    "    Methods such as low-rank approximations, kernel-based techniques, and random feature mappings (e.g., the Performer model) aim to approximate the full softmax attention more efficiently. These techniques can sometimes reduce the complexity from quadratic to linear or near-linear time while still preserving much of the model’s effectiveness.\n",
    "\n",
    "## Interpretability\n",
    "\n",
    "Transformers, especially large-scale models with billions of parameters, often function as \"black boxes.\" Understanding the decision-making process and the roles of individual parameters or attention heads is a significant challenge.\n",
    "\n",
    "- **Opaque Decision-Making:**  \n",
    "  The internal workings of transformers are complex, making it difficult to trace how specific outputs are generated from a given input. The sheer size of modern models complicates efforts to assign meaning to individual weights or neurons.\n",
    "  \n",
    "- **Techniques for Interpretability:**  \n",
    "  Several methods are being developed to gain insights into transformer behavior:\n",
    "  - **Visualization of Attention Weights:**  \n",
    "    By examining the attention weights, researchers can sometimes identify patterns in how the model focuses on different parts of the input. However, these visualizations do not always provide a complete explanation of the model’s decisions.\n",
    "  - **Attention Rollouts and Attribution Methods:**  \n",
    "    Techniques such as attention rollouts, gradient-based attributions, and integrated gradients help in understanding the contribution of various tokens and layers. These methods can shed light on which parts of the input are most influential for the model's predictions.\n",
    "  - **Investigating Multi-Head Attention:**  \n",
    "    Researchers are also analyzing the roles of different attention heads within multi-head attention. Some heads may focus on syntactic relationships while others capture semantic nuances. This investigation helps in understanding how the model decomposes and processes information, although a complete picture remains elusive.\n",
    "\n",
    "## Domain Adaptation\n",
    "\n",
    "Domain adaptation refers to the challenge of applying a pre-trained transformer model, which is typically trained on large general corpora, to specialized domains that may have unique terminology or stylistic differences.\n",
    "\n",
    "- **Generalization Issues:**  \n",
    "  Models pre-trained on broad datasets might not perform optimally when applied to domain-specific tasks. The language and context in specialized fields (e.g., medical, legal, or technical documents) can differ significantly from general text.\n",
    "  \n",
    "- **Strategies for Domain Adaptation:**  \n",
    "  To address these issues, several approaches are being pursued:\n",
    "  - **Fine-Tuning on Domain-Specific Data:**  \n",
    "    One common approach is to fine-tune the pre-trained model on a smaller, domain-specific dataset. This helps the model adjust its representations to better capture the nuances of the target domain.\n",
    "  - **Domain-Specific Pre-Training:**  \n",
    "    In some cases, researchers pre-train models from scratch or continue pre-training on domain-specific corpora to create a specialized model.\n",
    "  - **Adapter Modules:**  \n",
    "    Adapter modules are lightweight, task-specific layers that can be inserted into a pre-trained model. These adapters allow for efficient domain adaptation without the need to fine-tune the entire model, thus preserving the general knowledge while incorporating domain-specific nuances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2beb58",
   "metadata": {},
   "source": [
    "# Real World ChatBot Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796418d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from openai import OpenAI\n",
    "\n",
    "# Set up OpenAI client\n",
    "client = OpenAI(api_key=api_key)  # Replace with your OpenAI API key\n",
    "\n",
    "# Streamlit UI\n",
    "st.title(\"💬 OpenAI Chatbot\")\n",
    "st.write(\"A simple chatbot powered by OpenAI.\")\n",
    "\n",
    "# Ensure session state is initialized\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state[\"messages\"] = []\n",
    "\n",
    "# Display chat history using Streamlit's `st.chat_message`\n",
    "for msg in st.session_state[\"messages\"]:\n",
    "    with st.chat_message(msg[\"role\"]):\n",
    "        st.write(msg[\"content\"])\n",
    "\n",
    "# User input\n",
    "if user_input := st.chat_input(\"Type your message...\"):\n",
    "    # Add user input to chat history\n",
    "    st.session_state[\"messages\"].append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    # Display user message in chat interface\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.write(user_input)\n",
    "\n",
    "    # Show a loading indicator while waiting for OpenAI's response\n",
    "    with st.spinner(\"Thinking...\"):\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=st.session_state[\"messages\"]\n",
    "        )\n",
    "\n",
    "    # Get AI response\n",
    "    ai_response = response.choices[0].message.content\n",
    "\n",
    "    # Add AI response to chat history\n",
    "    st.session_state[\"messages\"].append({\"role\": \"assistant\", \"content\": ai_response})\n",
    "\n",
    "    # Display AI response in chat interface\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        st.write(ai_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c538b3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
      "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://192.168.1.17:8501\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  For better performance, install the Watchdog module:\u001b[0m\n",
      "\n",
      "  $ xcode-select --install\n",
      "  $ pip install watchdog\n",
      "            \u001b[0m\n",
      "2025-03-24 19:16:20.735 Uncaught app exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jonathanschlosser/anaconda3/envs/LLMTestEnvironment/lib/python3.10/site-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 535, in _run_script\n",
      "    exec(code, module.__dict__)\n",
      "  File \"/Users/jonathanschlosser/Library/CloudStorage/GoogleDrive-jonathanphilipschlosser@gmail.com/My Drive/UNC MADS/DATA 785 - Deep Learning/Week 11 - Transformer Variants/chatbot.py\", line 5, in <module>\n",
      "    client = OpenAI(api_key=api_key)  # Replace with your OpenAI API key\n",
      "NameError: name 'api_key' is not defined\n"
     ]
    }
   ],
   "source": [
    "!streamlit run chatbot.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1779b58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c9dc80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b507fc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:LLMTestEnvironment]",
   "language": "python",
   "name": "conda-env-LLMTestEnvironment-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
