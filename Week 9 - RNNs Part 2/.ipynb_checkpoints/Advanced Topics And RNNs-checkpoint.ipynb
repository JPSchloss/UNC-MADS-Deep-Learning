{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48578814",
   "metadata": {},
   "source": [
    "# Advanced RNN Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd96d08",
   "metadata": {},
   "source": [
    "## Using RNNs For Classification\n",
    "\n",
    "\n",
    "When you have a sequential input (such as a sentence, an email, or any time series data) and you wish to assign a single label or category to the entire sequence, an RNN-based classification approach is often very effective. The general idea is to treat the RNN as a feature extractor—processing the sequence step by step—and then use the resulting representation to predict the classification label.\n",
    "\n",
    "RNNs are well-suited for these tasks because they process data one element at a time while maintaining a hidden state that carries information about previous elements in the sequence. This ability makes RNNs an excellent candidate for classification tasks where the input is a sequence and the goal is to assign a label to the entire sequence.\n",
    "\n",
    "Imagine two common examples:\n",
    "- **Sentiment Analysis:** You have a string of text (e.g., a movie review) and you want to classify it as expressing positive or negative sentiment.\n",
    "- **Spam Detection:** You have the content of an email and you want to label it as \"spam\" or \"not spam.\"\n",
    "\n",
    "In both cases, the input is a sequence (words in a sentence or tokens in an email), and the goal is to produce a single output label that summarizes the entire sequence.\n",
    "\n",
    "### Model Architecture Overview\n",
    "\n",
    "The typical approach involves two main components:\n",
    "\n",
    "1. **The RNN as a Featurizer:**  \n",
    "   - The RNN (or its variants like LSTM/GRU) processes the input sequence step by step.\n",
    "   - At each step, it updates its hidden state, which is designed to capture the context of the sequence seen so far.\n",
    "   - In many classification tasks, we only need a summary of the sequence, so we might take the hidden state from the final time step. This final hidden state is considered a learned representation (or feature) of the whole sequence.\n",
    "\n",
    "2. **The Classifier:**  \n",
    "   - After obtaining the sequence representation from the RNN, this representation is fed into a classifier. \n",
    "   - The classifier can be a feed-forward neural network (also known as a fully connected layer), or even a simpler model like logistic regression.\n",
    "   - This component outputs the final class probabilities (e.g., positive/negative for sentiment analysis or spam/not spam for email classification).\n",
    "\n",
    "In this sense, the RNN acts as a feature extractor, transforming raw sequential data into a fixed-size representation that captures its underlying patterns, while the classifier makes the final decision based on these features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bd87cf",
   "metadata": {},
   "source": [
    "### TensorFlow Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cccd567",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-17 01:43:50.932677: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-17 01:44:11.653427: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 555ms/step - loss: 0.8707\n",
      "1/1 [==============================] - 0s 144ms/step\n",
      "Predicted class for test input: 0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "\n",
    "# Build a simple RNN classifier using Keras\n",
    "def build_rnn_classifier(input_size, hidden_size, num_classes, seq_length):\n",
    "    model = models.Sequential()\n",
    "    # SimpleRNN layer automatically processes the sequence and returns the last output by default\n",
    "    model.add(layers.SimpleRNN(hidden_size, input_shape=(seq_length, input_size)))\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 10     # Dimensionality of each input vector\n",
    "hidden_size = 20    # Size of the RNN hidden state\n",
    "num_classes = 2     # Number of classes for classification\n",
    "seq_length = 5      # Length of each input sequence\n",
    "batch_size = 16     # Batch size\n",
    "\n",
    "# Instantiate and compile the model\n",
    "model = build_rnn_classifier(input_size, hidden_size, num_classes, seq_length)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "# Create dummy input data and target labels for training\n",
    "inputs = np.random.randn(batch_size, seq_length, input_size)\n",
    "targets = np.random.randint(0, num_classes, size=(batch_size,))\n",
    "\n",
    "# Train for one epoch on the dummy data\n",
    "model.fit(inputs, targets, epochs=1, batch_size=batch_size, verbose=1)\n",
    "\n",
    "# Prediction: using a new dummy sample\n",
    "test_input = np.random.randn(1, seq_length, input_size)  # single sample\n",
    "predictions = model.predict(test_input)\n",
    "predicted_class = np.argmax(predictions, axis=1)[0]\n",
    "print(\"Predicted class for test input:\", predicted_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4637614",
   "metadata": {},
   "source": [
    "### PyTorch Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f56e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a simple RNN-based classifier\n",
    "class SimpleRNNClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(SimpleRNNClassifier, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_length, input_size]\n",
    "        out, _ = self.rnn(x)  # out shape: [batch_size, seq_length, hidden_size]\n",
    "        # Use the last time step's hidden state for classification\n",
    "        out = out[:, -1, :]   # shape: [batch_size, hidden_size]\n",
    "        out = self.fc(out)    # shape: [batch_size, num_classes]\n",
    "        return out\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 10     # Dimensionality of each input vector\n",
    "hidden_size = 20    # Size of the hidden state in the RNN\n",
    "num_layers = 1      # Number of RNN layers\n",
    "num_classes = 2     # Number of output classes (e.g., spam or not spam)\n",
    "batch_size = 16     # Batch size\n",
    "seq_length = 5      # Length of each input sequence\n",
    "\n",
    "# Instantiate the model, loss function, and optimizer\n",
    "model = SimpleRNNClassifier(input_size, hidden_size, num_layers, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Create dummy input data and target labels for training\n",
    "inputs = torch.randn(batch_size, seq_length, input_size)\n",
    "targets = torch.randint(0, num_classes, (batch_size,))\n",
    "\n",
    "# Training step\n",
    "model.train()\n",
    "optimizer.zero_grad()\n",
    "outputs = model(inputs)\n",
    "loss = criterion(outputs, targets)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(\"Training complete, loss:\", loss.item())\n",
    "\n",
    "# Prediction: using a new dummy sample\n",
    "model.eval()  # set to evaluation mode\n",
    "with torch.no_grad():\n",
    "    test_input = torch.randn(1, seq_length, input_size)  # single sample\n",
    "    test_output = model(test_input)\n",
    "    # Get predicted class (highest score)\n",
    "    predicted_class = torch.argmax(test_output, dim=1)\n",
    "    print(\"Predicted class for test input:\", predicted_class.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2799375a",
   "metadata": {},
   "source": [
    "## Additional Considerations\n",
    "\n",
    "### Variants in Representing the Sequence\n",
    "\n",
    "While the most common approach is to use the hidden state at the last time step, there are alternative strategies for summarizing the sequence:\n",
    "\n",
    "- **Element-wise Mean Pooling:**  \n",
    "  Instead of just taking the final state, compute the average of the hidden states over all time steps. This approach ensures that every part of the sequence contributes equally to the final representation.\n",
    "  \n",
    "- **Element-wise Max Pooling:**  \n",
    "  Alternatively, take the element-wise maximum over all hidden states. This method captures the most salient features that were activated at any time step in the sequence.\n",
    "\n",
    "Each of these strategies has its own strengths. For example, mean pooling can smooth out the features, whereas max pooling tends to emphasize the strongest signals across the sequence.\n",
    "\n",
    "### End-to-End Training\n",
    "\n",
    "One of the major advantages of this architecture is that it supports **end-to-end training**:\n",
    "\n",
    "- **Error Gradients Flow Back Through the Entire Network:**  \n",
    "  When you train the model, you compute a loss based on the classifier's output (e.g., cross-entropy loss for classification). The gradients of this loss are then backpropagated through the classifier and further into the RNN.\n",
    "\n",
    "- **RNN as Part of the Pipeline:**  \n",
    "  This means that the RNN not only learns to represent the sequence but also adapts its internal representation to improve the final classification. This synergy between the RNN and the classifier is essential for learning meaningful features from the data.\n",
    "  \n",
    "### Practical Considerations\n",
    "\n",
    "- **Choice of RNN Variant:**  \n",
    "  Depending on the complexity of the sequence and the amount of data, you might choose a vanilla RNN, Long Short-Term Memory (LSTM), or Gated Recurrent Unit (GRU). LSTMs and GRUs are often preferred because they better handle long-term dependencies.\n",
    "  \n",
    "- **Regularization and Overfitting:**  \n",
    "  Since RNNs can be prone to overfitting, techniques such as dropout, early stopping, and using more data are important considerations.\n",
    "\n",
    "- **Batching and Sequence Padding:**  \n",
    "  When working with sequences of different lengths, you'll need to pad them to a common length and use masking techniques so that the model does not treat the padded values as meaningful input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e91deaa",
   "metadata": {},
   "source": [
    "## Teacher Forcing\n",
    "\n",
    "**Teacher forcing** is a training strategy used primarily in sequence generation tasks, such as text generation or machine translation. In a typical sequence-to-sequence model, the RNN generates one token at a time based on the previous token it generated. However, during training, the model can become unstable if it relies solely on its own predictions as inputs for the next time step. \n",
    "\n",
    "Teacher forcing addresses this by replacing the model's previous prediction with the actual target (or \"ground truth\") token during training. In other words, **instead of feeding back the model's generated output, you \"force\" the correct answer into the next time step.** \n",
    "\n",
    "This method:\n",
    "- **Speeds up training:** Since the model is always receiving the correct context, it learns the mapping from input to output faster.\n",
    "- **Improves convergence:** It helps mitigate error accumulation over time, which is especially important in long sequences.\n",
    "  \n",
    "**Relation to Text Generation:**  \n",
    "When generating text, teacher forcing helps the model learn the correct sequence dynamics. For example, if the model is tasked with generating a sentence, using teacher forcing during training ensures that the RNN sees the correct word at each time step, rather than relying on its potentially flawed own output. This practice helps in developing a more robust sequence generation process, although during inference (when generating new text), the model must rely on its own predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd8aecf",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/v2/resize:fit:842/1*U3d8D_GnfW13Y3nDgvwJSw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29132404",
   "metadata": {},
   "source": [
    "## RNNs for Word Embeddings\n",
    "\n",
    "**Word embeddings** are dense vector representations of words that capture semantic meanings, where similar words are mapped to similar vectors. While many models (like Word2Vec or GloVe) are designed to learn word embeddings independently, RNNs can also be used to learn or refine embeddings within the context of sequential data.\n",
    "\n",
    "#### How It Works:\n",
    "- **Input Representations:** In an RNN, words are typically first converted into embeddings using an embedding layer. These embeddings are then fed into the RNN to capture contextual information.\n",
    "- **Contextualized Representations:** As the RNN processes the sequence, it not only considers the fixed embedding but also integrates context from surrounding words. This results in dynamic, context-aware representations that can capture nuances like polysemy (words having multiple meanings based on context).\n",
    "- **Learning Process:** During training, the gradients from the output (e.g., predicting the next word or classifying a sentence) backpropagate through the embedding layer, thus refining the embeddings based on the task at hand.\n",
    "\n",
    "Using RNNs in this manner allows the model to jointly learn the word embeddings and the sequential relationships in the data, often resulting in more effective representations for tasks like sentiment analysis, machine translation, or language modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9950e3",
   "metadata": {},
   "source": [
    "#### TensorFlow Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c239b292",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Define model hyperparameters\n",
    "vocab_size = 50       # Size of the vocabulary\n",
    "embedding_dim = 8     # Dimensionality of the word embeddings\n",
    "hidden_dim = 16       # Number of units in the RNN\n",
    "output_dim = 2        # Number of output classes (e.g., for classification)\n",
    "seq_length = 7        # Length of each input sequence\n",
    "batch_size = 4        # Batch size\n",
    "\n",
    "# Build a simple RNN classifier using Keras\n",
    "model = tf.keras.Sequential([\n",
    "    # Embedding layer: maps token indices to dense vectors\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=seq_length),\n",
    "    # SimpleRNN layer: processes the sequence of embeddings\n",
    "    tf.keras.layers.SimpleRNN(hidden_dim),\n",
    "    # Dense layer for classification with softmax activation\n",
    "    tf.keras.layers.Dense(output_dim, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model with an optimizer and loss suitable for classification\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "# Create dummy input data and target labels\n",
    "inputs = np.random.randint(0, vocab_size, (batch_size, seq_length))\n",
    "targets = np.random.randint(0, output_dim, (batch_size,))\n",
    "\n",
    "# Train the model for one epoch on the dummy data\n",
    "model.fit(inputs, targets, epochs=1, batch_size=batch_size, verbose=1)\n",
    "\n",
    "# Prediction: using the same dummy inputs for demonstration\n",
    "predictions = model.predict(inputs)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "print(\"TensorFlow predicted classes for inputs:\", predicted_classes)\n",
    "\n",
    "# Optional: Extract and inspect the learned embedding weights\n",
    "embedding_layer = model.layers[0]\n",
    "embedding_weights = embedding_layer.get_weights()[0]\n",
    "print(\"Shape of learned embedding weights:\", embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c27ae80",
   "metadata": {},
   "source": [
    "#### PyTorch Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9432e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a simple RNN model that includes a word embedding layer\n",
    "class RNNWordEmbeddingModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(RNNWordEmbeddingModel, self).__init__()\n",
    "        # Embedding layer to convert token indices to dense vectors\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # RNN layer: processes the sequence of embeddings\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        # Classifier: maps the RNN output to desired output dimensions (e.g., for classification)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_length] -> embedding: [batch_size, seq_length, embedding_dim]\n",
    "        embedded = self.embedding(x)\n",
    "        # Pass through RNN: out shape is [batch_size, seq_length, hidden_dim]\n",
    "        out, _ = self.rnn(embedded)\n",
    "        # Use the last time step's output for classification\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out, embedded\n",
    "\n",
    "# Hyperparameters\n",
    "vocab_size = 50       # Size of the vocabulary (number of unique tokens)\n",
    "embedding_dim = 8     # Dimensionality of the word embeddings\n",
    "hidden_dim = 16       # Size of the hidden state in the RNN\n",
    "output_dim = 2        # Number of output classes (e.g., positive/negative)\n",
    "batch_size = 4        # Batch size for training\n",
    "seq_length = 7        # Length of each input sequence\n",
    "\n",
    "# Instantiate the model, loss function, and optimizer\n",
    "model = RNNWordEmbeddingModel(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Create dummy input data: batch of tokenized sequences (indices)\n",
    "inputs = torch.randint(0, vocab_size, (batch_size, seq_length))\n",
    "# Dummy target labels for a classification task\n",
    "targets = torch.randint(0, output_dim, (batch_size,))\n",
    "\n",
    "# Training step\n",
    "model.train()\n",
    "optimizer.zero_grad()\n",
    "outputs, embedded = model(inputs)\n",
    "loss = criterion(outputs, targets)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(\"PyTorch training loss:\", loss.item())\n",
    "\n",
    "# Prediction: using a new dummy sample\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_input = torch.randint(0, vocab_size, (1, seq_length))  # single sample\n",
    "    test_output, test_embedded = model(test_input)\n",
    "    predicted_class = torch.argmax(test_output, dim=1)\n",
    "    print(\"Predicted class for test input:\", predicted_class.item())\n",
    "\n",
    "# Optional: Inspect learned word embeddings for the first sample\n",
    "print(\"Learned embeddings for first sample:\")\n",
    "print(embedded[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf14660",
   "metadata": {},
   "source": [
    "## Weight Tying\n",
    "\n",
    "**Weight tying** is a technique that reduces the number of parameters in a model by sharing weights between different layers. In language models and sequence-to-sequence models, weight tying is often used to share weights between the input embedding layer and the output softmax layer.\n",
    "\n",
    "#### Why Use Weight Tying?\n",
    "- **Parameter Efficiency:** It reduces the model size, which is beneficial for training and inference speed.\n",
    "- **Regularization:** Sharing weights can act as a form of regularization, helping to prevent overfitting.\n",
    "- **Empirical Performance:** Research has shown that weight tying can lead to better generalization and performance in language modeling tasks.\n",
    "\n",
    "In practice, when a word embedding matrix is tied with the output projection matrix, the model effectively learns a single representation for each word that is used both for encoding and decoding, ensuring consistency across the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7eb523e",
   "metadata": {},
   "source": [
    "![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRzYdyiXTw53dusWKRfNR9uxHmFLUgUqII3Gg&s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57f4164",
   "metadata": {},
   "source": [
    "## Gradient Computation with RNNs\n",
    "\n",
    "#### Problems:\n",
    "- **Repeated Multiplication of Weights:**  \n",
    "  The gradient at each time step involves repeated multiplication by the weight matrix (often denoted as **W**). This repetition can cause the gradients to either shrink or grow exponentially.\n",
    "  \n",
    "- **Vanishing Gradients:**  \n",
    "  When the weights are such that the repeated multiplication causes the gradient to become extremely small, the model has difficulty learning long-range dependencies because the gradients vanish before reaching earlier time steps.\n",
    "  \n",
    "- **Exploding Gradients:**  \n",
    "  Conversely, if the weights are too large, the gradient can grow exponentially, leading to numerical instability during training.\n",
    "\n",
    "#### Solutions:\n",
    "- **Special Activation Functions:**  \n",
    "  Activation functions like the Rectified Linear Unit (ReLU) have a gradient that does not vanish as easily as traditional functions like tanh or sigmoid. Using these activations can help mitigate the vanishing gradient problem.\n",
    "  \n",
    "- **Normalization Techniques:**  \n",
    "  - **Batch Normalization:** Normalizes the output of layers across the batch, stabilizing and speeding up the training.\n",
    "  - **Layer Normalization:** Applies normalization across the features of each data point rather than across the batch. This is particularly useful in RNNs because it handles varying sequence lengths more gracefully.\n",
    "  \n",
    "- **Gradient Clipping:**  \n",
    "  By setting a threshold for the maximum gradient value, gradient clipping prevents gradients from exceeding a certain magnitude, effectively controlling the exploding gradient problem. This involves computing the norm of the gradient and scaling it if it exceeds a predefined limit.\n",
    "\n",
    "Each of these techniques helps to stabilize the training of RNNs, ensuring that the gradients remain in a manageable range throughout the sequence, which is essential for learning effective representations from long sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dffcef7",
   "metadata": {},
   "source": [
    "## Greedy And Beam Search - Decoding Strategies\n",
    "\n",
    "When generating sequences, such as in machine translation or text generation, selecting the right decoding strategy is crucial for generating high-quality outputs. Two common strategies are **Greedy Decoding** and **Beam Search**. Both approaches aim to determine the best sequence of words based on the model's probability estimates, but they differ in how they explore the space of possible sequences.\n",
    "\n",
    "### Greedy Decoding\n",
    "\n",
    "**Greedy decoding** is the simplest method for sequence generation. \n",
    "\n",
    "It operates by making a series of local, one-step optimal decisions. At each time step, the algorithm looks at the probability distribution over the next possible words and selects the one with the highest probability. This decision is made without considering how the choice might influence future selections.\n",
    "\n",
    "Greedy decoding effectively builds a search tree where only one branch is followed — the one corresponding to the highest probability word at each step.\n",
    "\n",
    "Imagine a search tree where each node represents a possible word in the sequence. Greedy decoding only follows one branch — the one that seems best at the current step — ignoring all alternative paths that might lead to a better overall sequence. \n",
    "\n",
    "While this method is computationally efficient and straightforward to implement, its narrow focus on the immediate best choice often leads to outputs that are predictable and lack variety. In many cases, this leads to repetitive sequences that do not capture the full richness of the language.\n",
    "\n",
    "Due to its limitations, greedy decoding is not commonly used in practice for tasks that require nuanced or varied outputs, although it can be useful in simpler or more constrained settings.\n",
    "\n",
    "\n",
    "### Beam Search\n",
    "\n",
    "**Beam search** is a more sophisticated decoding strategy that addresses some of the limitations of greedy decoding.\n",
    "\n",
    "Instead of choosing only the best word at each time step, beam search keeps track of the top **K** candidate sequences, where **K** is known as the beam width. This allows the algorithm to explore several possible sequences simultaneously.\n",
    "\n",
    "The beam width (typically set between 5 and 10 in practice) controls the number of hypotheses maintained at each step. A larger beam width allows for a more exhaustive search, increasing the likelihood of finding a better overall sequence, but at the cost of increased computational complexity.\n",
    "\n",
    "#### Process Overview:\n",
    "  1. **Initialization:**  \n",
    "     Start with an initial token (often a start-of-sequence token) and initialize the beam with this starting point.\n",
    "  2. **Expansion:**  \n",
    "     At each time step, expand all candidate sequences in the beam by appending all possible next words and computing their cumulative probabilities.\n",
    "  3. **Pruning:**  \n",
    "     Retain only the top **K** sequences based on their cumulative probabilities.\n",
    "  4. **Termination:**  \n",
    "     Continue the expansion and pruning steps until all sequences in the beam reach an end-of-sequence token or a predetermined maximum length.\n",
    "\n",
    "#### Application:\n",
    "  Beam search is particularly popular in tasks like machine translation (e.g., translating English to German) where generating a grammatically coherent and contextually accurate sentence is critical. By considering multiple candidate sequences, beam search often produces more natural and varied outputs compared to greedy decoding.\n",
    "\n",
    "### Summary\n",
    "\n",
    "Both greedy decoding and beam search are used to generate sequences from probabilistic models:\n",
    "- **Greedy Decoding:**  \n",
    "  - Fast and simple.\n",
    "  - Locally optimal but often suboptimal overall.\n",
    "  - Tends to produce generic and repetitive outputs.\n",
    "- **Beam Search:**  \n",
    "  - More computationally expensive but explores multiple candidate sequences.\n",
    "  - Achieves a better balance between quality and diversity in the output.\n",
    "  - Commonly used in complex sequence generation tasks like machine translation.\n",
    "\n",
    "Selecting the appropriate decoding strategy depends on the specific task requirements and the trade-off between computational resources and the quality of generated sequences."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d192198a",
   "metadata": {},
   "source": [
    "![](https://heidloff.net/assets/img/2023/08/greedy-beam.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96afd57d",
   "metadata": {},
   "source": [
    "## Advanced Sampling Strategies\n",
    "\n",
    "Always selecting the highest probability word at every step (as in greedy decoding) typically produces sentences that are grammatically correct but can be overly predictable and repetitive. \n",
    "\n",
    "This occurs because the model continually picks the most common choices, leading to generic output. In many applications — such as creative text generation or dialogue systems — more interesting outputs are desired. To achieve this, advanced sampling strategies introduce controlled randomness to balance between quality and diversity. \n",
    "\n",
    "Two important factors come into play:\n",
    "\n",
    "- **Quality:** Favoring high-probability words to maintain coherence.\n",
    "- **Diversity:** Allowing less probable words a chance to be selected, enriching the output with variety.\n",
    "\n",
    "### Top-K Sampling\n",
    "\n",
    "Top-K sampling is a straightforward extension of greedy decoding. Instead of selecting only the single highest probability word, the algorithm proceeds as follows:\n",
    "\n",
    "1. **Candidate Selection:**  \n",
    "   At each time step, pre-select the top **k** most likely words based on the model's probability distribution.\n",
    "\n",
    "2. **Truncation and Renormalization:**  \n",
    "   Truncate the full probability distribution to include only these top-k candidates, then renormalize the probabilities so that they sum to one.\n",
    "\n",
    "3. **Random Sampling:**  \n",
    "   Randomly sample the next word from this reduced distribution according to the renormalized probabilities.\n",
    "\n",
    "This strategy strikes a balance between maintaining a high quality (by limiting choices to the most likely words) and introducing diversity (by allowing a random selection among them). \n",
    "\n",
    "However, because **k** is fixed, its effectiveness can vary depending on the nature of the original probability distribution. In some cases, the distribution might be very peaked, so the top-k choices cover nearly all the probability mass; in other cases, a flat distribution means that even the top-k words represent only a small fraction of the total mass.\n",
    "\n",
    "### Top-P Sampling (Nucleus Sampling)\n",
    "\n",
    "Top-P sampling, also known as nucleus sampling, dynamically adjusts the candidate pool based on the cumulative probability rather than a fixed number of words. \n",
    "\n",
    "The process is as follows:\n",
    "\n",
    "1. **Cumulative Probability Threshold:**  \n",
    "   At each step, sort all candidate words by their probability and select the smallest set of words whose cumulative probability exceeds a predetermined threshold **p**.\n",
    "\n",
    "2. **Renormalization and Sampling:**  \n",
    "   Renormalize this subset of candidates to form a valid probability distribution and sample the next word from this nucleus.\n",
    "\n",
    "By focusing on a threshold rather than a fixed count, top-P sampling adapts to the probability distribution's shape. For peaked distributions, the nucleus might be very small, while for flatter distributions, it will be larger. This flexibility ensures that only the most contextually relevant words are considered, balancing coherence with the potential for creative variations.\n",
    "\n",
    "### Temperature Sampling\n",
    "\n",
    "Temperature sampling modifies the overall probability distribution by scaling the logits (pre-softmax scores) before applying the softmax function. \n",
    "\n",
    "The process involves:\n",
    "\n",
    "1. **Logit Scaling:**  \n",
    "   Divide the logits by a temperature factor before computing the softmax. This adjustment reshapes the probability distribution.\n",
    "\n",
    "2. **Effect of Temperature:**  \n",
    "   - **Low Temperature (< 1):**  \n",
    "     The distribution becomes sharper; high-probability words become even more dominant, reducing randomness. This tends to favor high-quality, coherent outputs but can result in repetitive text.\n",
    "   - **High Temperature (> 1):**  \n",
    "     The distribution flattens, increasing the chance of selecting lower-probability words, which introduces more diversity and creativity into the generated text.\n",
    "\n",
    "Temperature sampling allows for a smooth trade-off between quality and diversity without discarding any part of the original probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c48fb5",
   "metadata": {},
   "source": [
    "## Sampling Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604bda48",
   "metadata": {},
   "source": [
    "### TensorFlow Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3b3630",
   "metadata": {},
   "source": [
    "#### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbd756de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431500f1",
   "metadata": {},
   "source": [
    "#### Top-K Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bfc2c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_top_k_tf(logits, k=10):\n",
    "    \"\"\"\n",
    "    Performs top-K sampling:\n",
    "      1. Select the top k logits.\n",
    "      2. Mask out others by setting them to a very low value.\n",
    "      3. Renormalize and sample.\n",
    "    \"\"\"\n",
    "    \n",
    "    logits = tf.convert_to_tensor(logits)\n",
    "    topk = tf.math.top_k(logits, k=k)\n",
    "    topk_logits = topk.values\n",
    "    topk_indices = topk.indices\n",
    "\n",
    "    # Create a mask for the top-k values\n",
    "    full_mask = tf.fill(tf.shape(logits), float('-inf'))\n",
    "    mask = tf.tensor_scatter_nd_update(full_mask, tf.expand_dims(topk_indices, 1), topk_logits)\n",
    "    probs = tf.nn.softmax(mask)\n",
    "    sample = tf.random.categorical(tf.math.log([probs]), num_samples=1)\n",
    "    \n",
    "    return tf.gather(tf.range(tf.shape(logits)[0]), tf.squeeze(sample, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55eedf81",
   "metadata": {},
   "source": [
    "#### Top-P / Nucleus Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8542e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_top_p_tf(logits, p=0.9):\n",
    "    \"\"\"\n",
    "    Performs top-P (nucleus) sampling:\n",
    "      1. Sort logits and compute probabilities.\n",
    "      2. Determine the minimal set where cumulative probability exceeds p.\n",
    "      3. Mask out others, renormalize, and sample.\n",
    "    \"\"\"\n",
    "    \n",
    "    logits = tf.convert_to_tensor(logits)\n",
    "    \n",
    "    # Sort logits in descending order\n",
    "    sorted_logits, sorted_indices = tf.math.top_k(logits, k=tf.shape(logits)[0])\n",
    "    sorted_probs = tf.nn.softmax(sorted_logits)\n",
    "    cumulative_probs = tf.math.cumsum(sorted_probs)\n",
    "    \n",
    "    # Create a mask: keep tokens where cumulative probability is less than p\n",
    "    mask = cumulative_probs <= p\n",
    "    \n",
    "    # Ensure at least one token is kept\n",
    "    mask = tf.concat([[True], mask[1:]], axis=0)\n",
    "    \n",
    "    # Set logits for tokens not in the nucleus to a very low value\n",
    "    masked_logits = tf.where(mask, sorted_logits, tf.fill(tf.shape(sorted_logits), float('-inf')))\n",
    "    new_probs = tf.nn.softmax(masked_logits)\n",
    "    sample = tf.random.categorical(tf.math.log([new_probs]), num_samples=1)\n",
    "    \n",
    "    # Map back to original indices\n",
    "    return tf.gather(sorted_indices, tf.squeeze(sample, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f56d707",
   "metadata": {},
   "source": [
    "#### Temperature Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6387fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_with_temperature_tf(logits, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Scale logits by temperature and sample one token.\n",
    "    \"\"\"\n",
    "    \n",
    "    scaled_logits = logits / temperature\n",
    "    probs = tf.nn.softmax(scaled_logits)\n",
    "    \n",
    "    # tf.random.categorical expects a 2D tensor; we expand dims and squeeze the output\n",
    "    sample = tf.random.categorical(tf.math.log([probs]), num_samples=1)\n",
    "    return tf.squeeze(sample, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e653f671",
   "metadata": {},
   "source": [
    "#### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6384ceef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature Sampled token index (TF): [20]\n",
      "Top-K Sampled token index (TF): [27]\n",
      "Top-P Sampled token index (TF): [43]\n"
     ]
    }
   ],
   "source": [
    "# Example usage with dummy logits\n",
    "vocab_size = 50\n",
    "dummy_logits_tf = tf.random.normal([vocab_size])  # Example logits for 50 tokens\n",
    "\n",
    "# Temperature sampling example\n",
    "temp_sample_tf = sample_with_temperature_tf(dummy_logits_tf, temperature=0.8)\n",
    "print(\"Temperature Sampled token index (TF):\", temp_sample_tf.numpy())\n",
    "\n",
    "# Top-K sampling example\n",
    "topk_sample_tf = sample_top_k_tf(dummy_logits_tf, k=10)\n",
    "print(\"Top-K Sampled token index (TF):\", topk_sample_tf.numpy())\n",
    "\n",
    "# Top-P sampling example\n",
    "topp_sample_tf = sample_top_p_tf(dummy_logits_tf, p=0.9)\n",
    "print(\"Top-P Sampled token index (TF):\", topp_sample_tf.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0644753b",
   "metadata": {},
   "source": [
    "### PyTorch Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724f01e9",
   "metadata": {},
   "source": [
    "#### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e958e5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463b40e1",
   "metadata": {},
   "source": [
    "#### Top-K Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0983b5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_top_k(logits, k=10):\n",
    "    \"\"\"\n",
    "    Performs top-K sampling:\n",
    "      1. Select the top k logits.\n",
    "      2. Mask out the rest (set to -infinity).\n",
    "      3. Renormalize and sample.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the top k logits and their indices\n",
    "    topk_logits, topk_indices = torch.topk(logits, k)\n",
    "    \n",
    "    # Create a mask that sets values not in top-k to -infinity\n",
    "    mask = torch.full_like(logits, float('-inf'))\n",
    "    mask[topk_indices] = topk_logits\n",
    "    \n",
    "    # Apply softmax to get a valid probability distribution\n",
    "    probs = F.softmax(mask, dim=0)\n",
    "    \n",
    "    return torch.multinomial(probs, num_samples=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60c636f",
   "metadata": {},
   "source": [
    "#### Top-P / Nucleus Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04feb229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_top_p(logits, p=0.9):\n",
    "    \"\"\"\n",
    "    Performs top-P (nucleus) sampling:\n",
    "      1. Sort logits and compute softmax probabilities.\n",
    "      2. Determine the minimal set of tokens where the cumulative probability exceeds p.\n",
    "      3. Mask out tokens outside this set, renormalize, and sample.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sort logits in descending order\n",
    "    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "    sorted_probs = F.softmax(sorted_logits, dim=0)\n",
    "    \n",
    "    # Compute cumulative probabilities\n",
    "    cumulative_probs = torch.cumsum(sorted_probs, dim=0)\n",
    "    \n",
    "    # Determine which tokens to keep (ensure at least one token is kept)\n",
    "    cutoff = cumulative_probs > p\n",
    "    cutoff[0] = False  # Always keep the top token\n",
    "    \n",
    "    # Mask out tokens beyond the nucleus\n",
    "    sorted_logits[cutoff] = float('-inf')\n",
    "    \n",
    "    # Renormalize the masked logits\n",
    "    probs = F.softmax(sorted_logits, dim=0)\n",
    "    sample = torch.multinomial(probs, num_samples=1)\n",
    "    \n",
    "    # Map the sample back to the original indices\n",
    "    return sorted_indices[sample]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fd48aa",
   "metadata": {},
   "source": [
    "#### Temperature Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4976c99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_with_temperature(logits, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Scale logits by temperature and sample one token.\n",
    "    Lower temperatures (<1) sharpen the distribution, higher temperatures (>1) flatten it.\n",
    "    \"\"\"\n",
    "    scaled_logits = logits / temperature\n",
    "    probs = F.softmax(scaled_logits, dim=0)\n",
    "    # Multinomial sampling from the probability distribution\n",
    "    return torch.multinomial(probs, num_samples=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c395879",
   "metadata": {},
   "source": [
    "#### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c7166f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage with dummy logits\n",
    "vocab_size = 50\n",
    "dummy_logits = torch.randn(vocab_size)  # Example logits for 50 tokens\n",
    "\n",
    "# Temperature sampling example\n",
    "temp_sample = sample_with_temperature(dummy_logits, temperature=0.8)\n",
    "print(\"Temperature Sampled token index:\", temp_sample.item())\n",
    "\n",
    "# Top-K sampling example\n",
    "topk_sample = sample_top_k(dummy_logits, k=10)\n",
    "print(\"Top-K Sampled token index:\", topk_sample.item())\n",
    "\n",
    "# Top-P sampling example\n",
    "topp_sample = sample_top_p(dummy_logits, p=0.9)\n",
    "print(\"Top-P Sampled token index:\", topp_sample.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5136637d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6c30e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
