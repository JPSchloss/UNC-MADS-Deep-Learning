{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63406632",
   "metadata": {},
   "source": [
    "# Batch Layer Norms and Residual Connections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d6da02",
   "metadata": {},
   "source": [
    "Deep neural networks are highly capable but can sometimes be difficult to train due to issues like vanishing or exploding gradients and intense training resources. To address these challenges, several techniques have been developed that help stabilize and accelerate training. Some methods include batch normalization, layer normalization, and residual connections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a66a8eb",
   "metadata": {},
   "source": [
    "![](Batch_Layer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84891ea7",
   "metadata": {},
   "source": [
    "## Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe82184",
   "metadata": {},
   "source": [
    "Batch normalization was introduced to improve the training of deep networks by controlling the distribution of layer outputs. \n",
    "\n",
    "In this technique, the activations from a layer are normalized using the statistics computed from a mini-batch. For each mini-batch, the mean and variance of the activations are calculated, and then each activation is normalized by subtracting the mean and dividing by the standard deviation (with a small constant added for stability). After normalization, learnable scale and shift parameters are applied, allowing the network to recover the original representations if necessary. \n",
    "\n",
    "This approach can help the network converge faster and may permit the use of higher learning rates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9de5cde",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/v2/resize:fit:898/0*pSSzicm1IH4hXOHc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaa6def",
   "metadata": {},
   "source": [
    "### The Process\n",
    "\n",
    "In batch normalization, we adjust the activations within a mini-batch so that they have a mean of zero and a variance of one. Suppose you have a mini-batch containing $m$ examples, and you consider a particular activation $x$ (which could be a scalar value from a feature map or fully connected layer). The steps are as follows:\n",
    "\n",
    "1. **Compute the Mean:**  \n",
    "   For the mini-batch, the mean $\\mu_B$ is calculated by summing all the values and dividing by the number of samples:\n",
    "   $$\n",
    "   \\mu_B = \\frac{1}{m} \\sum_{i=1}^{m} x^{(i)}.\n",
    "   $$\n",
    "   This average value represents the central tendency of the activations within the batch.\n",
    "\n",
    "2. **Compute the Variance:**  \n",
    "   Next, the variance $\\sigma_B^2$ measures how much the activations vary around the mean:\n",
    "   $$\n",
    "   \\sigma_B^2 = \\frac{1}{m} \\sum_{i=1}^{m} \\left( x^{(i)} - \\mu_B \\right)^2.\n",
    "   $$\n",
    "   The variance gives us an idea of the spread or dispersion of the activation values.\n",
    "\n",
    "3. **Normalize the Activations:**  \n",
    "   Each activation is normalized by subtracting the mean and dividing by the square root of the variance (plus a small constant $\\epsilon$ for numerical stability). This yields:\n",
    "   $$\n",
    "   \\hat{x}^{(i)} = \\frac{x^{(i)} - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}.\n",
    "   $$\n",
    "   Here, the term $\\sqrt{\\sigma_B^2 + \\epsilon}$ ensures that even if the variance is very small, we avoid division by zero.\n",
    "\n",
    "4. **Apply Learnable Scaling and Shifting:**  \n",
    "   After normalization, the network applies an affine transformation with parameters $\\gamma$ (for scaling) and $\\beta$ (for shifting):\n",
    "   $$\n",
    "   y^{(i)} = \\gamma \\hat{x}^{(i)} + \\beta.\n",
    "   $$\n",
    "   This learnable transformation enables the network to recover the original representation if that is optimal, rather than forcing the activations to have zero mean and unit variance at all times. Additionally, during inference, running estimates of the batch statistics are used to maintain consistency. This series of steps not only makes the optimization landscape smoother but also helps maintain healthy gradients during training, contributing to faster and more stable convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a02862",
   "metadata": {},
   "source": [
    "### Benefits\n",
    "\n",
    "1. **Smoothes the Optimization Landscape:**  \n",
    "Batch normalization reduces the variability of activations across different mini-batches by ensuring that each mini-batch has a consistent mean and variance. This standardization smoothes the loss surface, making the optimization process more predictable and stable. With a smoother landscape, gradient descent can take larger steps—meaning a higher learning rate—without overshooting the minimum, which leads to faster convergence.\n",
    "\n",
    "2. **Uniformity Across Examples:**  \n",
    "By normalizing the activations within a batch, batch normalization makes the examples more uniform in terms of their statistical properties. This uniformity prevents certain samples with extreme values from dominating the gradient updates, resulting in a more balanced learning process. With reduced internal variability, the network can safely use a larger learning rate, which in turn accelerates training.\n",
    "\n",
    "3. **Regularization Through Noise:**  \n",
    "The use of mini-batch statistics introduces a degree of randomness (or noise) during training. This noise serves as an implicit regularizer, discouraging the model from overfitting to the training data. The stochastic variations in the normalization process help the network to find a more robust solution, often resulting in better generalization and improved performance on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c28402",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0d81e9",
   "metadata": {},
   "source": [
    "#### TensorFlow Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6692c848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow BatchNorm Output:\n",
      " tf.Tensor(\n",
      "[[-0.5078056 ]\n",
      " [ 0.9584242 ]\n",
      " [-0.12232404]\n",
      " [ 0.19608045]\n",
      " [-2.4165165 ]], shape=(5, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class SimpleNetBN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(SimpleNetBN, self).__init__()\n",
    "        self.fc1 = tf.keras.layers.Dense(20)\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.relu = tf.keras.layers.ReLU()\n",
    "        self.fc2 = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, x, training=False):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x, training=training)  # Batch normalization applied here\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create network and sample input\n",
    "net_bn_tf = SimpleNetBN()\n",
    "sample_input_tf = tf.random.normal([5, 10])  # Batch size of 5, input dimension of 10\n",
    "output_bn_tf = net_bn_tf(sample_input_tf, training=True)\n",
    "print(\"TensorFlow BatchNorm Output:\\n\", output_bn_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944a06d2",
   "metadata": {},
   "source": [
    "#### PyTorch Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2787b5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleNetBN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNetBN, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 20)\n",
    "        self.bn1 = nn.BatchNorm1d(20)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(20, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)  # Batch normalization applied here\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create network and sample input\n",
    "net_bn = SimpleNetBN()\n",
    "sample_input = torch.randn(5, 10)  # Batch size of 5, input dimension of 10\n",
    "output_bn = net_bn(sample_input)\n",
    "print(\"PyTorch BatchNorm Output:\\n\", output_bn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d443a95b",
   "metadata": {},
   "source": [
    "### Real World Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc03823e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 03:08:19.877338: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training Model with Batch Normalization ===\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 26, 26, 32)       128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 24, 24, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 24, 24, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 12, 12, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 9216)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               1179776   \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 128)              512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,200,778\n",
      "Trainable params: 1,200,330\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 03:09:12.863341: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "422/422 [==============================] - 76s 177ms/step - loss: 0.1078 - accuracy: 0.9670 - val_loss: 1.3627 - val_accuracy: 0.6860\n",
      "Epoch 2/5\n",
      "422/422 [==============================] - 76s 181ms/step - loss: 0.0315 - accuracy: 0.9911 - val_loss: 0.0406 - val_accuracy: 0.9897\n",
      "Epoch 3/5\n",
      "422/422 [==============================] - 85s 202ms/step - loss: 0.0174 - accuracy: 0.9951 - val_loss: 0.0382 - val_accuracy: 0.9888\n",
      "Epoch 4/5\n",
      "422/422 [==============================] - 134s 318ms/step - loss: 0.0101 - accuracy: 0.9972 - val_loss: 0.0327 - val_accuracy: 0.9912\n",
      "Epoch 5/5\n",
      "422/422 [==============================] - 86s 204ms/step - loss: 0.0073 - accuracy: 0.9981 - val_loss: 0.0463 - val_accuracy: 0.9888\n",
      "Test evaluation:\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 0.0364 - accuracy: 0.9894\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0363968163728714, 0.9894000291824341]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize pixel values to [0, 1]\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test  = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Expand dimensions to add the channel axis (MNIST is grayscale)\n",
    "x_train = x_train[..., tf.newaxis]\n",
    "x_test  = x_test[..., tf.newaxis]\n",
    "\n",
    "def create_model_batch_norm():\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "        layers.BatchNormalization(),  # Normalize across mini-batch\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model_bn = create_model_batch_norm()\n",
    "model_bn.compile(optimizer='adam',\n",
    "                 loss='sparse_categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "print(\"=== Training Model with Batch Normalization ===\")\n",
    "model_bn.summary()\n",
    "model_bn.fit(x_train, y_train, epochs=5, batch_size=128, validation_split=0.1)\n",
    "\n",
    "print(\"Test evaluation:\")\n",
    "model_bn.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef720cb",
   "metadata": {},
   "source": [
    "## Layer Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d66743",
   "metadata": {},
   "source": [
    "Layer normalization is another normalization technique that is particularly useful in scenarios where batch sizes are small or variable, such as in recurrent neural networks or transformer architectures. \n",
    "\n",
    "Unlike batch normalization, which computes normalization statistics across the mini-batch, layer normalization computes the mean and variance across the features of each individual sample. This per-sample normalization makes the technique robust to changes in batch size and is well-suited for sequential data. \n",
    "\n",
    "The process involves normalizing each sample by subtracting the mean and dividing by the standard deviation calculated over its features, followed by the application of learnable scaling and shifting parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7322f6",
   "metadata": {},
   "source": [
    "![](https://theaisummer.com/static/ac89fbcf1c115f07ae68af695c28c4a0/ee604/normalization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee8c53d",
   "metadata": {},
   "source": [
    "### The Process\n",
    "\n",
    "Layer normalization operates on a per-sample basis by normalizing the features within a single data point rather than across a mini-batch. Suppose you have an input vector $x \\in \\mathbb{R}^d$ for a single sample, where $d$ is the number of features. The process involves:\n",
    "\n",
    "1. **Compute the Mean Across Features:**  \n",
    "   For the input sample $x$, calculate the mean $\\mu$ of all its features:\n",
    "   $$\n",
    "   \\mu = \\frac{1}{d} \\sum_{j=1}^{d} x_j.\n",
    "   $$\n",
    "   This mean represents the average feature value for that specific sample.\n",
    "\n",
    "2. **Compute the Variance Across Features:**  \n",
    "   Next, determine the variance $\\sigma^2$ to measure the spread of the features:\n",
    "   $$\n",
    "   \\sigma^2 = \\frac{1}{d} \\sum_{j=1}^{d} \\left( x_j - \\mu \\right)^2.\n",
    "   $$\n",
    "   This variance tells us how much the feature values deviate from the mean.\n",
    "\n",
    "3. **Normalize the Features:**  \n",
    "   Each feature $x_j$ is then normalized by subtracting the mean and dividing by the standard deviation:\n",
    "   $$\n",
    "   \\hat{x}_j = \\frac{x_j - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}},\n",
    "   $$\n",
    "   where $\\epsilon$ is a small constant added to prevent division by zero. This ensures that each sample has features with zero mean and unit variance, regardless of the batch size.\n",
    "\n",
    "4. **Apply Learnable Scaling and Shifting:**  \n",
    "   Finally, similar to batch normalization, layer normalization uses learnable parameters $\\gamma_j$ and $\\beta_j$ for each feature:\n",
    "   $$\n",
    "   y_j = \\gamma_j \\hat{x}_j + \\beta_j.\n",
    "   $$\n",
    "   This step allows each feature to be scaled and shifted independently, so the network can adjust the normalized values as needed for optimal performance.\n",
    "\n",
    "Since layer normalization works on each individual sample, it is especially useful in scenarios where the batch size is small or even variable (for example, in recurrent neural networks or transformer models), ensuring that the dynamic range of features remains consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0893df",
   "metadata": {},
   "source": [
    "### Benefits\n",
    "\n",
    "1. **Stable Inputs Across Layers:**  \n",
    "Layer normalization normalizes the features of each individual sample, ensuring that the inputs to each layer remain on a consistent scale. By doing so, it prevents the activations from exploding or vanishing as they pass through the network. This stability is crucial for maintaining healthy gradients and ensuring that every layer receives inputs that are properly scaled, which facilitates the learning of deep representations.\n",
    "\n",
    "2. **Independence from Batch Size:**  \n",
    "Unlike batch normalization, layer normalization computes statistics based solely on the features of a single sample. This independence means that its performance is not affected by the size of the mini-batch, making it especially effective in scenarios where batch sizes are small or variable—such as in recurrent neural networks or transformer models. This property simplifies model design and training, as the normalization behavior remains consistent regardless of the batch size.\n",
    "\n",
    "3. **Consistent Behavior During Training and Testing:**  \n",
    "Since layer normalization normalizes each sample independently, the exact same procedure is applied during both training and inference. There is no need for moving averages or separate inference rules, which are required by batch normalization to handle different statistics in testing. This consistency eliminates any discrepancy between the training and testing phases, making the model's behavior more predictable and easier to debug."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a2ad90",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb26e833",
   "metadata": {},
   "source": [
    "#### TensorFlow Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8a7df4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow LayerNorm Output:\n",
      " tf.Tensor(\n",
      "[[-0.8342132 ]\n",
      " [-1.8368168 ]\n",
      " [-0.6121157 ]\n",
      " [-0.70810634]\n",
      " [-1.4149286 ]], shape=(5, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class SimpleNetLN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(SimpleNetLN, self).__init__()\n",
    "        self.fc1 = tf.keras.layers.Dense(20)\n",
    "        self.ln1 = tf.keras.layers.LayerNormalization()\n",
    "        self.relu = tf.keras.layers.ReLU()\n",
    "        self.fc2 = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.ln1(x)  # Layer normalization applied here\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create network and sample input\n",
    "net_ln_tf = SimpleNetLN()\n",
    "sample_input_tf = tf.random.normal([5, 10])\n",
    "output_ln_tf = net_ln_tf(sample_input_tf)\n",
    "print(\"TensorFlow LayerNorm Output:\\n\", output_ln_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c2a3af",
   "metadata": {},
   "source": [
    "#### PyTorch Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304dcaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleNetLN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNetLN, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 20)\n",
    "        self.ln1 = nn.LayerNorm(20)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(20, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.ln1(x)  # Layer normalization applied here\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create network and sample input\n",
    "net_ln = SimpleNetLN()\n",
    "sample_input = torch.randn(5, 10)\n",
    "output_ln = net_ln(sample_input)\n",
    "print(\"PyTorch LayerNorm Output:\\n\", output_ln)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ba919f",
   "metadata": {},
   "source": [
    "### Real World Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd8a64f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training Model with Layer Normalization ===\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " layer_normalization (LayerN  (None, 26, 26, 32)       64        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 24, 24, 64)        18496     \n",
      "                                                                 \n",
      " layer_normalization_1 (Laye  (None, 24, 24, 64)       128       \n",
      " rNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 12, 12, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 9216)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               1179776   \n",
      "                                                                 \n",
      " layer_normalization_2 (Laye  (None, 128)              256       \n",
      " rNormalization)                                                 \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,200,330\n",
      "Trainable params: 1,200,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "422/422 [==============================] - 169s 396ms/step - loss: 0.1270 - accuracy: 0.9628 - val_loss: 0.0451 - val_accuracy: 0.9865\n",
      "Epoch 2/5\n",
      "422/422 [==============================] - 132s 314ms/step - loss: 0.0300 - accuracy: 0.9914 - val_loss: 0.0413 - val_accuracy: 0.9893\n",
      "Epoch 3/5\n",
      "422/422 [==============================] - 116s 274ms/step - loss: 0.0160 - accuracy: 0.9954 - val_loss: 0.0458 - val_accuracy: 0.9878\n",
      "Epoch 4/5\n",
      "422/422 [==============================] - 117s 277ms/step - loss: 0.0089 - accuracy: 0.9976 - val_loss: 0.0363 - val_accuracy: 0.9905\n",
      "Epoch 5/5\n",
      "422/422 [==============================] - 107s 254ms/step - loss: 0.0062 - accuracy: 0.9985 - val_loss: 0.0413 - val_accuracy: 0.9885\n",
      "Test evaluation:\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.0410 - accuracy: 0.9885\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.04103352874517441, 0.9884999990463257]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "# Normalize pixel values to [0, 1]\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test  = x_test.astype('float32') / 255.0\n",
    "# Expand dimensions to add the channel axis (MNIST is grayscale)\n",
    "x_train = x_train[..., tf.newaxis]\n",
    "x_test  = x_test[..., tf.newaxis]\n",
    "\n",
    "\n",
    "def create_model_layer_norm():\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "        layers.LayerNormalization(),  # Normalize across the features of each sample\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.LayerNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.LayerNormalization(),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model_ln = create_model_layer_norm()\n",
    "model_ln.compile(optimizer='adam',\n",
    "                 loss='sparse_categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "print(\"\\n=== Training Model with Layer Normalization ===\")\n",
    "model_ln.summary()\n",
    "model_ln.fit(x_train, y_train, epochs=5, batch_size=128, validation_split=0.1)\n",
    "\n",
    "print(\"Test evaluation:\")\n",
    "model_ln.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733bd5f3",
   "metadata": {},
   "source": [
    "## Residual Connections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22106ae",
   "metadata": {},
   "source": [
    "Training very deep neural networks can be challenging, partly due to the difficulty of propagating gradients through many layers. Residual connections were introduced as a solution to this problem by allowing the gradient to flow more directly through the network. \n",
    "\n",
    "The core idea is to add the original input of a block to its output after a series of transformations. This creates a shortcut that helps preserve the original signal and facilitates the learning of identity mappings, if necessary. By effectively “skipping” layers, residual connections help prevent degradation in performance as the network depth increases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f8eeab",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/v2/resize:fit:1122/1*RTYKpn1Vqr-8zT5fqa8-jA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf321a6",
   "metadata": {},
   "source": [
    "### The Process\n",
    "\n",
    "Residual connections offer a strategy to ease the training of very deep neural networks by introducing shortcut paths that allow the gradient to flow more directly. Here’s a more detailed breakdown:\n",
    "\n",
    "1. **Learning a Residual Function:**  \n",
    "   In a conventional network block, one might aim to learn a direct mapping $H(x)$ from the input $x$ to the output. With residual connections, the block instead learns a residual function $F(x)$ such that:\n",
    "   $$\n",
    "   F(x) = H(x) - x.\n",
    "   $$\n",
    "   In other words, rather than learning the full transformation, the block learns the difference between the desired transformation and the identity function.\n",
    "\n",
    "2. **Combining the Input with the Residual:**  \n",
    "   The output of the residual block is then given by:\n",
    "   $$\n",
    "   y = F(x) + x.\n",
    "   $$\n",
    "   This simple addition means that if the optimal transformation is close to the identity function (i.e., $H(x) \\approx x$), the residual function $F(x)$ can easily learn to output values near zero, thereby preserving the original input.\n",
    "\n",
    "3. **Benefits for Gradient Flow:**  \n",
    "   During backpropagation, gradients can pass through the addition operation with minimal modification. This is because the derivative of the addition is one, which prevents the gradients from becoming too small (a phenomenon known as vanishing gradients). As a result, the network can be trained deeper without suffering from degradation in performance.\n",
    "\n",
    "4. **Facilitating Identity Mappings:**  \n",
    "   If, in any layer, the best function to learn is simply the identity (i.e., no change to the input), residual connections make this easy. The network can set $F(x)$ to zero (or near zero) without any special architectural modifications, allowing the original input to pass unchanged through the block.\n",
    "\n",
    "This approach has proven especially effective in architectures like ResNet, where networks with hundreds of layers have been successfully trained, largely thanks to the enhanced gradient flow provided by residual connections.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d68798",
   "metadata": {},
   "source": [
    "### Benefits\n",
    "\n",
    "1. **Expanded Representational Capacity:**  \n",
    "Residual connections allow a network to learn residual functions—i.e., the differences between the desired transformation and the identity function. This design makes it easier for the network to represent complex functions because the layers can focus on learning the modifications necessary to improve the input rather than learning the entire transformation from scratch. Consequently, the network can represent a broader range of functions, increasing its overall expressive power.\n",
    "\n",
    "2. **Prevention of Shattered Gradients:**  \n",
    "Deep networks often suffer from shattered gradients, where gradients become noisy and unstable as they propagate back through many layers. Residual connections provide shortcut paths that allow gradients to bypass multiple layers, maintaining their strength and consistency. By preserving the gradient signal, residual connections prevent the degradation of the gradient during backpropagation, which is critical for the successful training of very deep networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bec3278",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57ae501",
   "metadata": {},
   "source": [
    "#### TensorFlow Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "574c0253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Residual Block Output:\n",
      " tf.Tensor(\n",
      "[[0.         0.11795241 2.3836324  1.6010782  0.28818804 0.\n",
      "  0.05150962 0.         1.5185751  0.         0.         0.\n",
      "  0.44952118 1.3067663  0.         0.         0.         0.\n",
      "  1.2832165  0.6867311 ]\n",
      " [0.         1.9266136  0.         0.         2.7598815  0.5555755\n",
      "  0.19757074 0.         0.18135703 2.241429   0.56072474 1.8227056\n",
      "  0.         0.         0.         0.         0.59057254 0.\n",
      "  0.         1.3937647 ]\n",
      " [1.1942751  0.         0.4036159  2.1789103  0.         0.45983297\n",
      "  0.30273807 0.14427543 2.6960754  0.         0.         0.\n",
      "  0.24127856 0.5534748  0.         2.6060038  0.7564375  1.2216339\n",
      "  0.19340134 0.        ]\n",
      " [0.         0.69583654 0.         0.15185216 0.         0.31141677\n",
      "  2.115551   0.         0.4362759  0.         1.3639169  0.\n",
      "  0.         0.         1.3726174  0.4778955  1.6547519  1.867121\n",
      "  0.         0.        ]\n",
      " [1.1807977  0.5903413  1.3898568  2.0094955  0.27392915 1.4721\n",
      "  0.         0.28615826 0.         0.         1.3441072  0.21309097\n",
      "  0.         0.         0.47205922 0.         0.         0.63564503\n",
      "  0.         0.        ]], shape=(5, 20), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class ResidualBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, in_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.fc1 = tf.keras.layers.Dense(in_features)\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.relu = tf.keras.layers.ReLU()\n",
    "        self.fc2 = tf.keras.layers.Dense(in_features)\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "    \n",
    "    def call(self, x, training=False):\n",
    "        residual = x  # Save input for the shortcut connection\n",
    "        out = self.fc1(x)\n",
    "        out = self.bn1(out, training=training)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.bn2(out, training=training)\n",
    "        out = out + residual  # Add the shortcut connection\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "# Create residual block and sample input\n",
    "res_block_tf = ResidualBlock(in_features=20)\n",
    "sample_input_tf = tf.random.normal([5, 20])\n",
    "output_res_tf = res_block_tf(sample_input_tf, training=True)\n",
    "print(\"TensorFlow Residual Block Output:\\n\", output_res_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba05ac55",
   "metadata": {},
   "source": [
    "#### PyTorch Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3bc61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features, in_features)\n",
    "        self.bn1 = nn.BatchNorm1d(in_features)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(in_features, in_features)\n",
    "        self.bn2 = nn.BatchNorm1d(in_features)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x  # Save input for the shortcut connection\n",
    "        out = self.fc1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += residual  # Add the shortcut connection\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "# Create residual block and sample input\n",
    "res_block = ResidualBlock(in_features=20)\n",
    "sample_input = torch.randn(5, 20)\n",
    "output_res = res_block(sample_input)\n",
    "print(\"PyTorch Residual Block Output:\\n\", output_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd49314d",
   "metadata": {},
   "source": [
    "### Real World Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "652c7349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training Model with Residual Connections ===\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "                                                                 \n",
      " residual_block_1 (ResidualB  (None, 28, 28, 32)       18496     \n",
      " lock)                                                           \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 14, 14, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 14, 14, 64)        18496     \n",
      "                                                                 \n",
      " residual_block_2 (ResidualB  (None, 14, 14, 64)       73856     \n",
      " lock)                                                           \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 7, 7, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 3136)              0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 128)               401536    \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 513,994\n",
      "Trainable params: 513,994\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "422/422 [==============================] - 103s 243ms/step - loss: 0.1676 - accuracy: 0.9482 - val_loss: 0.0549 - val_accuracy: 0.9873\n",
      "Epoch 2/5\n",
      "422/422 [==============================] - 104s 246ms/step - loss: 0.0434 - accuracy: 0.9869 - val_loss: 0.0390 - val_accuracy: 0.9888\n",
      "Epoch 3/5\n",
      "422/422 [==============================] - 105s 249ms/step - loss: 0.0283 - accuracy: 0.9912 - val_loss: 0.0313 - val_accuracy: 0.9912\n",
      "Epoch 4/5\n",
      "422/422 [==============================] - 107s 253ms/step - loss: 0.0217 - accuracy: 0.9929 - val_loss: 0.0327 - val_accuracy: 0.9920\n",
      "Epoch 5/5\n",
      "422/422 [==============================] - 111s 262ms/step - loss: 0.0155 - accuracy: 0.9947 - val_loss: 0.0235 - val_accuracy: 0.9928\n",
      "Test evaluation:\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.0227 - accuracy: 0.9925\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.02266356721520424, 0.9925000071525574]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "# Normalize pixel values to [0, 1]\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test  = x_test.astype('float32') / 255.0\n",
    "# Expand dimensions to add the channel axis (MNIST is grayscale)\n",
    "x_train = x_train[..., tf.newaxis]\n",
    "x_test  = x_test[..., tf.newaxis]\n",
    "\n",
    "# Define a custom Residual Block\n",
    "class ResidualBlock(layers.Layer):\n",
    "    def __init__(self, filters, kernel_size=3):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = layers.Conv2D(filters, kernel_size, padding='same', activation='relu')\n",
    "        self.conv2 = layers.Conv2D(filters, kernel_size, padding='same', activation=None)\n",
    "        self.relu = layers.ReLU()\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.conv2(x)\n",
    "        x += inputs  # Add shortcut (residual connection)\n",
    "        return self.relu(x)\n",
    "\n",
    "\n",
    "def create_model_residual():\n",
    "    inputs = layers.Input(shape=(28, 28, 1))\n",
    "    # Initial convolution layer\n",
    "    x = layers.Conv2D(32, (3, 3), padding='same', activation='relu')(inputs)\n",
    "    \n",
    "    # First residual block\n",
    "    x = ResidualBlock(32)(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    # Second convolution and residual block\n",
    "    x = layers.Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = ResidualBlock(64)(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    outputs = layers.Dense(10, activation='softmax')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "model_res = create_model_residual()\n",
    "model_res.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "print(\"\\n=== Training Model with Residual Connections ===\")\n",
    "model_res.summary()\n",
    "model_res.fit(x_train, y_train, epochs=5, batch_size=128, validation_split=0.1)\n",
    "\n",
    "print(\"Test evaluation:\")\n",
    "model_res.evaluate(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669eafb7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
