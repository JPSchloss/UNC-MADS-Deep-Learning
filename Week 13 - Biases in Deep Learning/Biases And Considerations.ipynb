{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76873217",
   "metadata": {},
   "source": [
    "# Biases in Deep Learning\n",
    "\n",
    "Deep learning models, whether built on RNNs for sequential data, CNNs for image tasks, Transformers for language processing, or Generative AI for creative tasks, can have significant effects on real-world decision-making and carry inherent risks of propagating and even amplifying biases.\n",
    "\n",
    "When deep learning models are deployed, their decisions can influence areas such as hiring, loan approval, or law enforcement. For example, a resume screening system might filter out qualified candidates if its training data or optimization strategy embeds historical biases. \n",
    "\n",
    "This effect is not limited to one architecture; it spans across RNNs, CNNs, Transformers, and even Generative AI. Each model class has unique characteristics that interact with bias in different ways, whether through the structure of its training data or the constraints imposed by its architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71dc603",
   "metadata": {},
   "source": [
    "# Sources of Bias in Deep Learning Systems\n",
    "\n",
    "## Bias in Data and Sampling\n",
    "   Data is the foundation of any deep learning model, and any bias inherent in the dataset will likely be learned by the model. \n",
    "   \n",
    "In technical terms, data bias can stem from:\n",
    "\n",
    "   - **Sampling Bias:** When the training set does not accurately represent the target population, the model may perform poorly on underrepresented groups. For instance, a dataset for a resume screening tool might be dominated by one demographic.\n",
    "   - **Measurement Bias:** Errors in data collection can lead to systematic discrepancies. For example, if certain features are recorded inaccurately for specific groups, the model will learn these erroneous patterns.\n",
    "   - **Historical Bias:** Data collected from past decisions may reflect societal inequities. Even with a perfectly designed algorithm, historical imbalances (such as gender or racial disparities in hiring) can be encoded into the model.\n",
    "   \n",
    "Mitigation strategies at this stage include re-sampling techniques, data augmentation, and carefully curating balanced datasets. Pre-processing steps such as normalization and bias correction can also help ensure that the model receives a more equitable view of the world.\n",
    "\n",
    "\n",
    "## Optimizing Towards a Biased Objective\n",
    "   The objective function that guides model training plays a crucial role in determining outcomes. Often, models are optimized solely for predictive accuracy, which might inadvertently reward biased patterns:\n",
    "\n",
    "   - **Loss Function Limitations:** Traditional loss functions like cross-entropy or mean squared error focus on overall accuracy without accounting for fairness. A model might achieve high performance by favoring the majority class.\n",
    "   - **Regularization and Fairness Constraints:** Incorporating fairness as a regularization term or adding constraints to the optimization process can help align the model’s objective with ethical goals. Techniques such as multi-objective optimization allow for simultaneous consideration of accuracy and fairness.\n",
    "   - **Trade-offs:** There is often a trade-off between optimizing for performance and ensuring fairness. Technical methods such as Pareto optimization can help balance these competing objectives, although they require careful tuning and validation.\n",
    "\n",
    "## Inductive Bias\n",
    "   Every model comes with built-in assumptions, inductive bias, that shape how it generalizes from training data to unseen examples:\n",
    "\n",
    "   - **Architecture-Specific Biases:** For example, CNNs incorporate a bias towards spatial invariance, which is beneficial for image tasks but may overlook context-specific features. RNNs assume sequential dependencies that might not capture all nuances of language.\n",
    "   - **Feature Learning:** The process by which models learn features from data can itself introduce bias. If the learned representations emphasize certain patterns over others, the model might perform unevenly across different scenarios.\n",
    "   - **Model Complexity:** Highly complex models may overfit to biases present in the training data, while simpler models might lack the capacity to capture diverse patterns. Choosing the right model complexity is a technical challenge that directly impacts fairness.\n",
    "\n",
    "## Bias Amplification in Learned Models\n",
    "   Even subtle biases in the input data can be magnified during the training process:\n",
    "\n",
    "   - **Iterative Reinforcement:** Deep learning models iteratively adjust their parameters to minimize loss. In doing so, they might reinforce minor imbalances into significant disparities in output predictions.\n",
    "   - **Feedback Loops:** In systems where model outputs influence future training data (e.g., recommendation systems), initial biases can create self-perpetuating cycles that worsen over time.\n",
    "   - **Algorithmic Sensitivity:** Some models are more sensitive to small perturbations in the data distribution, leading to an amplification of bias. Analyzing model sensitivity and stability can help detect and counteract this effect.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afb6623",
   "metadata": {},
   "source": [
    "# Evaluating For Biases\n",
    "\n",
    "Before deploying a model, it is crucial to evaluate its performance not only in terms of accuracy but also in terms of addressing bias. This evaluation ensures that the model does not systematically disadvantage certain groups, thereby promoting ethical and equitable decision-making. Below are some strategies to assess and address bias:\n",
    "\n",
    "## Fairness Metrics\n",
    "Fairness metrics provide quantitative measures to evaluate the degree of bias present in your model’s predictions. Some common metrics include:\n",
    "\n",
    "- **Demographic Parity:**\n",
    "    - This metric checks whether the positive prediction rate is similar across different groups. In other words, if 30% of individuals in group A receive a positive prediction, then roughly 30% of individuals in group B should also receive a positive prediction.\n",
    "\n",
    "- **Equalized Odds:**\n",
    "    - Equalized odds require that both the true positive rates (TPR) and false positive rates (FPR) are equal across groups. This metric ensures that the model is equally accurate (and equally prone to error) for different groups.\n",
    "\n",
    "- **Disparate Impact:**\n",
    "    - Disparate impact measures the ratio of favorable outcomes between a protected group and a reference group. A common rule of thumb is that the ratio should be close to 1; a significantly lower value may indicate bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6669262b",
   "metadata": {},
   "source": [
    "### Examples\n",
    "\n",
    "#### Calculating Demographic Parity\n",
    "\n",
    "Below is a Python example using a simulated dataset. The code calculates the positive prediction rate for two sensitive groups. In this snippet, the average of the binary predictions (0 or 1) within each group represents the positive rate. Differences between these rates can signal potential bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9ea9c605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive rates by group:\n",
      "sensitive_group\n",
      "A    0.553571\n",
      "B    0.568182\n",
      "Name: predicted, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Simulate a dataset with predictions and a sensitive attribute (e.g., group membership)\n",
    "np.random.seed(42)\n",
    "df = pd.DataFrame({\n",
    "    'predicted': np.random.randint(0, 2, 100),\n",
    "    'sensitive_group': np.random.choice(['A', 'B'], 100)\n",
    "})\n",
    "\n",
    "# Calculate the positive prediction rate by sensitive group\n",
    "positive_rates = df.groupby('sensitive_group')['predicted'].mean()\n",
    "print(\"Positive rates by group:\")\n",
    "print(positive_rates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd2380e",
   "metadata": {},
   "source": [
    "#### Evaluating Equalized Odds\n",
    "\n",
    "If you have ground truth labels available, you can further examine metrics like the true positive rate (TPR) for each group. By comparing the TPR across groups, you can assess whether the model is equally effective at identifying positive cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a341de60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positive Rates by group:\n",
      "sensitive_group\n",
      "A    0.551724\n",
      "B    0.600000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Adding a simulated ground truth label\n",
    "df['true_label'] = np.random.randint(0, 2, 100)\n",
    "\n",
    "# Function to calculate true positive rate (TPR)\n",
    "def true_positive_rate(group):\n",
    "    true_positives = ((group['predicted'] == 1) & (group['true_label'] == 1)).sum()\n",
    "    actual_positives = (group['true_label'] == 1).sum()\n",
    "    return true_positives / actual_positives if actual_positives > 0 else 0\n",
    "\n",
    "tpr_by_group = df.groupby('sensitive_group').apply(true_positive_rate)\n",
    "print(\"True Positive Rates by group:\")\n",
    "print(tpr_by_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4405fdb5",
   "metadata": {},
   "source": [
    "## Subgroup Analysis\n",
    "\n",
    "Subgroup analysis involves dissecting model performance by analyzing different demographic or sensitive groups separately. This detailed error analysis helps identify if certain groups are systematically disadvantaged.\n",
    "\n",
    "**Steps for Subgroup Analysis:**\n",
    "\n",
    "- **Identify Sensitive Attributes:**\n",
    "    - Determine which demographic or sensitive factors (e.g., age, gender, ethnicity) are relevant for your analysis.\n",
    "\n",
    "- **Calculate Performance Metrics:**\n",
    "    - Evaluate metrics such as accuracy, precision, recall, and error rates within each subgroup.\n",
    "\n",
    "- **Error Analysis:**\n",
    "    - Investigate misclassification errors in each group. Look for patterns or systematic errors that might indicate bias.\n",
    "    \n",
    "### Examples\n",
    "\n",
    "#### Error Rate by Subgroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5e95e1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rates by group:\n",
      "sensitive_group\n",
      "A    0.500000\n",
      "B    0.477273\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate error rate for a group\n",
    "def error_rate(group):\n",
    "    errors = (group['predicted'] != group['true_label']).sum()\n",
    "    return errors / len(group)\n",
    "\n",
    "error_by_group = df.groupby('sensitive_group').apply(error_rate)\n",
    "print(\"Error rates by group:\")\n",
    "print(error_by_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfcc6fe",
   "metadata": {},
   "source": [
    "## Simulation and Stress Testing\n",
    "\n",
    "Simulation and stress testing involve creating scenarios to test how the model behaves under different conditions or in the presence of synthetic biases. This process can help reveal vulnerabilities that might not be evident in standard evaluation.\n",
    "\n",
    "**Key Approaches:**\n",
    "\n",
    "- **Synthetic Bias Injection:**\n",
    "    - Simulate bias by deliberately modifying the data for a specific subgroup. For example, you might add noise or flip a portion of the predictions for one group to see how performance metrics change.\n",
    "\n",
    "- **Adversarial Testing:**\n",
    "    - Test the model with adversarial examples that mimic real-world shifts in data distribution. This can help ensure that the model maintains fairness even under challenging conditions.\n",
    "\n",
    "### Examples\n",
    "\n",
    "#### Bias Injection and Stress Testing\n",
    "\n",
    "The following code demonstrates how to inject synthetic bias into a subgroup and then analyze the error rates. By comparing the performance metrics before and after bias injection, you can assess the model’s robustness and sensitivity to biased inputs. This type of stress testing is crucial for understanding how the model might perform in real-world scenarios where data distributions can shift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0b4e1477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rates by group after bias injection:\n",
      "sensitive_group\n",
      "A    0.500000\n",
      "B    0.454545\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Function to inject synthetic bias into a specified subgroup\n",
    "def inject_bias(df, subgroup, noise_level=0.3):\n",
    "    biased_df = df.copy()\n",
    "    mask = biased_df['sensitive_group'] == subgroup\n",
    "    # Flip the prediction with a given probability (noise level) for the specified subgroup\n",
    "    flip_mask = np.random.rand(mask.sum()) < noise_level\n",
    "    biased_df.loc[mask, 'predicted'] = biased_df.loc[mask, 'predicted'].where(~flip_mask, 1 - biased_df.loc[mask, 'predicted'])\n",
    "    return biased_df\n",
    "\n",
    "# Inject bias into group 'B'\n",
    "df_biased = inject_bias(df, subgroup='B', noise_level=0.3)\n",
    "\n",
    "# Recalculate error rates after bias injection\n",
    "error_by_group_biased = df_biased.groupby('sensitive_group').apply(error_rate)\n",
    "print(\"Error rates by group after bias injection:\")\n",
    "print(error_by_group_biased)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb36dc31",
   "metadata": {},
   "source": [
    "# Bias Mitigation Techniques\n",
    "\n",
    "When building machine learning models, it’s not enough to simply evaluate for biases—you must also actively mitigate them. Bias mitigation techniques can be applied at different stages of the machine learning pipeline: before, during, and after model training.\n",
    "\n",
    "## Pre-processing Techniques\n",
    "\n",
    "Pre-processing techniques focus on transforming the input data before training. These methods address imbalance or bias in the dataset itself, ensuring that the model sees a more equitable representation of all groups.\n",
    "\n",
    "### Resampling and Reweighting\n",
    "\n",
    "- Re-sampling:\n",
    "    - Re-sampling adjusts the training data distribution by oversampling underrepresented classes or undersampling overrepresented ones. This helps ensure that the model does not favor the majority class simply because it has more examples.\n",
    "\n",
    "- Re-weighting:\n",
    "    - Instead of modifying the data, re-weighting assigns different importance (weights) to each training example. During training, the loss associated with underrepresented examples is increased, encouraging the model to pay more attention to them.\n",
    "    \n",
    "### Data Augmentation\n",
    "\n",
    "Data augmentation involves generating synthetic data for underrepresented groups. By creating new, plausible data points, you can balance the training set without simply duplicating existing samples.\n",
    "\n",
    "    \n",
    "### Examples\n",
    "\n",
    "#### Resampling with Python\n",
    "\n",
    "Below is an example using the imbalanced-learn library to perform oversampling on a simulated imbalanced dataset. The code snippet demonstrates how to increase the number of examples for the minority class, helping to reduce bias during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "34d72b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original distribution:\n",
      "label\n",
      "0.0    150\n",
      "1.0     50\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Resampled distribution:\n",
      "label\n",
      "1.0    150\n",
      "0.0    150\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Simulate an imbalanced dataset\n",
    "np.random.seed(42)\n",
    "df = pd.DataFrame({\n",
    "    'feature1': np.random.randn(200),\n",
    "    'feature2': np.random.randn(200),\n",
    "    'label': np.concatenate((np.zeros(150), np.ones(50)))  # imbalanced: 150 negatives vs 50 positives\n",
    "})\n",
    "\n",
    "# Display original distribution\n",
    "print(\"Original distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "# Separate the dataset into majority and minority classes\n",
    "df_majority = df[df['label'] == 0]\n",
    "df_minority = df[df['label'] == 1]\n",
    "\n",
    "# Oversample the minority class to match the majority class count\n",
    "df_minority_oversampled = df_minority.sample(n=len(df_majority), replace=True, random_state=42)\n",
    "\n",
    "# Combine the majority class with the oversampled minority class\n",
    "df_resampled = pd.concat([df_majority, df_minority_oversampled]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Display resampled distribution\n",
    "print(\"\\nResampled distribution:\")\n",
    "print(df_resampled['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3af04d2",
   "metadata": {},
   "source": [
    "#### Synthetic Data Generation\n",
    "\n",
    "Below is an example that uses the SMOTE (Synthetic Minority Over-sampling Technique) algorithm to create synthetic samples. SMOTE creates new samples by interpolating between existing minority class samples, thereby improving the representation of the underrepresented class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "512aa75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Simulate an imbalanced dataset\n",
    "np.random.seed(42)\n",
    "df = pd.DataFrame({\n",
    "    'feature1': np.random.randn(200),\n",
    "    'feature2': np.random.randn(200),\n",
    "    'label': np.concatenate((np.zeros(150), np.ones(50)))  # imbalanced: 150 negatives vs 50 positives\n",
    "})\n",
    "\n",
    "# Separate features and labels\n",
    "X = df[['feature1', 'feature2']]\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6527e9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE distribution:\n",
      "0.0    150\n",
      "1.0    150\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def manual_smote(X, y, minority_class=1, k_neighbors=5, random_state=42):\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # If X is a DataFrame, store its column names and convert to a NumPy array\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        X_columns = X.columns\n",
    "        X_array = X.values\n",
    "    else:\n",
    "        X_array = X\n",
    "\n",
    "    # Identify indices for minority and majority classes\n",
    "    minority_idx = np.where(y == minority_class)[0]\n",
    "    majority_idx = np.where(y != minority_class)[0]\n",
    "    \n",
    "    # Extract minority samples\n",
    "    X_min = X_array[minority_idx]\n",
    "    \n",
    "    n_min = len(minority_idx)\n",
    "    n_maj = len(majority_idx)\n",
    "    \n",
    "    # Calculate number of synthetic samples needed to balance the dataset\n",
    "    n_samples_needed = n_maj - n_min\n",
    "    if n_samples_needed <= 0:\n",
    "        return X, y  # No oversampling needed if the dataset is already balanced\n",
    "\n",
    "    # Fit NearestNeighbors on the minority samples (using NumPy array)\n",
    "    nbrs = NearestNeighbors(n_neighbors=k_neighbors + 1).fit(X_min)\n",
    "    \n",
    "    synthetic_samples = []\n",
    "    for _ in range(n_samples_needed):\n",
    "        # Randomly choose a minority sample\n",
    "        idx = np.random.randint(0, n_min)\n",
    "        sample = X_min[idx]\n",
    "        \n",
    "        # Find k-nearest neighbors; the first neighbor is the sample itself\n",
    "        neighbors = nbrs.kneighbors([sample], return_distance=False)[0]\n",
    "        # Randomly choose one of the neighbors (skipping the first one)\n",
    "        neighbor_idx = np.random.choice(neighbors[1:])\n",
    "        neighbor = X_min[neighbor_idx]\n",
    "        \n",
    "        # Create a synthetic sample by interpolating between the sample and its neighbor\n",
    "        gap = np.random.rand()\n",
    "        synthetic_sample = sample + gap * (neighbor - sample)\n",
    "        synthetic_samples.append(synthetic_sample)\n",
    "    \n",
    "    synthetic_samples = np.array(synthetic_samples)\n",
    "    \n",
    "    # Combine synthetic samples with the original data\n",
    "    X_new_array = np.concatenate([X_array, synthetic_samples], axis=0)\n",
    "    y_new = np.concatenate([y, np.array([minority_class] * n_samples_needed)])\n",
    "    \n",
    "    # If X was originally a DataFrame, convert the combined data back to a DataFrame\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        X_new = pd.DataFrame(X_new_array, columns=X_columns)\n",
    "    else:\n",
    "        X_new = X_new_array\n",
    "    \n",
    "    return X_new, y_new\n",
    "\n",
    "# Example usage:\n",
    "# Assume X and y are already defined (for example, from a previous dataset)\n",
    "X_smote, y_smote = manual_smote(X, y, minority_class=1, k_neighbors=5, random_state=42)\n",
    "\n",
    "print(\"SMOTE distribution:\")\n",
    "print(pd.Series(y_smote).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12855039",
   "metadata": {},
   "source": [
    "## In-Processing Techniques\n",
    "\n",
    "In-processing techniques involve modifying the learning algorithm itself to reduce bias during model training. This is achieved by integrating fairness considerations directly into the training process.\n",
    "\n",
    "### Fairness-Aware Learning Algorithms\n",
    "\n",
    "Fairness-aware algorithms introduce fairness constraints into the training objective. One popular approach is adversarial debiasing, where an additional adversary network is trained alongside the main model to detect and penalize biased representations.\n",
    "\n",
    "### Regularization Methods\n",
    "\n",
    "Regularization methods add a fairness term to the loss function. This additional term penalizes the model when it learns correlations that contribute to bias.\n",
    "\n",
    "### Examples\n",
    "\n",
    "#### Adversarial Debiasing\n",
    "\n",
    "While implementing full adversarial debiasing can be complex, here’s a conceptual outline using pseudo-code. In this approach, the primary model is penalized if its internal representations allow the adversary to predict the sensitive attribute, encouraging fairness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "adca0544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20] | Primary Loss: 0.6947 | Adversary Loss: 0.6937\n",
      "Epoch [10/20] | Primary Loss: 0.6849 | Adversary Loss: 0.6955\n",
      "Epoch [15/20] | Primary Loss: 0.6808 | Adversary Loss: 0.6945\n",
      "Epoch [20/20] | Primary Loss: 0.6762 | Adversary Loss: 0.6943\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the Gradient Reversal Layer as a custom autograd Function\n",
    "class GradientReversal(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, lambda_value):\n",
    "        ctx.lambda_value = lambda_value\n",
    "        return x.clone()\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Reverse the gradients by multiplying with -lambda_value\n",
    "        return grad_output.neg() * ctx.lambda_value, None\n",
    "\n",
    "def grad_reverse(x, lambda_value=1.0):\n",
    "    return GradientReversal.apply(x, lambda_value)\n",
    "\n",
    "# Primary model: feature extractor + classifier for the main prediction task\n",
    "class PrimaryModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
    "        super(PrimaryModel, self).__init__()\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        logits = self.classifier(features)\n",
    "        return logits, features\n",
    "\n",
    "# Adversary model: takes the hidden representation and predicts the sensitive attribute\n",
    "class AdversaryModel(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_sensitive):\n",
    "        super(AdversaryModel, self).__init__()\n",
    "        self.adversary = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, num_sensitive)\n",
    "        )\n",
    "    \n",
    "    def forward(self, features, lambda_value):\n",
    "        # Apply gradient reversal to the features before passing them to the adversary\n",
    "        reversed_features = grad_reverse(features, lambda_value)\n",
    "        sensitive_logits = self.adversary(reversed_features)\n",
    "        return sensitive_logits\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = 10       # Number of input features\n",
    "hidden_dim = 20      # Dimension of hidden representation\n",
    "num_classes = 2      # Number of primary task classes\n",
    "num_sensitive = 2    # Number of sensitive attribute classes (e.g., gender)\n",
    "lambda_value = 1.0   # Trade-off hyperparameter for adversarial loss\n",
    "\n",
    "# Instantiate models\n",
    "primary_model = PrimaryModel(input_dim, hidden_dim, num_classes)\n",
    "adversary_model = AdversaryModel(hidden_dim, num_sensitive)\n",
    "\n",
    "# Loss functions for primary classification and adversary prediction\n",
    "criterion_primary = nn.CrossEntropyLoss()\n",
    "criterion_adversary = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizers for each model\n",
    "optimizer_primary = optim.Adam(primary_model.parameters(), lr=0.01)\n",
    "optimizer_adversary = optim.Adam(adversary_model.parameters(), lr=0.01)\n",
    "\n",
    "# Generate synthetic data for demonstration\n",
    "num_samples = 1000\n",
    "X = torch.randn(num_samples, input_dim)\n",
    "y_primary = torch.randint(0, num_classes, (num_samples,))      # Labels for the primary task\n",
    "y_sensitive = torch.randint(0, num_sensitive, (num_samples,))    # Sensitive attribute labels\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    primary_model.train()\n",
    "    adversary_model.train()\n",
    "    \n",
    "    optimizer_primary.zero_grad()\n",
    "    optimizer_adversary.zero_grad()\n",
    "    \n",
    "    # Forward pass through the primary model\n",
    "    logits, features = primary_model(X)\n",
    "    loss_primary = criterion_primary(logits, y_primary)\n",
    "    \n",
    "    # Forward pass through the adversary model\n",
    "    sensitive_logits = adversary_model(features, lambda_value)\n",
    "    loss_adversary = criterion_adversary(sensitive_logits, y_sensitive)\n",
    "    \n",
    "    # Total loss: adversary loss is added here, but due to the gradient reversal,\n",
    "    # the gradient w.r.t. the feature extractor from this term is reversed.\n",
    "    total_loss = loss_primary + loss_adversary\n",
    "    \n",
    "    total_loss.backward()\n",
    "    optimizer_primary.step()\n",
    "    optimizer_adversary.step()\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] | Primary Loss: {loss_primary.item():.4f} | Adversary Loss: {loss_adversary.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815f033f",
   "metadata": {},
   "source": [
    "#### Fairness Regularizer in a Loss Function\n",
    "\n",
    "Below is an illustrative example using PyTorch. In this example, a custom fairness regularizer is added to the standard loss function. This example shows how you can adjust the loss function to include a fairness penalty, encouraging the model to produce similar predictions for different sensitive groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2c7add0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100] Loss: 0.6728 | Fairness Diff: 0.0004\n",
      "Epoch [20/100] Loss: 0.6473 | Fairness Diff: 0.0000\n",
      "Epoch [30/100] Loss: 0.6244 | Fairness Diff: 0.0013\n",
      "Epoch [40/100] Loss: 0.6017 | Fairness Diff: 0.0006\n",
      "Epoch [50/100] Loss: 0.5771 | Fairness Diff: 0.0005\n",
      "Epoch [60/100] Loss: 0.5524 | Fairness Diff: 0.0006\n",
      "Epoch [70/100] Loss: 0.5253 | Fairness Diff: 0.0002\n",
      "Epoch [80/100] Loss: 0.4969 | Fairness Diff: 0.0010\n",
      "Epoch [90/100] Loss: 0.4718 | Fairness Diff: 0.0006\n",
      "Epoch [100/100] Loss: 0.4485 | Fairness Diff: 0.0022\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Dummy data and labels\n",
    "features = torch.randn(100, 10)\n",
    "labels = torch.randint(0, 2, (100,), dtype=torch.float32)\n",
    "sensitive_attr = torch.randint(0, 2, (100,), dtype=torch.float32)  # e.g., 0 for one group, 1 for another\n",
    "\n",
    "# Simple model definition\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 5),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(5, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Custom fairness regularizer (for demonstration purposes)\n",
    "d e difference in mean predictions between groups\n",
    "    group0_mean = predictions[sensitive_attr == 0].mean()\n",
    "    group1_mean = predictions[sensitive_attr == 1].mean()\n",
    "    return torch.abs(group0_mean - group1_mean)\n",
    "\n",
    "# Training loop with fairness regularization\n",
    "lambda_fairness = 0.5  # weight for fairness regularizer\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    preds = model(features).squeeze()\n",
    "    loss = criterion(preds, labels) + lambda_fairness * fairness_regularizer(preds, sensitive_attr)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Print loss and fairness difference every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        group0_mean = preds[sensitive_attr == 0].mean().item()\n",
    "        group1_mean = preds[sensitive_attr == 1].mean().item()\n",
    "        fairness_diff = abs(group0_mean - group1_mean)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {loss.item():.4f} | Fairness Diff: {fairness_diff:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac26d3b6",
   "metadata": {},
   "source": [
    "## Post-processing Techniques\n",
    "\n",
    "Post-processing techniques modify the model’s outputs after training to reduce bias. These techniques are particularly useful when the underlying model cannot be easily altered.\n",
    "\n",
    "### Output Adjustment\n",
    "\n",
    "Output adjustment involves modifying the final predictions to achieve fairness. This can be done by calibrating thresholds differently for various groups to equalize performance metrics like precision or recall.\n",
    "\n",
    "### Ensemble Methods\n",
    "\n",
    "Ensemble methods combine multiple models to create a final prediction. By averaging or voting across models that may have different bias characteristics, the final outcome can be more balanced.\n",
    "\n",
    "### Examples\n",
    "\n",
    "#### Threshold Adjustment\n",
    "\n",
    "Suppose you have a binary classifier and you want to set different decision thresholds for two groups. The following code snippet demonstrates how to adjust those thresholds. By setting a higher threshold for one group, you can calibrate the classifier to mitigate unfair advantages or disadvantages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b9f5ae8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted probabilities:\n",
      "[0.16949275 0.55680126 0.93615477 0.6960298  0.57006117 0.09717649\n",
      " 0.61500723 0.99005385 0.14008402 0.51832965 0.87737307 0.74076862\n",
      " 0.69701574 0.70248408 0.35949115 0.29359184 0.80936116 0.81011339\n",
      " 0.86707232 0.91324055 0.5113424  0.50151629 0.79829518 0.64996393\n",
      " 0.70196688 0.79579267 0.89000534 0.33799516 0.37558295 0.09398194\n",
      " 0.57828014 0.03594227 0.46559802 0.54264463 0.28654125 0.59083326\n",
      " 0.03050025 0.03734819 0.82260056 0.36019064 0.12706051 0.52224326\n",
      " 0.76999355 0.21582103 0.62289048 0.08534746 0.05168172 0.53135463\n",
      " 0.54063512 0.6374299  0.72609133 0.97585208 0.51630035 0.32295647\n",
      " 0.79518619 0.27083225 0.43897142 0.07845638 0.02535074 0.96264841\n",
      " 0.83598012 0.69597421 0.40895294 0.17329432 0.15643704 0.2502429\n",
      " 0.54922666 0.71459592 0.66019738 0.2799339  0.95486528 0.73789692\n",
      " 0.55435405 0.61172075 0.41960006 0.24773099 0.35597268 0.75784611\n",
      " 0.01439349 0.11607264 0.04600264 0.0407288  0.85546058 0.70365786\n",
      " 0.47417383 0.09783416 0.49161588 0.47347177 0.17320187 0.43385165\n",
      " 0.39850473 0.6158501  0.63509365 0.04530401 0.37461261 0.62585992\n",
      " 0.50313626 0.85648984 0.65869363 0.16293443]\n",
      "\n",
      "Sensitive group assignments:\n",
      "[1 1 0 1 0 1 1 1 0 1 1 1 1 0 1 0 0 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 0 1 1 0 0\n",
      " 1 0 0 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 0\n",
      " 1 1 1 0 0 1 1 0 1 1 0 1 0 0 0 0 0 0 0 1 0 0 1 1 1 0]\n",
      "\n",
      "Adjusted predictions:\n",
      "[0 0 1 1 1 0 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 0 0 1 0 0 0 0 1 0\n",
      " 0 1 0 0 0 1 0 1 0 0 0 0 1 1 1 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 1 1 1\n",
      " 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 0 1 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def adjust_thresholds(predictions, sensitive_group, threshold_group0=0.5, threshold_group1=0.5):\n",
    "    # Apply different thresholds based on the sensitive group\n",
    "    adjusted_preds = []\n",
    "    for pred, group in zip(predictions, sensitive_group):\n",
    "        threshold = threshold_group0 if group == 0 else threshold_group1\n",
    "        adjusted_preds.append(1 if pred >= threshold else 0)\n",
    "    return np.array(adjusted_preds)\n",
    "\n",
    "# Example usage with simulated probabilities\n",
    "predicted_probs = np.random.rand(100)\n",
    "sensitive_group = np.random.randint(0, 2, 100)\n",
    "adjusted_predictions = adjust_thresholds(predicted_probs, sensitive_group, threshold_group0=0.5, threshold_group1=0.6)\n",
    "\n",
    "# Print the results\n",
    "print(\"Predicted probabilities:\")\n",
    "print(predicted_probs)\n",
    "\n",
    "print(\"\\nSensitive group assignments:\")\n",
    "print(sensitive_group)\n",
    "\n",
    "print(\"\\nAdjusted predictions:\")\n",
    "print(adjusted_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cf950d",
   "metadata": {},
   "source": [
    "#### Simple Ensemble Voting\n",
    "\n",
    "Below is an example where predictions from two different models are combined using a simple majority vote. Ensembling can help mitigate biases if the individual models make different errors, resulting in a more robust and fair final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "661e6db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 Predictions:\n",
      "[1 0 0 0 0 1 1 0 0 0 1 0 0 1 1 0 1 1 0 1 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0\n",
      " 0 1 1 0 1 0 1 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 1 0 1 0 1 1 0 0 0 1\n",
      " 0 1 1 0 0 1 1 0 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 1 1 0]\n",
      "\n",
      "Model 2 Predictions:\n",
      "[0 1 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 1 1 1 0 1 0 0\n",
      " 0 0 1 1 0 0 1 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 0 0 0 0 1 0]\n",
      "\n",
      "Ensemble Predictions:\n",
      "[1 1 0 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 0 0\n",
      " 0 1 1 1 1 0 1 0 1 0 0 1 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Simulated predictions from two models\n",
    "model1_preds = np.random.randint(0, 2, 100)\n",
    "model2_preds = np.random.randint(0, 2, 100)\n",
    "\n",
    "# Print individual model predictions\n",
    "print(\"Model 1 Predictions:\")\n",
    "print(model1_preds)\n",
    "\n",
    "print(\"\\nModel 2 Predictions:\")\n",
    "print(model2_preds)\n",
    "\n",
    "# Combine predictions using majority vote:\n",
    "# Here, we predict 1 if at least one of the models predicts 1.\n",
    "ensemble_preds = (model1_preds + model2_preds) >= 1  \n",
    "ensemble_preds = ensemble_preds.astype(int)\n",
    "\n",
    "# Print the ensemble predictions\n",
    "print(\"\\nEnsemble Predictions:\")\n",
    "print(ensemble_preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb366e18",
   "metadata": {},
   "source": [
    "# Tools and Libraries\n",
    "\n",
    "A variety of open-source libraries have been developed to help practitioners both evaluate and mitigate bias in deep learning and machine learning systems. These tools integrate with common frameworks and pipelines, providing both metric evaluations and mitigation algorithms. \n",
    "\n",
    "\n",
    "## Fairlearn\n",
    "\n",
    "**Overview:**\n",
    "\n",
    "Fairlearn is a Python library that provides algorithms for assessing and reducing bias in machine learning models. It offers metrics to quantify fairness issues (such as demographic parity, equalized odds, etc.) as well as mitigation algorithms that can adjust predictions or training procedures to promote fairness.\n",
    "\n",
    "**Key Features:**\n",
    "\n",
    "- **Fairness Metrics:** Quickly compute fairness measures for your model’s predictions.\n",
    "- **Mitigation Algorithms:** Tools like the *Exponentiated Gradient Reduction* allow you to enforce fairness constraints during model training.\n",
    "- **Integration:** Works with scikit-learn and other common Python libraries.\n",
    "\n",
    "**Resource Link:**\n",
    "[Fairlearn User Guide](https://fairlearn.org/main/user_guide/index.html)\n",
    "\n",
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db43ab42",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fairlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ad03b03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Demographic Parity Difference: 0.012513801987486195\n",
      "Mitigated Demographic Parity Difference: 0.0050300576616366666\n",
      "Baseline Accuracy: 0.605\n",
      "Mitigated Accuracy: 0.61\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from fairlearn.metrics import demographic_parity_difference\n",
    "from fairlearn.reductions import ExponentiatedGradient, DemographicParity\n",
    "\n",
    "# Simulated dataset\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(200, 5)\n",
    "y = (np.random.rand(200) > 0.5).astype(int)\n",
    "sensitive_feature = (np.random.rand(200) > 0.7).astype(int)  # e.g., binary gender\n",
    "\n",
    "# Train a baseline classifier\n",
    "baseline_model = LogisticRegression(solver='liblinear')\n",
    "baseline_model.fit(X, y)\n",
    "y_pred_baseline = baseline_model.predict(X)\n",
    "\n",
    "# Evaluate fairness metric (Demographic Parity Difference)\n",
    "dp_diff = demographic_parity_difference(y, y_pred_baseline, sensitive_features=sensitive_feature)\n",
    "print(\"Baseline Demographic Parity Difference:\", dp_diff)\n",
    "\n",
    "# Mitigation using Fairlearn's Exponentiated Gradient Reduction\n",
    "constraint = DemographicParity()\n",
    "mitigator = ExponentiatedGradient(LogisticRegression(solver='liblinear'), constraint)\n",
    "mitigator.fit(X, y, sensitive_features=sensitive_feature)\n",
    "y_pred_mitigated = mitigator.predict(X)\n",
    "\n",
    "# Re-evaluate fairness metric after mitigation\n",
    "dp_diff_mitigated = demographic_parity_difference(y, y_pred_mitigated, sensitive_features=sensitive_feature)\n",
    "print(\"Mitigated Demographic Parity Difference:\", dp_diff_mitigated)\n",
    "\n",
    "# Optionally, compare overall accuracy\n",
    "print(\"Baseline Accuracy:\", accuracy_score(y, y_pred_baseline))\n",
    "print(\"Mitigated Accuracy:\", accuracy_score(y, y_pred_mitigated))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6aaae2",
   "metadata": {},
   "source": [
    "## AI Fairness 360 (AIF360)\n",
    "\n",
    "**Overview:**\n",
    "\n",
    "Developed by IBM, AI Fairness 360 is a comprehensive toolkit offering a suite of bias detection and mitigation algorithms. The toolkit supports various fairness metrics and provides pre-processing, in-processing, and post-processing techniques to address bias in datasets and models.\n",
    "\n",
    "**Key Features:**\n",
    "\n",
    "- **Bias Detection:** Contains many metrics (e.g., statistical parity, disparate impact) to evaluate fairness.\n",
    "- **Bias Mitigation:** Offers algorithms for reweighting, relabeling, and other interventions across the machine learning pipeline.\n",
    "- **Dataset Support:** Comes with utilities to convert common datasets into standardized formats (e.g., `BinaryLabelDataset`).\n",
    "\n",
    "**Resource Link:**\n",
    "[AIF360 Github Link](https://github.com/Trusted-AI/AIF360)\n",
    "\n",
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0230f11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install aif360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "465dd418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistical Parity Difference: -1.0\n"
     ]
    }
   ],
   "source": [
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.metrics import ClassificationMetric\n",
    "import pandas as pd\n",
    "\n",
    "# Simulated DataFrame with a sensitive attribute 'gender'\n",
    "df = pd.DataFrame({\n",
    "    'feature1': np.random.randn(200),\n",
    "    'label': np.concatenate((np.zeros(150), np.ones(50))),\n",
    "    'gender': np.concatenate((np.zeros(150), np.ones(50)))  # 0 and 1 representing two groups\n",
    "})\n",
    "\n",
    "# Convert DataFrame into AIF360's BinaryLabelDataset format\n",
    "dataset = BinaryLabelDataset(\n",
    "    df=df,\n",
    "    label_names=['label'],\n",
    "    protected_attribute_names=['gender']\n",
    ")\n",
    "\n",
    "# Suppose we have a set of predictions from a classifier\n",
    "# Here, we simply use the original labels as a placeholder\n",
    "predicted_dataset = dataset.copy(deepcopy=True)\n",
    "\n",
    "# Compute fairness metrics: Statistical Parity Difference, for example\n",
    "metric = ClassificationMetric(\n",
    "    dataset, \n",
    "    predicted_dataset,\n",
    "    unprivileged_groups=[{'gender': 0}],\n",
    "    privileged_groups=[{'gender': 1}]\n",
    ")\n",
    "\n",
    "print(\"Statistical Parity Difference:\", metric.statistical_parity_difference())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf32e873",
   "metadata": {},
   "source": [
    "## TensorFlow Model Analysis (TFMA)\n",
    "\n",
    "**Overview:**\n",
    "\n",
    "TFMA is an evaluation library that integrates with TensorFlow Extended (TFX) pipelines. It is designed to evaluate TensorFlow models at scale and includes built-in fairness evaluation. TFMA provides detailed performance and fairness metrics, slicing the results by various feature dimensions to identify any disparities.\n",
    "\n",
    "**Key Features:**\n",
    "\n",
    "- **Integration with TFX:** Easily incorporate into your production pipelines.\n",
    "- **Slicing:** Evaluate model performance and fairness across different segments (slices) of the data.\n",
    "- **Visualization:** Generate dashboards and reports to interpret results.\n",
    "\n",
    "**Resource Link:**\n",
    "[TensorFlow Model Analysis](https://www.tensorflow.org/tfx/model_analysis/get_started)\n",
    "\n",
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c27dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow_model_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785f868e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_model_analysis as tfma\n",
    "\n",
    "# Create dummy evaluation examples as tf.Example protos\n",
    "# Each example contains a 'label' (true value) and 'predictions' (model output probability)\n",
    "def create_dummy_examples(num_examples=100):\n",
    "    examples = []\n",
    "    for i in range(num_examples):\n",
    "        # Alternate labels for binary classification (0 or 1)\n",
    "        label = float(i % 2)\n",
    "        # Generate a random prediction probability between 0 and 1\n",
    "        prediction = np.random.rand()\n",
    "        example = tf.train.Example(features=tf.train.Features(feature={\n",
    "            'label': tf.train.Feature(float_list=tf.train.FloatList(value=[label])),\n",
    "            'predictions': tf.train.Feature(float_list=tf.train.FloatList(value=[prediction])),\n",
    "        }))\n",
    "        examples.append(example.SerializeToString())\n",
    "    return examples\n",
    "\n",
    "# Write the dummy examples to a temporary TFRecord file\n",
    "def write_tf_record(examples, file_path):\n",
    "    with tf.io.TFRecordWriter(file_path) as writer:\n",
    "        for ex in examples:\n",
    "            writer.write(ex)\n",
    "\n",
    "# Create temporary directory and file for evaluation data\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "eval_data_path = os.path.join(temp_dir, 'eval_data.tfrecord')\n",
    "\n",
    "dummy_examples = create_dummy_examples(num_examples=100)\n",
    "write_tf_record(dummy_examples, eval_data_path)\n",
    "\n",
    "# Define an evaluation configuration for TFMA\n",
    "eval_config = tfma.EvalConfig(\n",
    "    model_specs=[tfma.ModelSpec(label_key='label')],\n",
    "    slicing_specs=[tfma.SlicingSpec()],  # Global metrics; add more slices if needed.\n",
    "    metrics_specs=[tfma.MetricsSpec(metrics=[\n",
    "        tfma.MetricConfig(class_name=\"ExampleCount\"),\n",
    "        tfma.MetricConfig(class_name=\"BinaryAccuracy\"),\n",
    "        tfma.MetricConfig(class_name=\"AUC\"),\n",
    "    ])]\n",
    ")\n",
    "\n",
    "# Run TFMA analysis on the dummy evaluation data\n",
    "# Since we already include predictions in the examples, model_location is not needed.\n",
    "eval_result = tfma.run_model_analysis(\n",
    "    data_location=eval_data_path,\n",
    "    eval_config=eval_config,\n",
    "    output_path=temp_dir,\n",
    "    model_location=None\n",
    ")\n",
    "\n",
    "# Print the evaluation results (a dictionary containing metric values)\n",
    "print(\"TFMA Evaluation Results:\")\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3389158",
   "metadata": {},
   "source": [
    "## Aequitas\n",
    "\n",
    "**Overview:**\n",
    "\n",
    "Aequitas is an open-source bias audit toolkit that helps evaluate fairness by providing group-level metrics and visualizations. It supports a wide range of fairness measures and is designed to work with various model outputs, enabling you to compare disparities across multiple demographic groups.\n",
    "\n",
    "**Key Features:**\n",
    "\n",
    "- **Group Metrics:** Compute fairness metrics like false positive rate, false negative rate, and statistical parity for different subgroups.\n",
    "- **Interactive Dashboards:** Visualize and compare the fairness of different models.\n",
    "- **Ease of Use:** Integrates with pandas DataFrames, making it straightforward to audit datasets and model outputs.\n",
    "\n",
    "**Resource Link:**\n",
    "[Aequitas Link](https://dssg.github.io/aequitas/)\n",
    "\n",
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7d4d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install aequitas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3211e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from aequitas.group import Group\n",
    "from aequitas.metrics import compute_group_metrics\n",
    "\n",
    "# Create a simple example dataset\n",
    "data = {\n",
    "    'score': [0.9, 0.8, 0.2, 0.3, 0.7, 0.4],\n",
    "    'label_value': [1, 1, 0, 0, 1, 0],\n",
    "    'gender': ['male', 'female', 'male', 'female', 'male', 'female']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define the attribute to analyze\n",
    "protected_attribute = 'gender'\n",
    "\n",
    "# Compute group-level metrics\n",
    "group_obj = Group()\n",
    "xtab, _ = group_obj.get_crosstabs(df)\n",
    "metrics = compute_group_metrics(xtab)\n",
    "\n",
    "print(metrics[['attribute_name', 'attribute_value', 'pp_rate', 'fpr', 'fnr']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a019719b",
   "metadata": {},
   "source": [
    "## InterpretML\n",
    "\n",
    "**Overview:**\n",
    "\n",
    "InterpretML is an open-source toolkit that focuses on model interpretability but also offers insights into fairness. By explaining individual predictions and overall model behavior, it can help identify patterns that may indicate bias.\n",
    "\n",
    "**Key Features:**\n",
    "\n",
    "- **Explainability Methods:** Includes techniques such as SHAP and LIME to interpret model predictions.\n",
    "- **Visualization:** Provides intuitive visualizations for feature importance and decision-making processes.\n",
    "- **Fairness Insights:** Although not exclusively for fairness, understanding model explanations can uncover biased behaviors.\n",
    "\n",
    "**Resource Link:**\n",
    "[Interpret ML Github](https://github.com/interpretml/interpret/)\n",
    "\n",
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0306648",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install interpret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f3ef8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from interpret.blackbox import ShapKernel\n",
    "from interpret import show\n",
    "\n",
    "# Create a simulated dataset\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(200, 5)\n",
    "y = (np.random.rand(200) > 0.5).astype(int)\n",
    "feature_names = [f'feature_{i}' for i in range(1, 6)]\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "\n",
    "# Train a simple model\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(df, y)\n",
    "\n",
    "# Create a SHAP explainer using InterpretML\n",
    "explainer = ShapKernel(model.predict_proba, df)\n",
    "shap_values = explainer.explain_global(df)\n",
    "show(shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3f9d6c",
   "metadata": {},
   "source": [
    "# Continuous Monitoring and Ethical Oversight\n",
    "\n",
    "Bias mitigation is not a one-time task. Models must be continuously monitored and updated to ensure they remain fair as data distributions and societal norms evolve:\n",
    "\n",
    "- **Regular Auditing:** Implement ongoing audits to check for drift in model performance and fairness. This should include both automated monitoring and periodic manual reviews.\n",
    "- **Feedback Loops:** Create mechanisms for users and stakeholders to report potential biases or adverse outcomes, ensuring that the model can be iteratively improved.\n",
    "- **Ethical Considerations:** Beyond technical metrics, continuously engage with ethical frameworks to assess whether deploying a model is appropriate. Sometimes, the very problem a model aims to solve may be better addressed by alternative, non-automated approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9ccd6c",
   "metadata": {},
   "source": [
    "# Ethical Considerations: Should the Model Be Built?\n",
    "\n",
    "At a higher level, it is essential to question the ethical implications of building a particular model:\n",
    "\n",
    "- **Problem Relevance:** Assess whether the problem being addressed justifies the risks. For example, while automating resume screening can enhance efficiency, it must be weighed against the potential to entrench discriminatory practices.\n",
    "- **Stakeholder Impact:** Consider the broader impact on society, including who benefits from the model and who might be harmed. A thorough stakeholder analysis can reveal unforeseen consequences.\n",
    "- **Transparency and Accountability:** Develop models with clear interpretability and robust mechanisms for accountability. This ensures that decisions made by the model can be scrutinized and contested by affected parties.\n",
    "- **Alternative Solutions:** Explore whether other, less bias-prone approaches might be available to address the problem without relying heavily on automated systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_env]",
   "language": "python",
   "name": "conda-env-pytorch_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
