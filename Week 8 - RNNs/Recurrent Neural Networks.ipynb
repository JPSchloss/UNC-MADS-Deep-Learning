{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd2e344d",
   "metadata": {},
   "source": [
    "# Sequential Modeling and RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0f3ada",
   "metadata": {},
   "source": [
    "## Sequential Data\n",
    "\n",
    "At its simplest, sequential data is just a list of items arranged in order. It is inherently ordered, meaning that the position of each element matters. Whether you're forecasting the weather or predicting stock market trends, understanding how each element in the sequence relates to its predecessors is key. The challenge lies in capturing the relationships between these items to make accurate predictions.\n",
    "\n",
    "### Examples of Sequential Data\n",
    "\n",
    "Consider these real-world applications where sequential data is fundamental:\n",
    "\n",
    "- **Weather Forecasting:** Predicting future conditions based on historical weather patterns.\n",
    "- **Stock Market Trends:** Analyzing time series data of stock prices.\n",
    "- **Autocomplete for Text:** Suggesting the next word or phrase while you type.\n",
    "- **Genetic Sequencing:** Interpreting the order of nucleotides in DNA.\n",
    "- **Speech Recognition:** Transcribing spoken language by understanding sequential audio signals.\n",
    "- **Video Frame Prediction:** Forecasting the next frame in a video sequence.\n",
    "- **Music Composition:** Generating sequences of musical notes that form a coherent melody.\n",
    "\n",
    "\n",
    "### Two Main Goals with Sequential Data\n",
    "\n",
    "1. **Computing the Probability of a Sequence:**  \n",
    "   The goal is to determine the likelihood that a particular sequence of events or items occurs. This involves calculating the probability of each element in the context of those that came before it.\n",
    "\n",
    "2. **Predicting the Next Item in a Sequence:**  \n",
    "   Given a history of previous items, we want to forecast what the next item will be. This is critical for tasks like text autocomplete or predicting market trends.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1109914",
   "metadata": {},
   "source": [
    "## Reviewing Probability Concepts\n",
    "\n",
    "\n",
    "### Conditional Probability\n",
    "\n",
    "Conditional probability is all about context - it tells us the probability of an event occurring given that another event has already happened. Mathematically, if we have two events $A$ and $B$, the conditional probability of $B$ given $A$ is defined as:\n",
    "\n",
    "$$\n",
    "P(B \\mid A) = \\frac{P(A, B)}{P(A)}\n",
    "$$\n",
    "\n",
    "This formula assumes that $P(A)$ is not zero. It tells us how likely $B$ is to occur when we already know that $A$ has occurred. In sequential data, this means that the probability of seeing a certain word, for example, may depend on the words that came before it.\n",
    "\n",
    "\n",
    "### The Chain Rule\n",
    "\n",
    "The chain rule of probability is a tool that allows us to break down the probability of a long sequence into a product of conditional probabilities. Suppose you have a sequence of items $x_1, x_2, \\dots, x_n$. The chain rule states that the joint probability of the entire sequence can be expressed as:\n",
    "\n",
    "$$\n",
    "P(x_1, x_2, \\dots, x_n) = P(x_1) \\times P(x_2 \\mid x_1) \\times P(x_3 \\mid x_1, x_2) \\times \\cdots \\times P(x_n \\mid x_1, x_2, \\dots, x_{n-1})\n",
    "$$\n",
    "\n",
    "By decomposing a complex joint probability into manageable pieces, we can handle the computation more effectively. \n",
    "\n",
    "#### Step-by-Step Explanation\n",
    "\n",
    "1. **Start Simple:**  \n",
    "   The first term $P(x_1)$ is the probability of the first item in the sequence. There’s no history, so it stands alone.\n",
    "\n",
    "2. **Build Context:**  \n",
    "   For the second item, $P(x_2 \\mid x_1)$ tells us the probability of $x_2$ occurring after $x_1$. It captures how $x_1$ influences $x_2$.\n",
    "\n",
    "3. **Continuing the Sequence:**  \n",
    "   For the third item, $P(x_3 \\mid x_1, x_2)$ incorporates the context of both previous items, and so on. Each term adds more context, making the overall model richer.\n",
    "\n",
    "\n",
    "### Challenges\n",
    "\n",
    "While the chain rule provides a clean theoretical framework, real-world applications often run into two major issues:\n",
    "\n",
    "1. **Computational Complexity:**\n",
    "   - **Too Many Parameters:**  \n",
    "     With longer sequences, the number of possible combinations of previous items becomes enormous. This leads to a combinatorial explosion in the number of conditional probabilities that need to be estimated, making it computationally expensive or even infeasible to model every possible sequence.\n",
    "\n",
    "2. **Generalization to Unseen Data:**\n",
    "   - **Data Sparsity:**  \n",
    "     Since it’s impractical to encounter every possible sequence during training, models often face situations where they must predict probabilities for sequences or elements they have never seen before.\n",
    "   - **Zero-Probability Problem:**  \n",
    "     If a sequence or a particular context has not been observed during training, the model might erroneously assign it a probability of zero, even though the sequence could be perfectly plausible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662f3dbd",
   "metadata": {},
   "source": [
    "## Markov Models\n",
    "\n",
    "Markov Models are one of the earliest and simplest approaches to modeling sequential data. They are built on a key assumption: the **Markov Assumption**.\n",
    "\n",
    "### The Markov Assumption\n",
    "\n",
    "At its core, the Markov Assumption states that the future state depends only on a limited part of the past. In other words, once you know the present (or a limited number of previous states), you don't need the entire history to predict the future. This assumption simplifies the modeling process by limiting the context window we need to consider when predicting the next item.\n",
    "\n",
    "This idea can be expressed mathematically:\n",
    "\n",
    "- For a **First Order Markov Model**:\n",
    "\n",
    "$$\n",
    "P(x_t \\mid x_1, x_2, \\dots, x_{t-1}) \\approx P(x_t \\mid x_{t-1})\n",
    "$$\n",
    "\n",
    "- For a **Second Order Markov Model**:\n",
    "\n",
    "$$\n",
    "P(x_t \\mid x_1, x_2, \\dots, x_{t-1}) \\approx P(x_t \\mid x_{t-2}, x_{t-1})\n",
    "$$\n",
    "\n",
    "\n",
    "### First Order Markov Model\n",
    "\n",
    "In a first order Markov model, the current state depends solely on the immediately preceding state. For example, when modeling weather, one might assume that tomorrow's weather depends only on today's weather, not on the weather of days before today.\n",
    "\n",
    "#### How It Works\n",
    "\n",
    "- **State Representation:**  \n",
    "  Each state represents a possible condition or observation (e.g., \"Sunny\", \"Cloudy\", \"Rainy\").\n",
    "\n",
    "- **Transition Probabilities:**  \n",
    "  The model uses a transition matrix where each entry represents the probability of moving from one state to another. The current state completely encapsulates the necessary context for predicting the next state.\n",
    "\n",
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "816b0546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated Weather Sequence:\n",
      "['Sunny', 'Rainy', 'Cloudy', 'Rainy', 'Rainy', 'Rainy', 'Cloudy', 'Cloudy', 'Rainy', 'Sunny', 'Rainy']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Define the states and transition probabilities.\n",
    "states = [\"Sunny\", \"Cloudy\", \"Rainy\"]\n",
    "transition_matrix = {\n",
    "    \"Sunny\": {\"Sunny\": 0.7, \"Cloudy\": 0.2, \"Rainy\": 0.1},\n",
    "    \"Cloudy\": {\"Sunny\": 0.3, \"Cloudy\": 0.4, \"Rainy\": 0.3},\n",
    "    \"Rainy\": {\"Sunny\": 0.2, \"Cloudy\": 0.3, \"Rainy\": 0.5}\n",
    "}\n",
    "\n",
    "def next_state(current_state):\n",
    "    next_probs = transition_matrix[current_state]\n",
    "    next_states = list(next_probs.keys())\n",
    "    probabilities = list(next_probs.values())\n",
    "    return random.choices(next_states, probabilities)[0]\n",
    "\n",
    "# Simulate a sequence of weather states\n",
    "current_state = \"Sunny\"\n",
    "sequence = [current_state]\n",
    "for _ in range(10):\n",
    "    current_state = next_state(current_state)\n",
    "    sequence.append(current_state)\n",
    "\n",
    "print(\"Simulated Weather Sequence:\")\n",
    "print(sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422ef270",
   "metadata": {},
   "source": [
    "### Second Order Markov Model\n",
    "\n",
    "A second order Markov model extends the first order idea by considering the two most recent states instead of just the one. This allows for a slightly richer context when predicting the next state. For instance, the weather tomorrow might depend on both today’s and yesterday’s weather.\n",
    "\n",
    "#### How It Works\n",
    "\n",
    "- **State Representation:**  \n",
    "  In a second order model, each \"state\" becomes a pair of consecutive observations (e.g., (\"Sunny\", \"Cloudy\")).\n",
    "\n",
    "- **Transition Probabilities:**  \n",
    "  Transition probabilities are defined for these pairs transitioning to the next state. This added context can provide better predictions in some scenarios, though it also increases the number of parameters required.\n",
    "\n",
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73f5e2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated Weather Sequence (Second Order):\n",
      "['Sunny', 'Cloudy', 'Rainy', 'Rainy', 'Sunny', 'Cloudy', 'Sunny', 'Cloudy', 'Cloudy', 'Cloudy', 'Rainy', 'Rainy']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Define the states\n",
    "states = [\"Sunny\", \"Cloudy\", \"Rainy\"]\n",
    "\n",
    "# Define the transition probabilities for a second order Markov model.\n",
    "# The keys are tuples representing the previous two states,\n",
    "# and the values are dictionaries mapping the next state to its probability.\n",
    "\n",
    "transition_matrix = {\n",
    "    (\"Sunny\", \"Sunny\"): {\"Sunny\": 0.6, \"Cloudy\": 0.3, \"Rainy\": 0.1},\n",
    "    (\"Sunny\", \"Cloudy\"): {\"Sunny\": 0.3, \"Cloudy\": 0.4, \"Rainy\": 0.3},\n",
    "    (\"Sunny\", \"Rainy\"): {\"Sunny\": 0.2, \"Cloudy\": 0.3, \"Rainy\": 0.5},\n",
    "    (\"Cloudy\", \"Sunny\"): {\"Sunny\": 0.5, \"Cloudy\": 0.3, \"Rainy\": 0.2},\n",
    "    (\"Cloudy\", \"Cloudy\"): {\"Sunny\": 0.2, \"Cloudy\": 0.5, \"Rainy\": 0.3},\n",
    "    (\"Cloudy\", \"Rainy\"): {\"Sunny\": 0.1, \"Cloudy\": 0.3, \"Rainy\": 0.6},\n",
    "    (\"Rainy\", \"Sunny\"): {\"Sunny\": 0.3, \"Cloudy\": 0.4, \"Rainy\": 0.3},\n",
    "    (\"Rainy\", \"Cloudy\"): {\"Sunny\": 0.2, \"Cloudy\": 0.4, \"Rainy\": 0.4},\n",
    "    (\"Rainy\", \"Rainy\"): {\"Sunny\": 0.1, \"Cloudy\": 0.3, \"Rainy\": 0.6}\n",
    "}\n",
    "\n",
    "def next_state(prev_state, current_state):\n",
    "    # Get the transition probabilities based on the last two states\n",
    "    next_probs = transition_matrix[(prev_state, current_state)]\n",
    "    next_states = list(next_probs.keys())\n",
    "    probabilities = list(next_probs.values())\n",
    "    return random.choices(next_states, probabilities)[0]\n",
    "\n",
    "# Initialize the sequence with two states.\n",
    "initial_state1 = \"Sunny\"\n",
    "initial_state2 = \"Cloudy\"\n",
    "sequence = [initial_state1, initial_state2]\n",
    "\n",
    "# Generate the rest of the sequence\n",
    "for _ in range(10):\n",
    "    next_st = next_state(sequence[-2], sequence[-1])\n",
    "    sequence.append(next_st)\n",
    "\n",
    "print(\"Simulated Weather Sequence (Second Order):\")\n",
    "print(sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be44dd4",
   "metadata": {},
   "source": [
    "### Limitations of Markov Models\n",
    "\n",
    "Despite their simplicity and ease of understanding, Markov models come with several limitations:\n",
    "\n",
    "- **Finite Context:**  \n",
    "  Because the model only considers a fixed number of previous states, it is unable to capture long-term dependencies. This finite context might be insufficient for tasks where earlier events have a significant influence on future outcomes.\n",
    "\n",
    "- **Lack of Memory:**  \n",
    "  Markov models inherently have no memory beyond their defined order. They cannot retain information about events outside of the fixed context window, such as long-term trends or seasonal effects in data.\n",
    "\n",
    "- **Stationarity Assumption:**  \n",
    "  Markov models assume that the transition probabilities remain constant over time. In many real-world situations, the underlying data-generating processes may change, which can reduce the effectiveness of the model if these changes are not taken into account.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae2bb25",
   "metadata": {},
   "source": [
    "## RNNs – Recurrent Neural Networks\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are a powerful class of neural networks specifically designed to handle sequential data. Unlike traditional feedforward networks that assume each input is independent, RNNs introduce a notion of \"memory\" through a hidden state that is recursively updated. This allows them to capture information from previous inputs, making them well-suited for tasks where context matters.\n",
    "\n",
    "![](FNN_RNN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cfbc25",
   "metadata": {},
   "source": [
    "### How RNNs Work\n",
    "\n",
    "At each time step, an RNN takes an input (e.g., a word or a data point) and combines it with its hidden state from the previous time step to produce a new hidden state. This process can be expressed mathematically as:\n",
    "\n",
    "$$\n",
    "h_t = f(W_{xh} \\, x_t + W_{hh} \\, h_{t-1} + b_h)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $x_t$ is the input at time $t$,\n",
    "- $h_t$ is the hidden state at time $t$,\n",
    "- $W_{xh}$ and $W_{hh}$ are weight matrices,\n",
    "- $b_h$ is a bias term,\n",
    "- $f$ is an activation function (such as tanh or ReLU).\n",
    "\n",
    "The key idea is that the hidden state $h_t$ carries information from previous time steps, allowing the network to maintain a form of memory. This recursive update mechanism enables RNNs to handle varying context sizes without needing a fixed-length input.\n",
    "\n",
    "![](RNN_Representation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11834d56",
   "metadata": {},
   "source": [
    "### Auto-Regressive Models\n",
    "\n",
    "RNNs are inherently auto-regressive models. This means that they predict the value at time $t$ based on a function of the previous values (e.g., $t-1$, $t-2$, etc.). In the context of language modeling, the probability of a sequence is modeled as:\n",
    "\n",
    "$$\n",
    "P(x_1, x_2, \\dots, x_T) = \\prod_{t=1}^{T} P(x_t \\mid x_1, x_2, \\dots, x_{t-1})\n",
    "$$\n",
    "\n",
    "The auto-regressive property enables RNNs to generate coherent sequences, as each prediction is conditioned on the sequence generated so far."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cf81c4",
   "metadata": {},
   "source": [
    "### Generating Sequences Using RNNs\n",
    "\n",
    "One common application of RNNs is sequence generation. For instance, when generating text, an RNN is trained to predict the next word given a sequence of previous words. During generation, the network outputs a probability distribution over the vocabulary at each step, and a word is chosen (often in an auto-regressive manner) to be fed back into the network for the next prediction.\n",
    "\n",
    "A simplified conceptual flow for generating a sentence is as follows:\n",
    "1. **Initialization:** Start with a seed word or sequence.\n",
    "2. **Prediction:** Use the RNN to predict the probability of each possible next word.\n",
    "3. **Selection:** Pick the next word based on the probability distribution.\n",
    "4. **Recursion:** Feed the chosen word back into the RNN and repeat until a stopping condition (like a special end-of-sequence token) is reached.\n",
    "\n",
    "#### Example\n",
    "\n",
    "In this example, the network is trained to predict the next token given a sequence. The generate_sequence function demonstrates the auto-regressive generation process by feeding back each predicted token into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f90268f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-03 15:55:37.226896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-03 15:55:44.819653: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "16/16 [==============================] - 2s 21ms/step - loss: 8.5178\n",
      "Epoch 2/3\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 7.8444\n",
      "Epoch 3/3\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 7.1248\n",
      "Generated Sequence with RNN: [2041, 3989, 895, 3355, 1323, 4535, 1648, 3064, 3901, 1617, 3923, 3615, 2982, 3681, 281, 2468, 2265, 2644, 1793, 3102, 649, 1204, 1471, 513, 2857, 1378, 2057, 2021, 1616, 3874, 410, 410, 410, 410, 410, 410, 410, 410, 410, 410, 410, 410, 410, 410, 410, 410, 410, 410, 410, 410]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "\n",
    "# Example parameters\n",
    "vocab_size = 5000      # size of our vocabulary\n",
    "embedding_dim = 64\n",
    "rnn_units = 128\n",
    "sequence_length = 30   # length of input sequences\n",
    "\n",
    "# Build a simple RNN model\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, input_length=sequence_length),\n",
    "    SimpleRNN(rnn_units),\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "# Assume you have your training data ready in variables `input_sequences` and `target_words`\n",
    "# For demonstration, we use random data:\n",
    "input_sequences = np.random.randint(0, vocab_size, (1000, sequence_length))\n",
    "target_words = np.random.randint(0, vocab_size, (1000,))\n",
    "\n",
    "model.fit(input_sequences, target_words, epochs=3, batch_size=64)\n",
    "\n",
    "# Auto-Regressive Generation\n",
    "def generate_sequence(model, seed_seq, num_tokens):\n",
    "    generated = list(seed_seq)\n",
    "    for _ in range(num_tokens):\n",
    "        # Ensure the input is of the proper sequence length\n",
    "        input_seq = np.array(generated[-sequence_length:]).reshape(1, -1)\n",
    "        preds = model.predict(input_seq, verbose=0)[0]\n",
    "        # Choose the most probable token (or sample from the distribution)\n",
    "        next_token = np.argmax(preds)\n",
    "        generated.append(next_token)\n",
    "    return generated\n",
    "\n",
    "# Generate a sequence using a random seed\n",
    "seed_seq = list(np.random.randint(0, vocab_size, sequence_length))\n",
    "generated_seq = generate_sequence(model, seed_seq, num_tokens=20)\n",
    "print(\"Generated Sequence with RNN:\", generated_seq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ba8ae6",
   "metadata": {},
   "source": [
    "### Stacked RNNs\n",
    "\n",
    "For more complex tasks, a single RNN layer might not capture all the nuances of the data. **Stacked RNNs** address this by employing multiple layers of RNNs. In a stacked RNN, the hidden states produced by one layer become the inputs for the next layer. This allows the network to learn hierarchical representations of the data:\n",
    "- **Lower layers** may capture local, short-term dependencies.\n",
    "- **Higher layers** may capture more abstract, long-term patterns.\n",
    "\n",
    "Stacking layers can greatly enhance the model's ability to understand complex sequential patterns, though it requires more computational resources and can make training more challenging.\n",
    "\n",
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ad43bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-03 16:54:14.971179: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-03 16:54:20.173872: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "16/16 [==============================] - 3s 42ms/step - loss: 8.5196\n",
      "Epoch 2/3\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 7.6668\n",
      "Epoch 3/3\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 6.9996\n",
      "Generated Sequence with Stacked RNN: [508, 4308, 2310, 4743, 934, 2476, 4721, 2064, 1699, 3853, 1600, 2155, 1112, 2120, 2796, 1250, 3541, 4650, 4067, 905, 3729, 2400, 1857, 3407, 1798, 1857, 1379, 396, 3682, 159, 2026, 2026, 2026, 2026, 2026, 2026, 2026, 2026, 2026, 2026, 2026, 2026, 2026, 2026, 2026, 2026, 2026, 2026, 2026, 2026]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "\n",
    "# Example parameters\n",
    "vocab_size = 5000\n",
    "embedding_dim = 64\n",
    "rnn_units = 128\n",
    "sequence_length = 30\n",
    "\n",
    "# Build a stacked RNN model with two RNN layers\n",
    "stacked_model = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, input_length=sequence_length),\n",
    "    SimpleRNN(rnn_units, return_sequences=True),  # returns output at each time step\n",
    "    SimpleRNN(rnn_units),  # only returns the last output\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "\n",
    "stacked_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "# Use the same random data as before for demonstration:\n",
    "input_sequences = np.random.randint(0, vocab_size, (1000, sequence_length))\n",
    "target_words = np.random.randint(0, vocab_size, (1000,))\n",
    "\n",
    "stacked_model.fit(input_sequences, target_words, epochs=3, batch_size=64)\n",
    "\n",
    "# Generating a sequence from the stacked model is similar to before:\n",
    "def generate_sequence_stacked(model, seed_seq, num_tokens):\n",
    "    generated = list(seed_seq)\n",
    "    for _ in range(num_tokens):\n",
    "        input_seq = np.array(generated[-sequence_length:]).reshape(1, -1)\n",
    "        preds = model.predict(input_seq, verbose=0)[0]\n",
    "        next_token = np.argmax(preds)\n",
    "        generated.append(next_token)\n",
    "    return generated\n",
    "\n",
    "seed_seq = list(np.random.randint(0, vocab_size, sequence_length))\n",
    "generated_seq = generate_sequence_stacked(stacked_model, seed_seq, num_tokens=20)\n",
    "print(\"Generated Sequence with Stacked RNN:\", generated_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c70d55",
   "metadata": {},
   "source": [
    "## Other Options: LSTM and GRU\n",
    "\n",
    "Standard RNNs can struggle with learning long-term dependencies due to issues like vanishing and exploding gradients. Two popular alternatives have been developed to address these challenges:\n",
    "\n",
    "- **LSTM (Long Short-Term Memory):**  \n",
    "  LSTMs introduce gating mechanisms (input, forget, and output gates) that allow the network to control the flow of information and maintain long-term dependencies more effectively.\n",
    "\n",
    "- **GRU (Gated Recurrent Unit):**  \n",
    "  GRUs are a simplified version of LSTMs that combine the forget and input gates into a single update gate. They are often faster to train and can perform comparably well on many tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee8ede1",
   "metadata": {},
   "source": [
    "### LSTM Example\n",
    "\n",
    "LSTM (Long Short-Term Memory) networks are a variant of RNNs designed to overcome the vanishing gradient problem and better capture long-term dependencies. They use gating mechanisms (input, forget, and output gates) to control the flow of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ad49e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "16/16 [==============================] - 3s 44ms/step - loss: 8.5171\n",
      "Epoch 2/3\n",
      "16/16 [==============================] - 1s 42ms/step - loss: 8.3309\n",
      "Epoch 3/3\n",
      "16/16 [==============================] - 1s 42ms/step - loss: 7.5151\n",
      "Generated Sequence with LSTM: [1946, 574, 3982, 789, 1761, 3594, 1466, 4348, 4689, 1863, 733, 4377, 2897, 4716, 2403, 2770, 2739, 2831, 1955, 4915, 3913, 3152, 4112, 16, 508, 3553, 653, 297, 4136, 4177, 3840, 3840, 3840, 3840, 3840, 3840, 3840, 3840, 3840, 3840, 3840, 3840, 3840, 3840, 3840, 3840, 3840, 3840, 3840, 3840]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Example parameters\n",
    "vocab_size = 5000\n",
    "embedding_dim = 64\n",
    "lstm_units = 128\n",
    "sequence_length = 30\n",
    "\n",
    "# Build a simple LSTM model\n",
    "lstm_model = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, input_length=sequence_length),\n",
    "    LSTM(lstm_units),\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "\n",
    "lstm_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "# Again, using random data for demonstration\n",
    "input_sequences = np.random.randint(0, vocab_size, (1000, sequence_length))\n",
    "target_words = np.random.randint(0, vocab_size, (1000,))\n",
    "\n",
    "lstm_model.fit(input_sequences, target_words, epochs=3, batch_size=64)\n",
    "\n",
    "# Auto-Regressive Generation for LSTM\n",
    "def generate_sequence_lstm(model, seed_seq, num_tokens):\n",
    "    generated = list(seed_seq)\n",
    "    for _ in range(num_tokens):\n",
    "        input_seq = np.array(generated[-sequence_length:]).reshape(1, -1)\n",
    "        preds = model.predict(input_seq, verbose=0)[0]\n",
    "        next_token = np.argmax(preds)\n",
    "        generated.append(next_token)\n",
    "    return generated\n",
    "\n",
    "seed_seq = list(np.random.randint(0, vocab_size, sequence_length))\n",
    "generated_seq = generate_sequence_lstm(lstm_model, seed_seq, num_tokens=20)\n",
    "print(\"Generated Sequence with LSTM:\", generated_seq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d24689",
   "metadata": {},
   "source": [
    "### Comparison\n",
    "\n",
    "**Basic RNN vs. LSTM:**  \n",
    "Both models work in an auto-regressive fashion to generate sequences. However, LSTM networks include internal gating mechanisms that allow them to better remember information over longer sequences. This means that while a simple RNN may struggle with longer context due to vanishing gradients, an LSTM is more robust in capturing long-term dependencies.\n",
    "\n",
    "**Model Complexity:**  \n",
    "LSTM units are more complex than SimpleRNN cells because they maintain additional parameters (gates) to control memory. This typically results in better performance on tasks requiring long-term context, at the cost of increased computational resources.\n",
    "\n",
    "**Stacked Architectures:**  \n",
    "Just as with RNNs, LSTMs can also be stacked to further enhance learning. The approach is nearly identical—using the `return_sequences=True` parameter to ensure that intermediate LSTM layers pass their full sequence outputs to the next layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c37816",
   "metadata": {},
   "source": [
    "## Full Example: Using The Shakespeare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d4e971",
   "metadata": {},
   "source": [
    "### TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "662c8251",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-03 16:28:48.898626: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c45f0b0",
   "metadata": {},
   "source": [
    "#### Download And Prepare The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db2211ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us\n"
     ]
    }
   ],
   "source": [
    "# Download the Shakespeare text\n",
    "path_to_file = tf.keras.utils.get_file(\"shakespeare.txt\",\n",
    "                                       \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\")\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "print(\"Length of text:\", len(text))\n",
    "print(text[0:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1efae9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique characters: 65\n"
     ]
    }
   ],
   "source": [
    "# Create a vocabulary from the text\n",
    "vocab = sorted(set(text))\n",
    "vocab_size = len(vocab)\n",
    "print(\"Unique characters:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "002bb486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map characters to indices and vice versa\n",
    "char2idx = {u: i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80da141",
   "metadata": {},
   "source": [
    "#### Prepare The Dataset For TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39f24b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-03 16:28:58.088913: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Create training examples and targets\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text) // (seq_length + 1)\n",
    "\n",
    "# Create a TensorFlow Dataset of character sequences\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)\n",
    "\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e05451e",
   "metadata": {},
   "source": [
    "#### Build And Train The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb8777c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "172/172 [==============================] - 296s 2s/step - loss: 3.4235\n",
      "Epoch 2/3\n",
      "172/172 [==============================] - 237s 1s/step - loss: 2.7515\n",
      "Epoch 3/3\n",
      "172/172 [==============================] - 247s 1s/step - loss: 2.2415\n"
     ]
    }
   ],
   "source": [
    "# Batch and shuffle the data\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "# Build a stacked RNN model (using two SimpleRNN layers)\n",
    "embedding_dim = 256\n",
    "rnn_units = 1024\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[BATCH_SIZE, None]),\n",
    "    tf.keras.layers.SimpleRNN(rnn_units, return_sequences=True, stateful=True,\n",
    "                              recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.SimpleRNN(rnn_units, return_sequences=True, stateful=True,\n",
    "                              recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
    "\n",
    "# Train the model (for demonstration, we train for a few epochs)\n",
    "model.fit(dataset, epochs=3)\n",
    "\n",
    "# Save the weights after training\n",
    "model.save_weights('shakespeare_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07ebb6b",
   "metadata": {},
   "source": [
    "#### Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0330cce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO: YZO&$G&G&&&&R$&ZJ$&&&&G&$&&3W&G$$V&3VZK$&&&$&&R&G&$&&&RI&F$JG&&$&$$&&$$&&&$$&$&&&&&&&&&N&&&$x$c$J$&&&&g&&&&$$&&&&&&&V&RY$j&$&&$X3Z&&N$$z&$G&&&3$&SY$R-&NV$&$Z&&&$$&MGVV&&&KV$$$$F&&X&&&&F3Z&&&R3GJ&$&N&&SY$&&&&$&&BJ&PB&&&$&&$&&$&&&&&v&$$&$$&&&$$&N$$NG&$&N&&W$&&&&CIORP$&$$&R&&$X$&&$G&Z$&&NF$&&$C&&3$$&W&&&&&&K$$PG$j$V$CBJ&&MZ&&V$W$$&N$$z$XJ$M&J$&&3TW&RQ&$&$&$&&&RS$&&G$$&n$$&&R&R3K$$&$&&$$$&&&S$$&$&&$$S$$$$&&&&$$&&&&$&RG&&VUS&JRY$&&Z$&&&$$z&J&VG$$O&&Z&&&&&&&G$I$&$&&&P&&$&&&&F&&$&$&&M&X&&&&&NCE&&J&M&$B\n"
     ]
    }
   ],
   "source": [
    "# Rebuild the model for inference with batch size 1\n",
    "inference_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[1, None]),\n",
    "    tf.keras.layers.SimpleRNN(rnn_units, return_sequences=True, stateful=True,\n",
    "                              recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.SimpleRNN(rnn_units, return_sequences=True, stateful=True,\n",
    "                              recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "])\n",
    "\n",
    "# Load the weights into the inference model\n",
    "inference_model.load_weights('shakespeare_weights.h5')\n",
    "\n",
    "# Make sure to build the model\n",
    "inference_model.build(tf.TensorShape([1, None]))\n",
    "\n",
    "def generate_text(model, start_string, num_generate=500):\n",
    "    # Convert start string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    \n",
    "    text_generated = []\n",
    "    temperature = 1.0  # Lower temperature results in more predictable text\n",
    "    \n",
    "    model.reset_states()\n",
    "    for _ in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "        \n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "        \n",
    "    return start_string + ''.join(text_generated)\n",
    "\n",
    "print(generate_text(inference_model, start_string=\"ROMEO: \"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2e0759",
   "metadata": {},
   "source": [
    "#### What is happening with the output?\n",
    "\n",
    "The gibberish output is most likely due to one or more of the following factors:\n",
    "\n",
    "1. **Insufficient Training:**  \n",
    "   Training for only a few epochs (e.g., 3 epochs in the example) on a complex dataset like Shakespeare might not be enough for the model to learn coherent patterns. Increasing the number of epochs can help the model learn more meaningful representations.\n",
    "\n",
    "2. **Temperature Parameter:**  \n",
    "   The temperature parameter in the text generation function controls the randomness of the predictions. A higher temperature (e.g., 1.0 or above) produces more random outputs, while a lower temperature (e.g., 0.5) makes the model's predictions more conservative. Try lowering the temperature to see if it results in more coherent text.\n",
    "\n",
    "3. **Model Architecture and Capacity:**  \n",
    "   Although stacking RNN layers increases the model’s capacity, the architecture might still require tuning (e.g., number of units, number of layers) to better capture the complexity of the data. Experimenting with different architectures or even switching to LSTM/GRU (which tend to handle long-range dependencies better) might yield better results.\n",
    "\n",
    "4. **Preprocessing and Data Quality:**  \n",
    "   Ensure that your data preprocessing is done correctly. For character-level models, even small issues in tokenization or data batching can affect output quality.\n",
    "\n",
    "5. **Stateful vs. Stateless Training:**  \n",
    "   Stateful RNNs require careful handling of state between batches. When generating text, make sure the state is correctly reset and that the inference model is built with the appropriate batch size (as done with the inference model). If there’s any mismatch, the output can suffer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f17b5bc",
   "metadata": {},
   "source": [
    "#### How can we improve it? \n",
    "\n",
    "- **Train for More Epochs:**  \n",
    "  Increase the number of epochs significantly. Models trained on large text corpora like Shakespeare usually require many epochs (tens to hundreds) to generate coherent output.\n",
    "  \n",
    "- **Adjust the Temperature:**  \n",
    "  Experiment with lowering the temperature in the `generate_text` function. For example, try setting `temperature = 0.5` or even lower to see if the output improves.\n",
    "  \n",
    "- **Experiment with Model Variants:**  \n",
    "  - Try using LSTM layers instead of SimpleRNN layers.\n",
    "  - Alternatively, use a GRU, which might offer a balance between complexity and performance.\n",
    "  \n",
    "- **Check the Data Pipeline:**  \n",
    "  Make sure the data is being batched and fed to the model correctly. An issue in data preparation can propagate through and affect the generated output.\n",
    "\n",
    "By fine-tuning these factors, you should be able to get more coherent text generation from your model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c00d721",
   "metadata": {},
   "source": [
    "### PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "701ef434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92234480",
   "metadata": {},
   "source": [
    "#### Download And Prepare The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44376eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394\n"
     ]
    }
   ],
   "source": [
    "# Download the Shakespeare text\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "response = requests.get(url)\n",
    "text = response.text\n",
    "print(\"Length of text:\", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "140d2f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique characters: 65\n"
     ]
    }
   ],
   "source": [
    "# Create a vocabulary from the text\n",
    "vocab = sorted(set(text))\n",
    "vocab_size = len(vocab)\n",
    "print(\"Unique characters:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d62fb12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map characters to indices and vice versa\n",
    "char2idx = {ch: i for i, ch in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "text_as_int = np.array([char2idx[c] for c in text], dtype=np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd41276e",
   "metadata": {},
   "source": [
    "#### Prepare The Dataset For PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d60f28d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences from the text\n",
    "seq_length = 100\n",
    "input_sequences = []\n",
    "target_sequences = []\n",
    "for i in range(0, len(text_as_int) - seq_length):\n",
    "    input_sequences.append(text_as_int[i:i+seq_length])\n",
    "    target_sequences.append(text_as_int[i+1:i+seq_length+1])\n",
    "input_sequences = np.array(input_sequences)\n",
    "target_sequences = np.array(target_sequences)\n",
    "\n",
    "# Create a custom dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.inputs[idx]), torch.tensor(self.targets[idx])\n",
    "\n",
    "dataset = TextDataset(input_sequences, target_sequences)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d21918a",
   "metadata": {},
   "source": [
    "#### Build And Train The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd725679",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m     hidden \u001b[38;5;241m=\u001b[39m hidden\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m     31\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, vocab_size), targets\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 32\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     33\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define a stacked RNN model using nn.RNN\n",
    "class StackedRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units, num_layers=2):\n",
    "        super(StackedRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, rnn_units, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(rnn_units, vocab_size)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "\n",
    "embedding_dim = 256\n",
    "rnn_units = 512\n",
    "num_layers = 2\n",
    "model = StackedRNN(vocab_size, embedding_dim, rnn_units, num_layers)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop (simplified)\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    hidden = None\n",
    "    for inputs, targets in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs, hidden = model(inputs, hidden)\n",
    "        # Detach hidden state to prevent backpropagating through the entire training history\n",
    "        hidden = hidden.detach()\n",
    "        loss = criterion(outputs.reshape(-1, vocab_size), targets.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1} Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d5432c",
   "metadata": {},
   "source": [
    "#### Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddafb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text generation function for PyTorch\n",
    "def generate_text_pytorch(model, start_string, num_generate=500):\n",
    "    model.eval()\n",
    "    input_eval = torch.tensor([char2idx[s] for s in start_string]).unsqueeze(0)\n",
    "    hidden = None\n",
    "    generated = start_string\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_generate):\n",
    "            output, hidden = model(input_eval, hidden)\n",
    "            # Get the output for the last time step\n",
    "            output = output[:, -1, :]\n",
    "            predicted_id = torch.argmax(output, dim=1).item()\n",
    "            generated += idx2char[predicted_id]\n",
    "            input_eval = torch.tensor([[predicted_id]])\n",
    "    return generated\n",
    "\n",
    "print(generate_text_pytorch(model, start_string=\"ROMEO: \"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
