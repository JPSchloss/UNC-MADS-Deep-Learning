{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e2f0af9",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7162a9",
   "metadata": {},
   "source": [
    "---\n",
    "## Encoder-Decoder Architecture\n",
    "\n",
    "The encoder–decoder framework is a foundational approach designed to convert one sequence into another. It is often used in sequence-to-sequence learning and has been applied to a wide range of tasks, including language translation, text summarization and speech recognition.\n",
    "\n",
    "The model is built to handle pairs of sequences. The input sequence (e.g., a sentence in English) is transformed into an output sequence (e.g., the same sentence translated into French).\n",
    "\n",
    "The architecture divides the overall task into two main components:\n",
    " - **Encoder:** Processes the entire input sequence and compresses it into a compact representation, often called a context vector.\n",
    " - **Decoder:** Uses this context vector to generate the output sequence, one token at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711db6f0",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/v2/resize:fit:1400/1*1JcHGUU7rFgtXC_mydUA_Q.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a1b641",
   "metadata": {},
   "source": [
    "### How It Works\n",
    "\n",
    "1. **Encoding:**\n",
    "\n",
    "The encoder reads the input sequence, processing it token by token. At each step, it updates its internal state to capture the context and semantics of the input. The final hidden state of the encoder is then treated as a distilled summary of the entire input sequence. This summary is expected to capture all relevant information needed for generating the output.\n",
    "\n",
    "2. **Decoding:**\n",
    "\n",
    "Once the encoder has produced the context vector, the decoder begins generating the output sequence. Starting with an initial state (often derived from the context vector), the decoder predicts the first output token. It then uses that token, along with its current state and the context vector, to predict the next token, and so on. This process continues until a special end-of-sequence token is generated, signaling that the output is complete.\n",
    "\n",
    "\n",
    "### Strengths And Weaknesses\n",
    "\n",
    "1. **Strengths:**\n",
    "    - **Modularity:** By separating the encoding and decoding processes, the model can be more flexible and easier to adapt to different tasks.\n",
    "    - **Applicability:** The architecture is versatile and has been successfully applied to numerous tasks involving sequence transformations.\n",
    "    \n",
    "2. **Limitations:**\n",
    "    - **Information Bottleneck:** Compressing an entire input sequence into a single fixed-length context vector can result in the loss of fine-grained details, particularly for long or complex inputs.\n",
    "    - **Sequential Dependency:** Traditional implementations often rely on sequential processing (using RNNs, for example), which can make it challenging to capture long-range dependencies in the input.\n",
    "    \n",
    "\n",
    "The encoder–decoder model set the stage for subsequent innovations in sequence modeling. While early implementations often used Recurrent Neural Networks (RNNs) for both the encoder and decoder, the fundamental idea of transforming one sequence into another has inspired a range of advanced architectures. These include models that incorporate attention mechanisms to alleviate the information bottleneck and, eventually, the development of transformer architectures that further improve on both performance and scalability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb37087c",
   "metadata": {},
   "source": [
    "---\n",
    "## Encoder - Decoder RNNs\n",
    "\n",
    "The encoder–decoder model with RNNs is a specific implementation of the general encoder–decoder framework, where both the encoder and decoder are built using recurrent neural networks. This configuration leverages the sequential processing capabilities of RNNs to capture the temporal dynamics inherent in language, speech, and other sequential data.\n",
    "\n",
    "In traditional sequence-to-sequence tasks, the model must transform an input sequence into an output sequence, such as converting a sentence in one language to another. RNNs naturally handle sequential data by updating their hidden state with each new token. However, when RNNs are used in isolation, they can struggle with variable-length sequences and long-range dependencies. \n",
    "\n",
    "The encoder–decoder structure with RNNs was designed to address these challenges:\n",
    "\n",
    "**Encoder RNN:**\n",
    "- **Function:** Processes the input sequence token by token.\n",
    "- **Mechanism:** At each time step, the encoder updates its hidden state using the current token and the previous hidden state. This recursive process allows the network to build an internal representation of the entire sequence.\n",
    "- **Output:** The final hidden state acts as a compressed summary (context vector) of the entire input sequence.\n",
    "- **Benefit:** Captures sequential patterns and dependencies in the input, albeit in a fixed-length vector.\n",
    "\n",
    "\n",
    "**Decoder RNN:**\n",
    "- **Function:** Generates the output sequence based on the encoded representation.\n",
    "- **Mechanism:** Starting from the context vector, the decoder predicts the output token at each time step, using its previous outputs and hidden state to generate the next token.\n",
    "- **Output:** A sequence of tokens that represents the target sequence, such as a translated sentence.\n",
    "- **Benefit:** Provides a structured way to generate sequences while maintaining context across time steps.\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "#### Encoder\n",
    "\n",
    "Given an input sequence $ X = \\{x_1, x_2, \\dots, x_T\\} $, the encoder processes each token step-by-step using a recurrent formula:\n",
    "$$\n",
    "h_t = f(W_{xh} \\, x_t + W_{hh} \\, h_{t-1} + b_h)\n",
    "$$\n",
    "- $ h_t $: Hidden state at time $ t $\n",
    "- $ W_{xh} $ and $ W_{hh} $: Weight matrices\n",
    "- $ b_h $: Bias term\n",
    "- $ f $: Activation function (e.g., $\\tanh$, ReLU)\n",
    "\n",
    "The final hidden state $ h_T $ serves as the **context vector**:\n",
    "$$\n",
    "c = h_T\n",
    "$$\n",
    "\n",
    "#### Decoder\n",
    "\n",
    "The decoder RNN generates the output sequence $ Y = \\{y_1, y_2, \\dots, y_{T'}\\} $ conditioned on the context vector $ c $. Its recurrence is:\n",
    "$$\n",
    "s_t = f(W_{ys} \\, y_{t-1} + W_{cs} \\, c + W_{ss} \\, s_{t-1} + b_s)\n",
    "$$\n",
    "\n",
    "- $ s_t $: Decoder hidden state at time $ t $\n",
    "- $ y_{t-1} $: Previously generated output (usually embedded)\n",
    "- $ W_{ys} $, $ W_{cs} $, $ W_{ss} $: Weight matrices\n",
    "- $ b_s $: Bias term\n",
    "\n",
    "The output token is computed as:\n",
    "$$\n",
    "y_t = \\text{softmax}(W_{out} \\, s_t + b_{out})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177e712a",
   "metadata": {},
   "source": [
    "![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTBvrDTed79lFHC8GMLQ757v11n_Y1nV0V_1Q&s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8851a60d",
   "metadata": {},
   "source": [
    "### Limitations With This Approach\n",
    "\n",
    "While encoder–decoder RNNs were a significant breakthrough, they come with inherent limitations:\n",
    "\n",
    "- **Fixed-Length Context Vector:**\n",
    "Compressing an entire input sequence into a single vector may result in loss of important details, particularly for long or complex sequences. This fixed-size bottleneck can limit performance when the input contains intricate or diverse information.\n",
    "- **Sequential Bottleneck in Decoding:**\n",
    "Because the decoder relies on sequential prediction, the generation process can be slow and may struggle with maintaining long-term dependencies over extended outputs.\n",
    "\n",
    "These challenges led to the development of advanced techniques like attention mechanisms, which allow the decoder to dynamically refer back to different parts of the input sequence during generation, thereby alleviating the information bottleneck of a fixed-length context vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18075c5c",
   "metadata": {},
   "source": [
    "### TensorFlow Code Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c326fcc7",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fa90e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-16 23:02:34.458538: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89348ae4",
   "metadata": {},
   "source": [
    "#### Define The Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fdfc11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
    "        # return_sequences=True to get outputs at all time steps\n",
    "        # return_state=True to get the final hidden state\n",
    "        self.gru = GRU(enc_units, return_sequences=True, return_state=True)\n",
    "    \n",
    "    def call(self, x, hidden):\n",
    "        # x: (batch_size, sequence_length)\n",
    "        x = self.embedding(x)  # (batch_size, sequence_length, embedding_dim)\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.enc_units))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804b0698",
   "metadata": {},
   "source": [
    "#### Define The Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fb41de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = GRU(dec_units, return_sequences=True, return_state=True)\n",
    "        self.fc = Dense(vocab_size)\n",
    "    \n",
    "    def call(self, x, hidden):\n",
    "        # x: (batch_size, 1) as we process one token at a time\n",
    "        x = self.embedding(x)  # (batch_size, 1, embedding_dim)\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        # reshape output from (batch_size, 1, dec_units) to (batch_size, dec_units)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        x = self.fc(output)  # (batch_size, vocab_size)\n",
    "        return x, state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344270bb",
   "metadata": {},
   "source": [
    "#### Set Up Training Parameters And Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a55616f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-16 23:05:03.036286: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "vocab_inp_size = 10   # Input vocabulary size\n",
    "vocab_tar_size = 10   # Target vocabulary size\n",
    "embedding_dim = 16    # Embedding dimension\n",
    "units = 16            # Number of GRU units\n",
    "BATCH_SIZE = 1        # Batch size (for simplicity)\n",
    "\n",
    "# Create sample input and target sequences (batch_size=1)\n",
    "input_seq = tf.constant([[1, 2, 3, 4, 5]], dtype=tf.int32)   # shape: (1, sequence_length)\n",
    "target_seq = tf.constant([[1, 2, 3, 4, 6]], dtype=tf.int32)\n",
    "\n",
    "# Initialize encoder and decoder models\n",
    "encoder = Encoder(vocab_inp_size, embedding_dim, units)\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units)\n",
    "\n",
    "# Define the optimizer and the loss function\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8295072",
   "metadata": {},
   "source": [
    "#### Define Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a6041c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(input_seq, target_seq, teacher_forcing_ratio=0.5):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Initialize encoder hidden state\n",
    "        enc_hidden = encoder.initialize_hidden_state(BATCH_SIZE)\n",
    "        # Encode the input sequence\n",
    "        enc_output, enc_hidden = encoder(input_seq, enc_hidden)\n",
    "        \n",
    "        # Set initial state for decoder as the encoder's final hidden state\n",
    "        dec_hidden = enc_hidden\n",
    "        # Start-of-sequence token (here assumed to be index 0)\n",
    "        dec_input = tf.expand_dims([0] * BATCH_SIZE, 1)  # shape: (batch_size, 1)\n",
    "        \n",
    "        # Iterate over each token in the target sequence\n",
    "        for t in range(target_seq.shape[1]):\n",
    "            # Pass the current token and state through the decoder\n",
    "            predictions, dec_hidden = decoder(dec_input, dec_hidden)\n",
    "            # Compute loss comparing predicted token with actual target token\n",
    "            loss += loss_object(target_seq[:, t], predictions)\n",
    "            \n",
    "            # Decide whether to use teacher forcing\n",
    "            if np.random.rand() < teacher_forcing_ratio:\n",
    "                # Teacher forcing: feed the target token as the next input\n",
    "                dec_input = tf.expand_dims(target_seq[:, t], 1)\n",
    "            else:\n",
    "                # Without teacher forcing: use the decoder's prediction as the next input\n",
    "                predicted_ids = tf.argmax(predictions, axis=1, output_type=tf.int32)\n",
    "                dec_input = tf.expand_dims(predicted_ids, 1)\n",
    "    \n",
    "    # Compute gradients and update model parameters\n",
    "    batch_loss = loss / int(target_seq.shape[1])\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    \n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7c6e5d",
   "metadata": {},
   "source": [
    "#### Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51828ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.3087447\n"
     ]
    }
   ],
   "source": [
    "loss_val = train_step(input_seq, target_seq)\n",
    "print(\"Loss:\", loss_val.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41472da0",
   "metadata": {},
   "source": [
    "### PyTorch Code Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f3f760",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b82aa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca433ef6",
   "metadata": {},
   "source": [
    "#### Define The Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98b8744",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        # Embed the input token and reshape for the GRU\n",
    "        embedded = self.embedding(input).view(1, 1, self.hidden_size)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0774509",
   "metadata": {},
   "source": [
    "#### Define The Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a323e36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        # Embed the input token and reshape for the GRU\n",
    "        embedded = self.embedding(input).view(1, 1, self.hidden_size)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e68ce00",
   "metadata": {},
   "source": [
    "#### Training Loop For One Pair Of Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab87405",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Hyperparameters\n",
    "    input_size = 10    # vocabulary size for input\n",
    "    output_size = 10   # vocabulary size for output\n",
    "    hidden_size = 16\n",
    "    teacher_forcing_ratio = 0.5\n",
    "\n",
    "    # Example input and target sequences (represented as indices)\n",
    "    input_seq = torch.tensor([1, 2, 3, 4, 5], dtype=torch.long)   # Input sequence of length 5\n",
    "    target_seq = torch.tensor([1, 2, 3, 4, 6], dtype=torch.long)  # Target sequence of length 5\n",
    "\n",
    "    encoder = EncoderRNN(input_size, hidden_size)\n",
    "    decoder = DecoderRNN(hidden_size, output_size)\n",
    "    \n",
    "    # Optimizers and loss function for training\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=0.01)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=0.01)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    # Zero gradients\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # Encode the input sequence\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    for token in input_seq:\n",
    "        encoder_output, encoder_hidden = encoder(token.unsqueeze(0), encoder_hidden)\n",
    "    \n",
    "    # Initialize decoder: use a start-of-sequence token (e.g., index 0)\n",
    "    decoder_input = torch.tensor([0], dtype=torch.long)\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    loss = 0\n",
    "    # Decode the sequence using teacher forcing\n",
    "    for di, target_token in enumerate(target_seq):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        loss += criterion(decoder_output, target_token.unsqueeze(0))\n",
    "        \n",
    "        # Decide if we are going to use teacher forcing or not\n",
    "        use_teacher_forcing = True if torch.rand(1).item() < teacher_forcing_ratio else False\n",
    "        \n",
    "        if use_teacher_forcing:\n",
    "            # Teacher forcing: next input is current target token\n",
    "            decoder_input = target_token.unsqueeze(0)\n",
    "        else:\n",
    "            # Without teacher forcing: next input is decoder's own prediction\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach().unsqueeze(0)\n",
    "\n",
    "    # Backpropagate the error and update weights\n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    print(f\"Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70913dc3",
   "metadata": {},
   "source": [
    "## RNNs with Attention\n",
    "\n",
    "To address the bottleneck problem in encoder–decoder architectures, the attention mechanism was introduced. Instead of relying solely on the final encoder state, attention allows the decoder to dynamically focus on different parts of the input sequence at each step of the output generation. This results in a more flexible model that can better capture long-range dependencies.\n",
    "\n",
    "### How Attention Works\n",
    "\n",
    "#### Alignment Scores\n",
    "\n",
    "At each decoding step $ t $, the model calculates an alignment score between the decoder’s previous hidden state $ s_{t-1} $ and each encoder hidden state $ h_j $. \n",
    "\n",
    "For instance, in Bahdanau attention:\n",
    "$$\n",
    "e_{tj} = \\text{score}(s_{t-1}, h_j) = v_a^\\top \\tanh(W_a s_{t-1} + U_a h_j)\n",
    "$$\n",
    "\n",
    "- $ v_a $, $ W_a $, $ U_a $: Learnable parameters\n",
    "\n",
    "#### Attention Weights and Context Vector\n",
    "\n",
    "The alignment scores are normalized using the softmax function to yield attention weights:\n",
    "$$\n",
    "\\alpha_{tj} = \\frac{\\exp(e_{tj})}{\\sum_{k=1}^{T} \\exp(e_{tk})}\n",
    "$$\n",
    "\n",
    "These weights are then used to compute a dynamic context vector for each decoding step:\n",
    "$$\n",
    "c_t = \\sum_{j=1}^{T} \\alpha_{tj} h_j\n",
    "$$\n",
    "\n",
    "This context vector provides targeted information from the input sequence, allowing the decoder to focus on the most relevant parts.\n",
    "\n",
    "#### Updated Decoder Computation\n",
    "\n",
    "The decoder now incorporates the dynamic context vector $ c_t $ into its recurrence:\n",
    "$$\n",
    "s_t = f(W_{ys} \\, y_{t-1} + W_{cs} \\, c_t + W_{ss} \\, s_{t-1} + b_s)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91254ef",
   "metadata": {},
   "source": [
    "### TensorFlow Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc577905",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4b389b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fbc621",
   "metadata": {},
   "source": [
    "#### Define The Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5176088",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = GRU(enc_units, return_sequences=True, return_state=True)\n",
    "    \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)  # (batch, seq_len, embedding_dim)\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.enc_units))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f451daec",
   "metadata": {},
   "source": [
    "#### Bahdanau Attention Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dc19225",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = Dense(units)\n",
    "        self.W2 = Dense(units)\n",
    "        self.V = Dense(1)\n",
    "    \n",
    "    def call(self, query, values):\n",
    "        # query: decoder hidden state at current time step (batch, hidden)\n",
    "        # values: encoder outputs (batch, seq_len, hidden)\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)  # (batch, 1, hidden)\n",
    "        # Score: (batch, seq_len, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(query_with_time_axis) + self.W2(values)))\n",
    "        # Attention weights: (batch, seq_len, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        # Context vector: weighted sum of encoder outputs (batch, hidden)\n",
    "        context_vector = attention_weights * values  # (batch, seq_len, hidden)\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e986a85",
   "metadata": {},
   "source": [
    "#### Define The Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e409e35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = GRU(dec_units, return_sequences=True, return_state=True)\n",
    "        self.fc = Dense(vocab_size)\n",
    "        \n",
    "        self.attention = BahdanauAttention(dec_units)\n",
    "    \n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # x: (batch, 1) -> current input token\n",
    "        x = self.embedding(x)  # (batch, 1, embedding_dim)\n",
    "        # Calculate attention\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "        context_vector = tf.expand_dims(context_vector, 1)  # (batch, 1, dec_units)\n",
    "        # Concatenate context vector with embedding\n",
    "        x = Concatenate(axis=-1)([context_vector, x])  # (batch, 1, dec_units+embedding_dim)\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))  # (batch, dec_units)\n",
    "        x = self.fc(output)  # (batch, vocab_size)\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee3aba4",
   "metadata": {},
   "source": [
    "#### Training Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3926afc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Loss: 2.3010743\n"
     ]
    }
   ],
   "source": [
    "vocab_inp_size = 10\n",
    "vocab_tar_size = 10\n",
    "embedding_dim = 16\n",
    "units = 16\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "# Sample input and target sequences (batch size = 1)\n",
    "input_seq = tf.constant([[1, 2, 3, 4, 5]], dtype=tf.int32)   # (batch, seq_len)\n",
    "target_seq = tf.constant([[1, 2, 3, 4, 6]], dtype=tf.int32)\n",
    "\n",
    "encoder = Encoder(vocab_inp_size, embedding_dim, units)\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "@tf.function\n",
    "def train_step(input_seq, target_seq, teacher_forcing_ratio=0.5):\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        batch_size = input_seq.shape[0]\n",
    "        enc_hidden = encoder.initialize_hidden_state(batch_size)\n",
    "        enc_output, enc_hidden = encoder(input_seq, enc_hidden)\n",
    "        \n",
    "        dec_hidden = enc_hidden\n",
    "        # Start-of-sequence token assumed to be 0\n",
    "        dec_input = tf.expand_dims([0] * batch_size, 1)  # (batch, 1)\n",
    "        \n",
    "        for t in range(target_seq.shape[1]):\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "            loss += loss_object(target_seq[:, t], predictions)\n",
    "            \n",
    "            if np.random.rand() < teacher_forcing_ratio:\n",
    "                dec_input = tf.expand_dims(target_seq[:, t], 1)\n",
    "            else:\n",
    "                predicted_ids = tf.argmax(predictions, axis=1, output_type=tf.int32)\n",
    "                dec_input = tf.expand_dims(predicted_ids, 1)\n",
    "    \n",
    "    batch_loss = loss / int(target_seq.shape[1])\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return batch_loss\n",
    "\n",
    "loss_val = train_step(input_seq, target_seq)\n",
    "print(\"TensorFlow Loss:\", loss_val.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b0be38",
   "metadata": {},
   "source": [
    "### PyTorch Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a0db65",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedb3c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bf6aa6",
   "metadata": {},
   "source": [
    "#### Define The Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5857de4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, self.hidden_size)  # (1, batch, hidden_size)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc24681",
   "metadata": {},
   "source": [
    "#### Bahdanau Attention Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09ad84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "    \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # hidden: (1, batch, hidden_size) current decoder hidden state\n",
    "        # encoder_outputs: (seq_len, batch, hidden_size)\n",
    "        seq_len = encoder_outputs.size(0)\n",
    "        batch_size = encoder_outputs.size(1)\n",
    "        \n",
    "        # Repeat decoder hidden state seq_len times\n",
    "        hidden = hidden.repeat(seq_len, 1, 1)  # (seq_len, batch, hidden_size)\n",
    "        # Concatenate encoder outputs and repeated hidden state along the last dim\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), 2)))  # (seq_len, batch, hidden_size)\n",
    "        # Compute alignment scores (dot product with v)\n",
    "        energy = energy.permute(1, 0, 2)  # (batch, seq_len, hidden_size)\n",
    "        v = self.v.repeat(batch_size, 1).unsqueeze(1)  # (batch, 1, hidden_size)\n",
    "        scores = torch.bmm(v, energy.permute(0, 2, 1))  # (batch, 1, seq_len)\n",
    "        attn_weights = torch.softmax(scores, dim=2)  # (batch, 1, seq_len)\n",
    "        # Compute context vector as weighted sum of encoder_outputs\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)  # (batch, seq_len, hidden_size)\n",
    "        context = torch.bmm(attn_weights, encoder_outputs)  # (batch, 1, hidden_size)\n",
    "        context = context.permute(1, 0, 2)  # (1, batch, hidden_size)\n",
    "        return context, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7d462e",
   "metadata": {},
   "source": [
    "#### Define The Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31546900",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNNWithAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNNWithAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.attention = BahdanauAttention(hidden_size)\n",
    "        # Combine context vector and embedding before feeding into GRU\n",
    "        self.gru = nn.GRU(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        # Embed input token\n",
    "        embedded = self.embedding(input).view(1, 1, self.hidden_size)\n",
    "        # Compute attention context vector\n",
    "        context, attn_weights = self.attention(hidden, encoder_outputs)\n",
    "        # Concatenate embedded input and context vector\n",
    "        rnn_input = torch.cat((embedded, context), 2)  # (1, 1, 2*hidden_size)\n",
    "        output, hidden = self.gru(rnn_input, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0e7dc0",
   "metadata": {},
   "source": [
    "#### Example Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83127b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Hyperparameters\n",
    "    input_size = 10\n",
    "    output_size = 10\n",
    "    hidden_size = 16\n",
    "    teacher_forcing_ratio = 0.5\n",
    "    \n",
    "    # Dummy input and target sequences (indices)\n",
    "    input_seq = torch.tensor([1, 2, 3, 4, 5], dtype=torch.long)    # Length = 5\n",
    "    target_seq = torch.tensor([1, 2, 3, 4, 6], dtype=torch.long)   # Length = 5\n",
    "    \n",
    "    encoder = EncoderRNN(input_size, hidden_size)\n",
    "    decoder = DecoderRNNWithAttention(hidden_size, output_size)\n",
    "    \n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=0.01)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=0.01)\n",
    "    criterion = nn.NLLLoss()\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    encoder_outputs = torch.zeros(len(input_seq), 1, hidden_size)\n",
    "    \n",
    "    # Encode the input sequence\n",
    "    for t, token in enumerate(input_seq):\n",
    "        encoder_output, encoder_hidden = encoder(token.unsqueeze(0), encoder_hidden)\n",
    "        encoder_outputs[t] = encoder_output[0]\n",
    "    \n",
    "    decoder_input = torch.tensor([0], dtype=torch.long)  # Start-of-sequence token\n",
    "    decoder_hidden = encoder_hidden\n",
    "    loss = 0\n",
    "    \n",
    "    # Decode with attention and teacher forcing\n",
    "    for t, target_token in enumerate(target_seq):\n",
    "        decoder_output, decoder_hidden, attn_weights = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "        loss += criterion(decoder_output, target_token.unsqueeze(0))\n",
    "        use_teacher_forcing = True if torch.rand(1).item() < teacher_forcing_ratio else False\n",
    "        \n",
    "        if use_teacher_forcing:\n",
    "            decoder_input = target_token.unsqueeze(0)\n",
    "        else:\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach().unsqueeze(0)\n",
    "    \n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    print(\"PyTorch Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bede958",
   "metadata": {},
   "source": [
    "## Transformers\n",
    "\n",
    "Transformers mark a paradigm shift in sequence modeling by eliminating the need for recurrent architectures. Introduced in the seminal paper \"Attention is All You Need,\" transformers rely solely on attention mechanisms and feed-forward neural networks. This design enables the model to process entire sequences in parallel, vastly improving training efficiency and scalability, particularly for long sequences.\n",
    "\n",
    "Traditional sequence models like RNNs or LSTMs process tokens sequentially, meaning that each token's representation depends on the previous ones. While effective for capturing temporal dependencies, this sequential nature makes training inefficient and limits the model's ability to capture long-range dependencies due to vanishing gradients.\n",
    "\n",
    "Transformers address these challenges by:\n",
    "\n",
    "- **Parallel Processing:**  \n",
    "  Instead of processing tokens one at a time, transformers analyze the entire sequence simultaneously. This parallelism speeds up training on modern hardware (e.g., GPUs) and allows for more efficient use of computational resources.\n",
    "\n",
    "- **Long-Range Dependency Modeling:**  \n",
    "  With attention mechanisms, each token in a sequence can directly interact with every other token. This direct connectivity helps the model capture long-range dependencies that are often lost in RNNs.\n",
    "\n",
    "- **Scalability:**  \n",
    "  The architecture scales well with increasing amounts of data and longer sequences, making it suitable for large-scale natural language processing tasks and beyond."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab4c39b",
   "metadata": {},
   "source": [
    "![](https://aiml.com/wp-content/uploads/2023/09/Annotated-Transformers-Architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ef98f2",
   "metadata": {},
   "source": [
    "## Core Building Blocks Of Transformers\n",
    "\n",
    "Transformers are composed of several key components that work together to transform input sequences into meaningful outputs. \n",
    "\n",
    "### Self-Attention\n",
    "\n",
    "Self-attention is the central mechanism that allows transformers to weigh the importance of different tokens in a sequence relative to one another. This mechanism helps the model understand context by dynamically adjusting the influence of each token based on its relationship with all other tokens.\n",
    "\n",
    "#### Key, Query, and Value\n",
    "\n",
    "Each input token is transformed into three distinct vectors:\n",
    "- **Query (Q):** Represents the current token’s request for information.\n",
    "- **Key (K):** Encodes the content of each token so that it can be matched with queries.\n",
    "- **Value (V):** Contains the actual information or features of the token that can be passed to subsequent layers.\n",
    "\n",
    "For an input matrix $X$ (with shape $T \\times d_{\\text{model}}$, where $T$ is the sequence length and $d_{\\text{model}}$ is the embedding dimension), these vectors are computed as:\n",
    "\n",
    "$$\n",
    "Q = XW^Q, \\quad K = XW^K, \\quad V = XW^V\n",
    "$$\n",
    "\n",
    "- **Learned Projections:**  \n",
    "  The weight matrices $W^Q$, $W^K$, and $W^V$ are learned during training. They transform the input embeddings into different subspaces, allowing the model to capture various aspects of the data.\n",
    "\n",
    "#### Scaled Dot-Product Attention\n",
    "\n",
    "Once the queries, keys, and values are computed, the model determines how much attention to pay to each token. This is done using the scaled dot-product attention:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "- **Dot-Product:**  \n",
    "  The product $QK^\\top$ computes the similarity between each query and all keys.\n",
    "- **Scaling Factor:**  \n",
    "  Dividing by $\\sqrt{d_k}$ (the square root of the key dimension) prevents the dot-product values from growing too large, which could lead to very small gradients when passed through the softmax function.\n",
    "- **Softmax:**  \n",
    "  The softmax function normalizes these scores into a probability distribution. Each weight in this distribution indicates the importance of the corresponding token.\n",
    "- **Weighted Sum:**  \n",
    "  The final output is a weighted sum of the value vectors $V$, where tokens with higher attention weights contribute more to the output.\n",
    "\n",
    "### Multi-Head Attention\n",
    "\n",
    "Instead of computing a single attention function, transformers use multi-head attention to capture information from multiple representation subspaces simultaneously. This is achieved by splitting the queries, keys, and values into multiple “heads,” each with its own learned projections.\n",
    "\n",
    "For each head $i$:\n",
    "\n",
    "$$\n",
    "\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
    "$$\n",
    "\n",
    "- **Multiple Heads:**  \n",
    "  Each head learns different aspects or features of the input by operating in its own subspace.\n",
    "- **Concatenation and Projection:**  \n",
    "  The outputs from all heads are concatenated and then projected using an output matrix $W^O$:\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h)W^O\n",
    "$$\n",
    "\n",
    "This allows the model to integrate diverse information from different heads, resulting in a richer representation.\n",
    "\n",
    "### Position-Wise Feed-Forward Networks\n",
    "\n",
    "After the multi-head attention layer, each token’s representation is processed independently through a feed-forward network. This network is applied identically to each position and is composed of two linear transformations with a non-linear activation function (typically ReLU) in between:\n",
    "\n",
    "$$\n",
    "\\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2\n",
    "$$\n",
    "\n",
    "- **Independent Processing:**  \n",
    "  Unlike attention, which mixes information between tokens, the feed-forward network operates on each token independently, allowing the model to learn complex transformations at each position.\n",
    "- **Enhancing Capacity:**  \n",
    "  This network increases the representational power of the model and helps it capture intricate patterns in the data.\n",
    "\n",
    "### Positional Encoding\n",
    "\n",
    "Since transformers process tokens in parallel and lack any inherent notion of token order, positional encodings are added to the input embeddings to inject information about the position of each token. A common method is to use sine and cosine functions of varying frequencies:\n",
    "\n",
    "$$\n",
    "\\text{PE}_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{PE}_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right)\n",
    "$$\n",
    "\n",
    "- **Encoding Positions:**  \n",
    "  Here, $pos$ represents the position of the token in the sequence, and $i$ indexes the dimension. The sine and cosine functions provide a smooth, continuous way to encode positional information.\n",
    "- **Incorporation into Embeddings:**  \n",
    "  These positional encodings are added directly to the token embeddings before they are processed by the transformer layers, allowing the model to learn about the order of tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ca9031",
   "metadata": {},
   "source": [
    "### TensorFlow Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d3e417a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2f0c90",
   "metadata": {},
   "source": [
    "#### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b070aa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, position, d_model):\n",
    "        \"\"\"\n",
    "        Create sinusoidal positional encodings.\n",
    "        Args:\n",
    "            position: Maximum position (sequence length).\n",
    "            d_model: Dimensionality of the embeddings.\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "    \n",
    "    def get_angles(self, pos, i, d_model):\n",
    "        angle_rates = 1 / (10000 ** (2 * (i // 2) / tf.cast(d_model, tf.float32)))\n",
    "        return pos * angle_rates\n",
    "    \n",
    "    def positional_encoding(self, position, d_model):\n",
    "        angle_rads = self.get_angles(\n",
    "            tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "            tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "            d_model)\n",
    "        \n",
    "        # Apply sin to even indices; cos to odd indices\n",
    "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "        \n",
    "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]  # Shape: (1, position, d_model)\n",
    "        return tf.cast(pos_encoding, tf.float32)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Add positional encodings to the input embeddings.\n",
    "        Args:\n",
    "            inputs: Tensor of shape (batch_size, seq_len, d_model)\n",
    "        Returns:\n",
    "            Tensor with positional encodings added.\n",
    "        \"\"\"\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f0a8c4",
   "metadata": {},
   "source": [
    "#### Building The Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3933c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, dff, num_layers, maximum_position_encoding, dropout_rate=0.1):\n",
    "        \"\"\"\n",
    "        A simple transformer for sequence-to-sequence tasks.\n",
    "        Args:\n",
    "            vocab_size: Vocabulary size (for both source and target).\n",
    "            d_model: Dimensionality of the embeddings and transformer model.\n",
    "            num_heads: Number of attention heads.\n",
    "            dff: Dimensionality of the feed-forward network.\n",
    "            num_layers: Number of encoder and decoder layers.\n",
    "            maximum_position_encoding: Maximum sequence length.\n",
    "            dropout_rate: Dropout rate.\n",
    "        \"\"\"\n",
    "        super(Transformer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Embedding layer and positional encoding for both source and target\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(maximum_position_encoding, d_model)\n",
    "        \n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        \n",
    "        # Encoder: stack of multi-head attention and feed-forward networks\n",
    "        self.enc_layers = [tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model) \n",
    "                           for _ in range(num_layers)]\n",
    "        self.ffn_layers_enc = [tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(dff, activation='relu'),\n",
    "            tf.keras.layers.Dense(d_model)\n",
    "        ]) for _ in range(num_layers)]\n",
    "        \n",
    "        # Decoder: each layer performs self-attention, cross-attention, and feed-forward\n",
    "        self.dec_layers = [tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model) \n",
    "                           for _ in range(num_layers)]\n",
    "        self.ffn_layers_dec = [tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(dff, activation='relu'),\n",
    "            tf.keras.layers.Dense(d_model)\n",
    "        ]) for _ in range(num_layers)]\n",
    "        \n",
    "        # Final dense layer to project the decoder output to the vocabulary space\n",
    "        self.final_layer = tf.keras.layers.Dense(vocab_size)\n",
    "    \n",
    "    def call(self, src, tgt, training):\n",
    "        \"\"\"\n",
    "        Forward pass of the transformer.\n",
    "        Args:\n",
    "            src: Source sequence tensor of shape (batch, src_seq_len).\n",
    "            tgt: Target sequence tensor of shape (batch, tgt_seq_len).\n",
    "            training: Boolean flag for training mode.\n",
    "        Returns:\n",
    "            Output logits of shape (batch, tgt_seq_len, vocab_size).\n",
    "        \"\"\"\n",
    "        # Embedding and positional encoding for source\n",
    "        src_emb = self.embedding(src)  # (batch, src_seq_len, d_model)\n",
    "        src_emb *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        src_emb = self.pos_encoding(src_emb)\n",
    "        src_emb = self.dropout(src_emb, training=training)\n",
    "        \n",
    "        # Pass through encoder layers\n",
    "        enc_output = src_emb\n",
    "        for i in range(self.num_layers):\n",
    "            # Self-attention: keys, queries, and values are all the encoder output\n",
    "            attn_output = self.enc_layers[i](query=enc_output, value=enc_output, key=enc_output)\n",
    "            attn_output = self.dropout(attn_output, training=training)\n",
    "            # Residual connection and layer normalization\n",
    "            enc_output = tf.keras.layers.LayerNormalization(epsilon=1e-6)(enc_output + attn_output)\n",
    "            \n",
    "            # Feed-forward network with residual connection and layer normalization\n",
    "            ffn_output = self.ffn_layers_enc[i](enc_output)\n",
    "            ffn_output = self.dropout(ffn_output, training=training)\n",
    "            enc_output = tf.keras.layers.LayerNormalization(epsilon=1e-6)(enc_output + ffn_output)\n",
    "        \n",
    "        # Embedding and positional encoding for target\n",
    "        tgt_emb = self.embedding(tgt)  # (batch, tgt_seq_len, d_model)\n",
    "        tgt_emb *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        tgt_emb = self.pos_encoding(tgt_emb)\n",
    "        tgt_emb = self.dropout(tgt_emb, training=training)\n",
    "        \n",
    "        # Pass through decoder layers\n",
    "        dec_output = tgt_emb\n",
    "        for i in range(self.num_layers):\n",
    "            # Self-attention on target (for simplicity, look-ahead mask is omitted)\n",
    "            attn1 = self.dec_layers[i](query=dec_output, value=dec_output, key=dec_output)\n",
    "            attn1 = self.dropout(attn1, training=training)\n",
    "            dec_output = tf.keras.layers.LayerNormalization(epsilon=1e-6)(dec_output + attn1)\n",
    "            \n",
    "            # Cross-attention: target attends to encoder output\n",
    "            attn2 = self.dec_layers[i](query=dec_output, value=enc_output, key=enc_output)\n",
    "            attn2 = self.dropout(attn2, training=training)\n",
    "            dec_output = tf.keras.layers.LayerNormalization(epsilon=1e-6)(dec_output + attn2)\n",
    "            \n",
    "            # Feed-forward network with residual connection and layer normalization\n",
    "            ffn_output = self.ffn_layers_dec[i](dec_output)\n",
    "            ffn_output = self.dropout(ffn_output, training=training)\n",
    "            dec_output = tf.keras.layers.LayerNormalization(epsilon=1e-6)(dec_output + ffn_output)\n",
    "        \n",
    "        # Final projection to vocabulary size\n",
    "        final_output = self.final_layer(dec_output)  # (batch, tgt_seq_len, vocab_size)\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c05ed84",
   "metadata": {},
   "source": [
    "#### Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69db8817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer loss: 7.017174\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Hyperparameters\n",
    "    vocab_size = 1000        # Size of the vocabulary\n",
    "    d_model = 128            # Embedding dimension and transformer model dimension\n",
    "    num_heads = 4            # Number of attention heads\n",
    "    dff = 512                # Dimension of the feed-forward network\n",
    "    num_layers = 2           # Number of layers in both encoder and decoder\n",
    "    maximum_position_encoding = 100  # Maximum sequence length\n",
    "    dropout_rate = 0.1\n",
    "    batch_size = 32\n",
    "    src_seq_len = 20         # Source sequence length\n",
    "    tgt_seq_len = 20         # Target sequence length\n",
    "\n",
    "    # Create dummy source and target sequences (batch, seq_len)\n",
    "    src = tf.random.uniform((batch_size, src_seq_len), minval=0, maxval=vocab_size, dtype=tf.int32)\n",
    "    tgt = tf.random.uniform((batch_size, tgt_seq_len), minval=0, maxval=vocab_size, dtype=tf.int32)\n",
    "\n",
    "    # Instantiate the transformer model\n",
    "    transformer = Transformer(vocab_size, d_model, num_heads, dff, num_layers, maximum_position_encoding, dropout_rate)\n",
    "\n",
    "    # Define optimizer and loss function\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    # Forward pass: compute model predictions\n",
    "    predictions = transformer(src, tgt, training=True)\n",
    "    # predictions shape: (batch, tgt_seq_len, vocab_size)\n",
    "\n",
    "    # Compute loss between target and predictions\n",
    "    loss = loss_object(tgt, predictions)\n",
    "    \n",
    "    # Backpropagation and optimizer step\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = transformer(src, tgt, training=True)\n",
    "        loss = loss_object(tgt, predictions)\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    print(\"Transformer loss:\", loss.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9401d18",
   "metadata": {},
   "source": [
    "### PyTorch Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcc2f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175ce9d9",
   "metadata": {},
   "source": [
    "#### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9b06f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        \"\"\"\n",
    "        Implements the sinusoidal positional encoding function.\n",
    "        Args:\n",
    "            d_model: the embedding dimension.\n",
    "            dropout: dropout rate.\n",
    "            max_len: maximum length of sequences.\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Create a long enough PEs matrix\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        # Compute the positional encodings once in log space.\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # Even indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # Odd indices\n",
    "        pe = pe.unsqueeze(1)  # Shape: (max_len, 1, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (seq_len, batch_size, d_model)\n",
    "        Returns:\n",
    "            Tensor with positional encodings added.\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5824f169",
   "metadata": {},
   "source": [
    "#### Defining The Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3837084d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, model_dim, num_heads, num_encoder_layers, \n",
    "                 num_decoder_layers, ff_dim, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size: Size of the vocabulary (for both src and tgt).\n",
    "            model_dim: Dimensionality of the embeddings and transformer.\n",
    "            num_heads: Number of attention heads.\n",
    "            num_encoder_layers: Number of encoder layers.\n",
    "            num_decoder_layers: Number of decoder layers.\n",
    "            ff_dim: Dimensionality of the feed-forward network.\n",
    "            dropout: Dropout rate.\n",
    "        \"\"\"\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.model_dim = model_dim\n",
    "        \n",
    "        # Embedding layers for input tokens\n",
    "        self.embedding = nn.Embedding(vocab_size, model_dim)\n",
    "        # Positional encoding to inject sequence order information\n",
    "        self.pos_encoder = PositionalEncoding(model_dim, dropout)\n",
    "        # PyTorch's built-in Transformer module\n",
    "        self.transformer = nn.Transformer(d_model=model_dim, nhead=num_heads,\n",
    "                                          num_encoder_layers=num_encoder_layers,\n",
    "                                          num_decoder_layers=num_decoder_layers,\n",
    "                                          dim_feedforward=ff_dim,\n",
    "                                          dropout=dropout)\n",
    "        # Final linear layer to project the transformer output to vocab size\n",
    "        self.fc_out = nn.Linear(model_dim, vocab_size)\n",
    "    \n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        \"\"\"\n",
    "        Generate a square mask for the sequence. The masked positions are filled with float('-inf').\n",
    "        Args:\n",
    "            sz: Size of the mask (typically the target sequence length)\n",
    "        Returns:\n",
    "            A tensor mask of shape (sz, sz)\n",
    "        \"\"\"\n",
    "        mask = torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "        return mask\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Source sequence tensor of shape (src_seq_len, batch_size)\n",
    "            tgt: Target sequence tensor of shape (tgt_seq_len, batch_size)\n",
    "            src_mask: Optional mask for the source sequence.\n",
    "            tgt_mask: Optional mask for the target sequence.\n",
    "        Returns:\n",
    "            Output logits of shape (tgt_seq_len, batch_size, vocab_size)\n",
    "        \"\"\"\n",
    "        # Embed the input tokens and scale them\n",
    "        src = self.embedding(src) * math.sqrt(self.model_dim)\n",
    "        src = self.pos_encoder(src)\n",
    "        tgt = self.embedding(tgt) * math.sqrt(self.model_dim)\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "        \n",
    "        # Pass through the transformer\n",
    "        output = self.transformer(src, tgt, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "        output = self.fc_out(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235f1b91",
   "metadata": {},
   "source": [
    "#### Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e2c830",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Hyperparameters\n",
    "    vocab_size = 100         # Example vocabulary size\n",
    "    model_dim = 512          # Embedding dimension and transformer model dimension\n",
    "    num_heads = 8            # Number of attention heads\n",
    "    num_encoder_layers = 3   # Number of encoder layers\n",
    "    num_decoder_layers = 3   # Number of decoder layers\n",
    "    ff_dim = 2048            # Feed-forward network dimension\n",
    "    dropout = 0.1\n",
    "    batch_size = 32\n",
    "    src_seq_len = 10         # Source sequence length\n",
    "    tgt_seq_len = 10         # Target sequence length\n",
    "\n",
    "    # Create dummy source and target sequences (each element is a token index)\n",
    "    src = torch.randint(0, vocab_size, (src_seq_len, batch_size))\n",
    "    tgt = torch.randint(0, vocab_size, (tgt_seq_len, batch_size))\n",
    "\n",
    "    # Create model instance\n",
    "    model = TransformerModel(vocab_size, model_dim, num_heads, num_encoder_layers,\n",
    "                             num_decoder_layers, ff_dim, dropout)\n",
    "    # Generate target mask for autoregressive decoding (prevents attending to future tokens)\n",
    "    tgt_mask = model.generate_square_subsequent_mask(tgt_seq_len)\n",
    "\n",
    "    # Define optimizer and loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Forward pass: obtain model predictions\n",
    "    output = model(src, tgt, src_mask=None, tgt_mask=tgt_mask)\n",
    "    # Output shape: (tgt_seq_len, batch_size, vocab_size)\n",
    "\n",
    "    # For example purposes, assume the target labels are the same as tgt.\n",
    "    # Reshape output and target for computing loss.\n",
    "    output_flat = output.view(-1, vocab_size)\n",
    "    tgt_flat = tgt.view(-1)\n",
    "    loss = criterion(output_flat, tgt_flat)\n",
    "    \n",
    "    # Backpropagation and optimizer step\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(\"Transformer loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0d1a62",
   "metadata": {},
   "source": [
    "## Real-World Example: Using Pre-Trained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858884f5",
   "metadata": {},
   "source": [
    "### TensorFlow Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79ad11a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathanschlosser/anaconda3/envs/tf/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "All model checkpoint layers were used when initializing TFMarianMTModel.\n",
      "\n",
      "All the layers of TFMarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-en-de.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated text: Diese Klasse hat viel zu viele Informationen und meine Schüler sterben.\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianTokenizer, TFAutoModelForSeq2SeqLM\n",
    "\n",
    "# Specify the model for English-to-German translation\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-de\"\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Input text\n",
    "text = \"This class has way too much information and my students are dying.\"\n",
    "\n",
    "# Tokenize and generate translation\n",
    "inputs = tokenizer(text, return_tensors=\"tf\", padding=True)\n",
    "translated_tokens = model.generate(**inputs)\n",
    "translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Translated text:\", translated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41f652e",
   "metadata": {},
   "source": [
    "### PyTorch Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b55a1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# Specify the model for English-to-German translation\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-de\"\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# Input text\n",
    "text = \"Hello, how are you?\"\n",
    "\n",
    "# Tokenize and generate translation\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "translated_tokens = model.generate(**inputs)\n",
    "translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Translated text:\", translated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8802f2e3",
   "metadata": {},
   "source": [
    "## Real-World Example Without Pre-Trained Models\n",
    "\n",
    "Below are examples for both TensorFlow and PyTorch that not only build a model using framework‑specific solutions but also include a simple (toy) inference routine that accepts an English phrase and produces a German translation. \n",
    "\n",
    "Keep in mind:\n",
    "- These examples assume you have already trained your model and built tokenization routines.\n",
    "- The tokenizers shown here are minimal “dummy” implementations; in production you’d use robust tokenizers (e.g. SentencePiece or subword-based methods).\n",
    "- The models are for demonstration. **Without training, the outputs will be random.**\n",
    "\n",
    "These snippets serve as a template for how you might set up and call such a system in a real-world project but, for a meaningful translation output, you will need to train the model on a corpus and use tokenization methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a86247",
   "metadata": {},
   "source": [
    "### TensorFlow Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "183a1871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " target (InputLayer)            [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " tgt_embedding (Embedding)      (None, None, 64)     640000      ['target[0][0]']                 \n",
      "                                                                                                  \n",
      " multi_head_attention_9 (MultiH  (None, None, 64)    66368       ['tgt_embedding[0][0]',          \n",
      " eadAttention)                                                    'tgt_embedding[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, None, 64)     0           ['multi_head_attention_9[0][0]'] \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, None, 64)     0           ['tgt_embedding[0][0]',          \n",
      "                                                                  'dropout_15[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_30 (LayerN  (None, None, 64)    128         ['add_10[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, None, 128)    8320        ['layer_normalization_30[0][0]'] \n",
      "                                                                                                  \n",
      " dense_21 (Dense)               (None, None, 64)     8256        ['dense_20[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)           (None, None, 64)     0           ['dense_21[0][0]']               \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, None, 64)     0           ['layer_normalization_30[0][0]', \n",
      "                                                                  'dropout_16[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_31 (LayerN  (None, None, 64)    128         ['add_11[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " source (InputLayer)            [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " output (Dense)                 (None, None, 10000)  650000      ['layer_normalization_31[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,373,200\n",
      "Trainable params: 1,373,200\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Translated text (TF demo): <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, MultiHeadAttention, LayerNormalization, Dropout, Add\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "# --- Model definition (same as before) ---\n",
    "def transformer_block(x, num_heads, ff_dim, dropout_rate=0.1):\n",
    "    # Self-attention using built-in MultiHeadAttention\n",
    "    attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=x.shape[-1])(x, x)\n",
    "    attn_output = Dropout(dropout_rate)(attn_output)\n",
    "    out1 = Add()([x, attn_output])\n",
    "    out1 = LayerNormalization(epsilon=1e-6)(out1)\n",
    "    \n",
    "    # Feed-forward network\n",
    "    ffn_output = Dense(ff_dim, activation='relu')(out1)\n",
    "    ffn_output = Dense(x.shape[-1])(ffn_output)\n",
    "    ffn_output = Dropout(dropout_rate)(ffn_output)\n",
    "    out2 = Add()([out1, ffn_output])\n",
    "    out2 = LayerNormalization(epsilon=1e-6)(out2)\n",
    "    return out2\n",
    "\n",
    "# Define inputs (source and target sequences)\n",
    "input_seq = Input(shape=(None,), name=\"source\")\n",
    "target_seq = Input(shape=(None,), name=\"target\")\n",
    "\n",
    "vocab_size = 10000  # Example vocabulary size\n",
    "d_model = 64        # Embedding dimension\n",
    "\n",
    "# Create embeddings for source and target\n",
    "src_embed = Embedding(vocab_size, d_model, name=\"src_embedding\")(input_seq)\n",
    "tgt_embed = Embedding(vocab_size, d_model, name=\"tgt_embedding\")(target_seq)\n",
    "\n",
    "# For demonstration, apply a transformer block independently on encoder and decoder sides.\n",
    "encoder_output = transformer_block(src_embed, num_heads=4, ff_dim=128)\n",
    "decoder_output = transformer_block(tgt_embed, num_heads=4, ff_dim=128)\n",
    "\n",
    "# Final softmax layer over vocabulary for output\n",
    "final_output = Dense(vocab_size, activation='softmax', name=\"output\")(decoder_output)\n",
    "\n",
    "# Build the model\n",
    "tf_model = Model(inputs=[input_seq, target_seq], outputs=final_output)\n",
    "tf_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "tf_model.summary()\n",
    "\n",
    "# --- Dummy Tokenizers and Inference Routine ---\n",
    "# For simplicity, we define a minimal vocabulary and dummy tokenization.\n",
    "# In practice, you would use a proper tokenizer.\n",
    "\n",
    "# Dummy vocabularies\n",
    "src_vocab = {\"hello\": 2, \"how\": 3, \"are\": 4, \"you\": 5}\n",
    "tgt_vocab = {\"<sos>\": 0, \"<eos>\": 1, \"hallo\": 2, \"wie\": 3, \"geht\": 4, \"es\": 5, \"dir\": 6}\n",
    "rev_tgt_vocab = {v: k for k, v in tgt_vocab.items()}\n",
    "\n",
    "def english_tokenize(text):\n",
    "    # Splits by whitespace and converts to token IDs; unknown words default to 0.\n",
    "    return [src_vocab.get(word.lower(), 0) for word in text.split()]\n",
    "\n",
    "def german_detokenize(token_ids):\n",
    "    # Converts token IDs back into a string (ignores special tokens in this demo)\n",
    "    words = [rev_tgt_vocab.get(token, \"<unk>\") for token in token_ids]\n",
    "    return \" \".join(words)\n",
    "\n",
    "def translate_tf(input_text, model, max_length=10):\n",
    "    # Tokenize source sentence\n",
    "    src_tokens = english_tokenize(input_text)\n",
    "    src_tensor = tf.expand_dims(src_tokens, axis=0)  # Shape: (1, seq_len)\n",
    "    \n",
    "    # Start with the <sos> token for the target sequence\n",
    "    tgt_tokens = [tgt_vocab[\"<sos>\"]]\n",
    "    \n",
    "    for i in range(max_length):\n",
    "        tgt_tensor = tf.expand_dims(tgt_tokens, axis=0)  # Shape: (1, current_len)\n",
    "        # Predict next tokens; note: real models require masks and proper training.\n",
    "        predictions = model([src_tensor, tgt_tensor], training=False)\n",
    "        # Get the token with the highest probability from the last time step\n",
    "        next_token = int(tf.argmax(predictions[0, -1, :]).numpy())\n",
    "        tgt_tokens.append(next_token)\n",
    "        if next_token == tgt_vocab[\"<eos>\"]:\n",
    "            break\n",
    "\n",
    "    # Convert token IDs (skipping <sos>) to a string\n",
    "    return german_detokenize(tgt_tokens[1:])\n",
    "\n",
    "# Example usage:\n",
    "input_phrase = \"Hello how are you\"\n",
    "translated = translate_tf(input_phrase, tf_model)\n",
    "print(\"Translated text (TF demo):\", translated)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e72775",
   "metadata": {},
   "source": [
    "### PyTorch Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e228123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# --- Model definition (same as before) ---\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # Shape: (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, d_model)\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerMTModel(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, nhead=8,\n",
    "                 num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=2048,\n",
    "                 dropout=0.1, max_len=5000):\n",
    "        super(TransformerMTModel, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.src_tok_emb = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_tok_emb = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout, max_len)\n",
    "        \n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers,\n",
    "                                          num_decoder_layers, dim_feedforward, dropout)\n",
    "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
    "    \n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None,\n",
    "                src_key_padding_mask=None, tgt_key_padding_mask=None,\n",
    "                memory_key_padding_mask=None):\n",
    "        # src, tgt: (batch_size, seq_len)\n",
    "        src_emb = self.src_tok_emb(src) * math.sqrt(self.d_model)\n",
    "        tgt_emb = self.tgt_tok_emb(tgt) * math.sqrt(self.d_model)\n",
    "        \n",
    "        src_emb = self.positional_encoding(src_emb)\n",
    "        tgt_emb = self.positional_encoding(tgt_emb)\n",
    "        \n",
    "        # nn.Transformer expects shape (seq_len, batch_size, d_model)\n",
    "        src_emb = src_emb.transpose(0, 1)\n",
    "        tgt_emb = tgt_emb.transpose(0, 1)\n",
    "        \n",
    "        output = self.transformer(src_emb, tgt_emb,\n",
    "                                  src_mask=src_mask, tgt_mask=tgt_mask,\n",
    "                                  src_key_padding_mask=src_key_padding_mask,\n",
    "                                  tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                                  memory_key_padding_mask=memory_key_padding_mask)\n",
    "        output = output.transpose(0, 1)  # Back to (batch_size, seq_len, d_model)\n",
    "        return self.fc_out(output)\n",
    "\n",
    "# --- Dummy tokenizers and vocabularies ---\n",
    "src_vocab = {\"hello\": 2, \"how\": 3, \"are\": 4, \"you\": 5}\n",
    "tgt_vocab = {\"<sos>\": 0, \"<eos>\": 1, \"hallo\": 2, \"wie\": 3, \"geht\": 4, \"es\": 5, \"dir\": 6}\n",
    "rev_tgt_vocab = {v: k for k, v in tgt_vocab.items()}\n",
    "\n",
    "def english_tokenize(text):\n",
    "    return [src_vocab.get(word.lower(), 0) for word in text.split()]\n",
    "\n",
    "def german_detokenize(token_ids):\n",
    "    words = [rev_tgt_vocab.get(token, \"<unk>\") for token in token_ids]\n",
    "    return \" \".join(words)\n",
    "\n",
    "# --- Inference routine with greedy decoding ---\n",
    "def translate_torch(input_text, model, max_length=10, device='cpu'):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Tokenize source sentence\n",
    "        src_tokens = english_tokenize(input_text)\n",
    "        src_tensor = torch.tensor(src_tokens, dtype=torch.long, device=device).unsqueeze(0)  # (1, seq_len)\n",
    "        \n",
    "        # Initialize target with <sos>\n",
    "        tgt_tokens = [tgt_vocab[\"<sos>\"]]\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            tgt_tensor = torch.tensor(tgt_tokens, dtype=torch.long, device=device).unsqueeze(0)  # (1, current_len)\n",
    "            output = model(src_tensor, tgt_tensor)  # (1, seq_len, vocab_size)\n",
    "            # Get the token with the highest probability from the last time step\n",
    "            next_token = output[0, -1, :].argmax().item()\n",
    "            tgt_tokens.append(next_token)\n",
    "            if next_token == tgt_vocab[\"<eos>\"]:\n",
    "                break\n",
    "\n",
    "    # Return decoded German sentence (skip <sos>)\n",
    "    return german_detokenize(tgt_tokens[1:])\n",
    "\n",
    "# --- Example usage ---\n",
    "src_vocab_size = 10000\n",
    "tgt_vocab_size = 10000\n",
    "device = torch.device(\"cpu\")\n",
    "pt_model = TransformerMTModel(src_vocab_size, tgt_vocab_size, d_model=128, nhead=4,\n",
    "                              num_encoder_layers=2, num_decoder_layers=2, dim_feedforward=256)\n",
    "pt_model.to(device)\n",
    "\n",
    "input_phrase = \"Hello how are you\"\n",
    "translated_pt = translate_torch(input_phrase, pt_model, device=device)\n",
    "print(\"Translated text (PyTorch demo):\", translated_pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc15e2c",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "1. **Encoder–Decoder RNNs:**\n",
    "   - **What:** A two-part model where one RNN encodes the input into a context vector and another decodes it into an output.\n",
    "   - **Why:** Simplifies mapping between sequences.\n",
    "   - **Best For:** Moderate-length sequences and early sequence-to-sequence tasks (e.g., machine translation).\n",
    "\n",
    "2. **RNNs with Attention:**\n",
    "   - **What:** Enhances the encoder–decoder model by allowing the decoder to attend to different parts of the input dynamically.\n",
    "   - **Why:** Mitigates the bottleneck of fixed-length context vectors and improves handling of long-range dependencies.\n",
    "   - **Best For:** Complex translation, image captioning, and tasks requiring dynamic context focus.\n",
    "\n",
    "3. **Transformers:**\n",
    "   - **What:** A non-recurrent architecture relying entirely on self-attention, multi-head attention, and feed-forward networks.\n",
    "   - **Why:** Enables parallel processing, scalability, and effective modeling of long sequences.\n",
    "   - **Best For:** Large-scale NLP tasks, computer vision, and any task requiring modeling of complex dependencies across sequences.\n",
    "\n",
    "This progression reflects the evolution of sequence modeling techniques as researchers sought more efficient, scalable, and context-aware models to tackle increasingly complex tasks in natural language processing and beyond.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95e0b3f",
   "metadata": {},
   "source": [
    "### Additional Resources\n",
    "\n",
    "- The Illustrated Transformer: https://jalammar.github.io/illustrated-transformer/\n",
    "- Tensor2Tensor Visualizer: https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
