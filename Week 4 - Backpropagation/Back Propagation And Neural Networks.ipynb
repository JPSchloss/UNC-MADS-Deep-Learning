{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f01bccc1",
   "metadata": {},
   "source": [
    "# Backpropagation and Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4e42aa",
   "metadata": {},
   "source": [
    "Backpropagation, short for \"backward propagation of errors,\" is the key algorithm that powers the training of artificial neural networks. It is a way of enabling deep networks to learn complex patterns in data through more efficient weight updates. \n",
    "\n",
    "The core idea behind backpropagation is to compute how much each network parameter (such as a weight or bias) contributes to the prediction error and to then adjust these parameters to minimize the overall error. This process relies on the chain rule of calculus to propagate the error backwards through the network, starting from the output layer and moving towards the input layer.\n",
    "\n",
    "Backpropagation works by iteratively updating weights using gradient descent. The algorithm involves two key phases: **the forward pass**, where the input data is passed through the network to generate predictions, and **the backward pass**, where the error is calculated and gradients are computed to update the weights. By iteratively reducing the error with each epoch, the network \"learns\" the optimal parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f3b730",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/v2/resize:fit:1080/0*d9yJ5xIqdbDyjCYR.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aeaa50d",
   "metadata": {},
   "source": [
    "## Back Propogation and The Chain Rule\n",
    "\n",
    "A key mathematical concept underlying backpropagation is the **chain rule**, which is used to compute derivatives of composite functions. Since a neural network is essentially a nested composition of functions — where each layer applies an activation function to a linear combination of inputs — the chain rule enables us to compute how changes in a given parameter (like a weight or bias) affect the final network output.\n",
    "\n",
    "The chain rule states that if we have a composite function $f(x) = g(h(x))$, then the derivative of $f$ with respect to $x$ is given by:  \n",
    "$$\n",
    "\\frac{df}{dx} = \\frac{dg}{dh} \\cdot \\frac{dh}{dx}\n",
    "$$  \n",
    "\n",
    "In the context of backpropagation, this means that we can compute the gradient of the overall loss $L$ with respect to the weights $w$ by sequentially propagating gradients backward through the network. For example, if $L$ depends on the output of layer $k$ and layer $k$ depends on layer $k-1$, the chain rule tells us that:  \n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial a_k} \\cdot \\frac{\\partial a_k}{\\partial z_k} \\cdot \\frac{\\partial z_k}{\\partial w}\n",
    "$$  \n",
    "\n",
    "where:\n",
    "- $a_k$ is the activation of the layer,\n",
    "- $z_k$ is the linear combination of inputs to that layer, and\n",
    "- $w$ represents the weights connecting to the layer.\n",
    "\n",
    "By applying the chain rule iteratively from the output layer back to the input layer, backpropagation efficiently computes the gradients needed to update all the parameters in the network. This is the key to optimizing deep networks through gradient descent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656ddd21",
   "metadata": {},
   "source": [
    "## Simple Backpropogation Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f10e0f",
   "metadata": {},
   "source": [
    "### Network Setup\n",
    "\n",
    "Let’s consider a simple neural network with:\n",
    "- 1 input layer (2 features),\n",
    "- 1 hidden layer (2 neurons), and\n",
    "- 1 output neuron.\n",
    "\n",
    "We will manually walk through the **forward pass** and **backpropagation** using the following data:\n",
    "\n",
    "**Inputs:**  \n",
    "$$\n",
    "X = \\begin{bmatrix} 0.5 \\\\ 0.2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**True output:** $ y = 1 $\n",
    "\n",
    "**Weights and biases:**  \n",
    "$$\n",
    "W_1 = \\begin{bmatrix} 0.4 & 0.1 \\\\ 0.3 & 0.7 \\end{bmatrix}, \\quad b_1 = \\begin{bmatrix} 0.1 \\\\ 0.2 \\end{bmatrix}\n",
    "$$  \n",
    "\n",
    "$$\n",
    "W_2 = \\begin{bmatrix} 0.5 & 0.6 \\end{bmatrix}, \\quad b_2 = 0.3\n",
    "$$\n",
    "\n",
    "### Walkthrough:\n",
    "\n",
    "#### Forward Pass:\n",
    "Compute the activations $ a_1 $ and output $ a_2 $.\n",
    "\n",
    "#### Loss Calculation:\n",
    "Compute the error using a loss function, like **Mean Squared Error**:  \n",
    "$$\n",
    "L = \\frac{1}{2} (a_2 - y)^2\n",
    "$$\n",
    "\n",
    "#### Backward Pass:  \n",
    "Compute gradients using the chain rule as described in the backpropagation section.\n",
    "\n",
    "#### Weight Updates:\n",
    "Update the weights using gradient descent, where $ \\eta $ is the learning rate:  \n",
    "\n",
    "$$\n",
    "W^{(l)} = W^{(l)} - \\eta \\frac{\\partial L}{\\partial W^{(l)}}\n",
    "$$  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1689bdde",
   "metadata": {},
   "source": [
    "## The Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0331f5c",
   "metadata": {},
   "source": [
    "In a neural network, the **forward pass** is the process of passing the input data through the network to produce an output. The input is propagated layer by layer through **linear transformations** and **non-linear activation functions** until a final prediction is obtained.\n",
    "\n",
    "At each layer $ l $, the input is transformed using the following steps:\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Linear Transformation:\n",
    "The inputs are multiplied by weights and added to biases:  \n",
    "$$\n",
    "z^{(l)} = W^{(l)} \\cdot a^{(l-1)} + b^{(l)}\n",
    "$$  \n",
    "\n",
    "where:  \n",
    "- $ z^{(l)} $ is the pre-activation value of layer $ l $,  \n",
    "- $ W^{(l)} $ is the weight matrix for layer $ l $,  \n",
    "- $ a^{(l-1)} $ is the activation from the previous layer, and  \n",
    "- $ b^{(l)} $ is the bias vector for layer $ l $.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Activation Function:**  \n",
    "A non-linear activation function is applied to the pre-activation $ z^{(l)} $ to get the activation of the current layer:  \n",
    "$$\n",
    "a^{(l)} = \\sigma(z^{(l)})\n",
    "$$  \n",
    "\n",
    "Common activation functions include sigmoid, ReLU, and tanh.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9d8601a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of the neural network: [[0.72346724]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the sigmoid activation function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Input data (2 features, 1 sample)\n",
    "X = np.array([[0.5], [0.2]])\n",
    "\n",
    "# Initialize weights and biases for a 2-layer neural network\n",
    "W1 = np.array([[0.4, 0.1], [0.3, 0.7]])  # Weights for the hidden layer\n",
    "b1 = np.array([[0.1], [0.2]])            # Biases for the hidden layer\n",
    "\n",
    "W2 = np.array([[0.5, 0.6]])              # Weights for the output layer\n",
    "b2 = np.array([[0.3]])                   # Bias for the output layer\n",
    "\n",
    "# Forward pass\n",
    "z1 = np.dot(W1, X) + b1  # Linear transformation for the hidden layer\n",
    "a1 = sigmoid(z1)         # Activation for the hidden layer\n",
    "\n",
    "z2 = np.dot(W2, a1) + b2  # Linear transformation for the output layer\n",
    "a2 = sigmoid(z2)          # Activation for the output layer (final prediction)\n",
    "\n",
    "print(\"Output of the neural network:\", a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26329cd3",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c51695d",
   "metadata": {},
   "source": [
    "The goal of backpropagation is to calculate the **gradient of the loss function** with respect to the network’s weights and biases, so that these parameters can be updated using gradient descent. The key to backpropagation is the **chain rule**, which allows us to propagate the error backward through the network.\n",
    "\n",
    "\n",
    "1. **Compute the Loss:**  \n",
    "    Assume a loss function $L$ (e.g., Mean Squared Error or Cross-Entropy Loss).\n",
    "\n",
    "2. **Compute Output Error:**  \n",
    "    Compute the derivative of the loss with respect to the output activation $a^{(L)}$:\n",
    "\n",
    "    $$\n",
    "    \\delta^{(L)} = \\frac{\\partial L}{\\partial a^{(L)}} \\cdot \\sigma'(z^{(L)})\n",
    "    $$\n",
    "\n",
    "    Here, $\\delta^{(L)}$ is the error signal at the output layer, and $\\sigma'(z)$ is the derivative of the activation function.\n",
    "\n",
    "3. **Backpropagate Error:**  \n",
    "    For each previous layer $l$, propagate the error backward:\n",
    "\n",
    "    $$\n",
    "    \\delta^{(l)} = \\big( W^{(l+1)} \\big)^T \\cdot \\delta^{(l+1)} \\cdot \\sigma'(z^{(l)})\n",
    "    $$\n",
    "\n",
    "4. **Calculate Gradients:**  \n",
    "    The gradients of the loss with respect to the weights and biases are:\n",
    "\n",
    "    $$\n",
    "    \\frac{\\partial L}{\\partial W^{(l)}} = \\delta^{(l)} \\cdot \\big( a^{(l-1)} \\big)^T \\quad \\text{and} \\quad \\frac{\\partial L}{\\partial b^{(l)}} = \\delta^{(l)}\n",
    "    $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e086de61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients for W1: [[-0.00337071 -0.00134828]\n",
      " [-0.00390986 -0.00156394]]\n",
      "Gradients for W2: [[-0.03205042 -0.03430665]]\n"
     ]
    }
   ],
   "source": [
    "# Define the derivative of the sigmoid function\n",
    "def sigmoid_derivative(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "# Assume the network's output and the true label\n",
    "y_true = np.array([[1]])  # True label\n",
    "loss_derivative = a2 - y_true  # Derivative of the Mean Squared Error loss\n",
    "\n",
    "# Backpropagation\n",
    "delta2 = loss_derivative * sigmoid_derivative(z2)  # Output layer error\n",
    "dW2 = np.dot(delta2, a1.T)  # Gradient for W2\n",
    "db2 = delta2  # Gradient for b2\n",
    "\n",
    "delta1 = np.dot(W2.T, delta2) * sigmoid_derivative(z1)  # Hidden layer error\n",
    "dW1 = np.dot(delta1, X.T)  # Gradient for W1\n",
    "db1 = delta1  # Gradient for b1\n",
    "\n",
    "print(\"Gradients for W1:\", dW1)\n",
    "print(\"Gradients for W2:\", dW2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19caefb6",
   "metadata": {},
   "source": [
    "## Modern Code Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e634708c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ef796a",
   "metadata": {},
   "source": [
    "#### Prepare The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1110f259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data  # Features (4 features per sample)\n",
    "y = (iris.target != 0).astype(int)  # Binary classification: class 0 vs. others\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6d1637",
   "metadata": {},
   "source": [
    "#### Build And Initialize The Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73211126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a simple neural network using TensorFlow\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, activation='relu', input_shape=(4,)),  # Hidden layer with 10 neurons\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "])\n",
    "\n",
    "# Initialize optimizer and loss function\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c049bb9b",
   "metadata": {},
   "source": [
    "#### Training The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "855174d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.6132457852363586\n",
      "Epoch 10, Loss: 0.5687472224235535\n",
      "Epoch 20, Loss: 0.5297806859016418\n",
      "Epoch 30, Loss: 0.49534478783607483\n",
      "Epoch 40, Loss: 0.4642595648765564\n",
      "Epoch 50, Loss: 0.4361940622329712\n",
      "Epoch 60, Loss: 0.4106963872909546\n",
      "Epoch 70, Loss: 0.3873886466026306\n",
      "Epoch 80, Loss: 0.3659735321998596\n",
      "Epoch 90, Loss: 0.34609031677246094\n"
     ]
    }
   ],
   "source": [
    "# Training loop with manual backpropagation using tf.GradientTape\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass: compute predictions\n",
    "        predictions = model(X_train, training=True)\n",
    "        # Compute the loss\n",
    "        loss = loss_fn(y_train, predictions)\n",
    "    \n",
    "    # Compute gradients (backpropagation)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    \n",
    "    # Apply gradients using the optimizer\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    # Print the loss every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e772a7",
   "metadata": {},
   "source": [
    "#### Evaluate The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07c75af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final test loss: 0.3009743094444275\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_loss = loss_fn(y_test, model(X_test)).numpy()\n",
    "print(f\"\\nFinal test loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c606ef72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Final test accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Predict on test data and compute accuracy\n",
    "predictions = model.predict(X_test)\n",
    "predicted_classes = (predictions > 0.5).astype(int)  # Convert probabilities to binary labels (0 or 1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = (predicted_classes.flatten() == y_test).mean()  # Flatten predictions and compare with true labels\n",
    "\n",
    "print(f\"Final test accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43322468",
   "metadata": {},
   "source": [
    "#### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a282a9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "Sample 0: Predicted = 1, True = 1\n",
      "Sample 1: Predicted = 0, True = 0\n",
      "Sample 2: Predicted = 1, True = 1\n",
      "Sample 3: Predicted = 1, True = 1\n",
      "Sample 4: Predicted = 1, True = 1\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on test data\n",
    "predictions = model.predict(X_test)\n",
    "predicted_classes = (predictions > 0.5).astype(int)  # Convert probabilities to binary labels (0 or 1)\n",
    "\n",
    "# Display predictions and ground truth\n",
    "for i in range(5):  # Display the first 5 predictions\n",
    "    print(f\"Sample {i}: Predicted = {predicted_classes[i][0]}, True = {y_test[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73547e0e",
   "metadata": {},
   "source": [
    "## What is missing here? \n",
    "\n",
    "There is no explicit call to do backward propogation here. Instead, TensorFlow automatically handles both the forward pass and backpropagation during training when model.fit() is called.\n",
    "\n",
    "During the forward pass, TensorFlow propagates the input through the network, computing the output by applying the weights, biases, and activation functions at each layer. \n",
    "\n",
    "It then calculates the loss by comparing the predicted output to the actual target using the specified loss function (e.g., mean squared error). \n",
    "\n",
    "In the backward pass (backpropagation), TensorFlow computes the gradients of the loss with respect to the model parameters (weights and biases) using automatic differentiation. The optimizer (e.g., Adam) updates the parameters using these gradients to minimize the loss.\n",
    "\n",
    "**This entire process is automated and optimized within TensorFlow using the computational graph and tf.GradientTape internally.** As a result, you don’t need to manually compute gradients or update weights as TensorFlow does these steps under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9dd8369b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 4.622866630554199\n",
      "Epoch 10, Loss: 4.375480651855469\n",
      "Epoch 20, Loss: 4.009067535400391\n",
      "Epoch 30, Loss: 3.8094234466552734\n",
      "Epoch 40, Loss: 3.6238064765930176\n",
      "Epoch 50, Loss: 3.4619410037994385\n",
      "Epoch 60, Loss: 3.313830614089966\n",
      "Epoch 70, Loss: 3.1676554679870605\n",
      "Epoch 80, Loss: 3.0220491886138916\n",
      "Epoch 90, Loss: 2.891594886779785\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# Manual training loop with backpropagation\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(X_train, training=True)  # Forward pass\n",
    "        loss = loss_fn(y_train, predictions)  # Compute loss\n",
    "    \n",
    "    # Backpropagation: compute gradients\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    \n",
    "    # Apply gradients to update weights\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea061a2",
   "metadata": {},
   "source": [
    "## Regression Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ac80aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc32f1da",
   "metadata": {},
   "source": [
    "#### Prepare The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a6ee54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathanschlosser/anaconda3/envs/OldJupyter/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the Boston Housing dataset\n",
    "boston = load_boston()\n",
    "X = boston.data  # Features (number of rooms, location, etc.)\n",
    "y = boston.target.reshape(-1, 1)  # House prices reshaped to 2D\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3f6f52",
   "metadata": {},
   "source": [
    "#### Build And Compile Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f67e3a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a simple neural network using TensorFlow\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(X_train.shape[1],)),  # Input layer (13 features)\n",
    "    tf.keras.layers.Dense(64, activation='relu'),  # Hidden layer with 64 neurons\n",
    "    tf.keras.layers.Dense(1)  # Output layer for regression (1 output)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b705f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with the optimizer and loss function\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a05220be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 608.2303 - mae: 22.5639 - val_loss: 374.4987 - val_mae: 17.8597\n",
      "Epoch 2/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 344.9490 - mae: 16.0672 - val_loss: 157.1886 - val_mae: 10.5884\n",
      "Epoch 3/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 142.6448 - mae: 9.3944 - val_loss: 73.2526 - val_mae: 6.3767\n",
      "Epoch 4/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 80.7419 - mae: 6.9983 - val_loss: 44.3582 - val_mae: 4.4533\n",
      "Epoch 5/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 36.9345 - mae: 4.6164 - val_loss: 46.5100 - val_mae: 4.7034\n",
      "Epoch 6/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 29.9583 - mae: 3.8852 - val_loss: 47.3100 - val_mae: 4.8097\n",
      "Epoch 7/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 22.1931 - mae: 3.5176 - val_loss: 42.0400 - val_mae: 4.4190\n",
      "Epoch 8/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 22.7674 - mae: 3.5035 - val_loss: 37.7920 - val_mae: 4.1375\n",
      "Epoch 9/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 18.3252 - mae: 3.2416 - val_loss: 34.7501 - val_mae: 3.9693\n",
      "Epoch 10/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 18.3005 - mae: 3.1949 - val_loss: 33.8330 - val_mae: 4.0218\n",
      "Epoch 11/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 16.1533 - mae: 2.9213 - val_loss: 32.9024 - val_mae: 4.0083\n",
      "Epoch 12/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 17.2545 - mae: 3.1321 - val_loss: 29.2654 - val_mae: 3.6819\n",
      "Epoch 13/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 12.9751 - mae: 2.7082 - val_loss: 29.5938 - val_mae: 3.7191\n",
      "Epoch 14/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 12.6959 - mae: 2.6186 - val_loss: 27.5984 - val_mae: 3.5864\n",
      "Epoch 15/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 13.6175 - mae: 2.6424 - val_loss: 27.3485 - val_mae: 3.5877\n",
      "Epoch 16/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 10.6521 - mae: 2.4438 - val_loss: 23.9618 - val_mae: 3.3259\n",
      "Epoch 17/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 9.4773 - mae: 2.2388 - val_loss: 24.1521 - val_mae: 3.3391\n",
      "Epoch 18/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 10.1623 - mae: 2.3460 - val_loss: 22.4621 - val_mae: 3.2810\n",
      "Epoch 19/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 11.5267 - mae: 2.4572 - val_loss: 22.6351 - val_mae: 3.3099\n",
      "Epoch 20/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 9.7417 - mae: 2.1898 - val_loss: 21.9434 - val_mae: 3.3103\n",
      "Epoch 21/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.6893 - mae: 2.1657 - val_loss: 21.7313 - val_mae: 3.2608\n",
      "Epoch 22/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 10.4109 - mae: 2.2879 - val_loss: 20.6171 - val_mae: 3.2012\n",
      "Epoch 23/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 10.0273 - mae: 2.1772 - val_loss: 22.6242 - val_mae: 3.3832\n",
      "Epoch 24/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.0492 - mae: 2.1687 - val_loss: 18.9052 - val_mae: 3.0414\n",
      "Epoch 25/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 9.5463 - mae: 2.2844 - val_loss: 21.9771 - val_mae: 3.2831\n",
      "Epoch 26/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 7.8413 - mae: 2.0079 - val_loss: 19.7080 - val_mae: 3.1639\n",
      "Epoch 27/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.4146 - mae: 2.1348 - val_loss: 21.4918 - val_mae: 3.2392\n",
      "Epoch 28/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.9069 - mae: 2.0588 - val_loss: 19.3732 - val_mae: 3.0961\n",
      "Epoch 29/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.9553 - mae: 1.9592 - val_loss: 18.5162 - val_mae: 3.0002\n",
      "Epoch 30/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.4736 - mae: 1.9632 - val_loss: 21.2011 - val_mae: 3.2222\n",
      "Epoch 31/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 8.1998 - mae: 2.0293 - val_loss: 19.7783 - val_mae: 3.1770\n",
      "Epoch 32/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 8.2028 - mae: 2.1087 - val_loss: 19.1788 - val_mae: 3.0875\n",
      "Epoch 33/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 8.3098 - mae: 2.1289 - val_loss: 22.2746 - val_mae: 3.3539\n",
      "Epoch 34/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.1579 - mae: 2.0565 - val_loss: 17.1532 - val_mae: 2.8597\n",
      "Epoch 35/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.6657 - mae: 2.0028 - val_loss: 23.2274 - val_mae: 3.4184\n",
      "Epoch 36/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.9162 - mae: 2.0633 - val_loss: 18.2774 - val_mae: 3.0293\n",
      "Epoch 37/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.8507 - mae: 2.0037 - val_loss: 19.1900 - val_mae: 3.0765\n",
      "Epoch 38/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8.2020 - mae: 2.0422 - val_loss: 20.3268 - val_mae: 3.1795\n",
      "Epoch 39/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6.1379 - mae: 1.8346 - val_loss: 18.5632 - val_mae: 3.0660\n",
      "Epoch 40/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.5126 - mae: 2.0562 - val_loss: 17.4005 - val_mae: 2.8972\n",
      "Epoch 41/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6.4619 - mae: 1.8791 - val_loss: 19.2468 - val_mae: 3.1339\n",
      "Epoch 42/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6.9467 - mae: 1.9260 - val_loss: 19.2400 - val_mae: 3.1453\n",
      "Epoch 43/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6.2448 - mae: 1.8458 - val_loss: 19.1659 - val_mae: 3.0779\n",
      "Epoch 44/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.0599 - mae: 2.0288 - val_loss: 16.2059 - val_mae: 2.8796\n",
      "Epoch 45/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.2199 - mae: 1.9258 - val_loss: 19.1137 - val_mae: 3.1010\n",
      "Epoch 46/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.7525 - mae: 1.9035 - val_loss: 17.0762 - val_mae: 2.9714\n",
      "Epoch 47/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.2621 - mae: 1.9839 - val_loss: 17.7696 - val_mae: 3.0570\n",
      "Epoch 48/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.5890 - mae: 1.8518 - val_loss: 17.1288 - val_mae: 2.9180\n",
      "Epoch 49/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.8849 - mae: 1.8085 - val_loss: 16.8974 - val_mae: 3.0391\n",
      "Epoch 50/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.0260 - mae: 1.8008 - val_loss: 18.5037 - val_mae: 2.9954\n",
      "Epoch 51/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.9362 - mae: 1.8117 - val_loss: 17.3592 - val_mae: 3.1563\n",
      "Epoch 52/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.5400 - mae: 1.7500 - val_loss: 16.0779 - val_mae: 2.8377\n",
      "Epoch 53/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.2214 - mae: 1.8294 - val_loss: 17.2095 - val_mae: 3.0264\n",
      "Epoch 54/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.2467 - mae: 1.8140 - val_loss: 18.1590 - val_mae: 3.1489\n",
      "Epoch 55/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.6304 - mae: 1.7682 - val_loss: 15.5436 - val_mae: 2.8317\n",
      "Epoch 56/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.3841 - mae: 1.7357 - val_loss: 18.5756 - val_mae: 3.0825\n",
      "Epoch 57/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.1202 - mae: 1.6764 - val_loss: 16.3310 - val_mae: 2.8830\n",
      "Epoch 58/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.6279 - mae: 1.5648 - val_loss: 17.0273 - val_mae: 3.0048\n",
      "Epoch 59/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.9341 - mae: 1.6879 - val_loss: 17.3386 - val_mae: 2.9964\n",
      "Epoch 60/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.4220 - mae: 1.7362 - val_loss: 16.6224 - val_mae: 2.9900\n",
      "Epoch 61/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.3859 - mae: 1.7043 - val_loss: 16.7465 - val_mae: 3.0247\n",
      "Epoch 62/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.6698 - mae: 1.6265 - val_loss: 17.5175 - val_mae: 3.0507\n",
      "Epoch 63/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.7392 - mae: 1.6427 - val_loss: 16.1795 - val_mae: 2.8784\n",
      "Epoch 64/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.2321 - mae: 1.8692 - val_loss: 15.2184 - val_mae: 2.9497\n",
      "Epoch 65/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.8068 - mae: 1.6830 - val_loss: 15.0415 - val_mae: 2.8395\n",
      "Epoch 66/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.6785 - mae: 1.8027 - val_loss: 15.2315 - val_mae: 2.8644\n",
      "Epoch 67/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.0877 - mae: 1.6686 - val_loss: 16.7385 - val_mae: 3.0404\n",
      "Epoch 68/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.0845 - mae: 1.6442 - val_loss: 16.1858 - val_mae: 2.9489\n",
      "Epoch 69/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.9329 - mae: 1.6108 - val_loss: 15.5585 - val_mae: 2.9212\n",
      "Epoch 70/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.7563 - mae: 1.6644 - val_loss: 15.1128 - val_mae: 2.8659\n",
      "Epoch 71/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.6615 - mae: 1.5924 - val_loss: 16.2304 - val_mae: 2.9844\n",
      "Epoch 72/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.7061 - mae: 1.6166 - val_loss: 15.8526 - val_mae: 2.9391\n",
      "Epoch 73/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.6533 - mae: 1.5532 - val_loss: 15.9149 - val_mae: 2.9970\n",
      "Epoch 74/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.6051 - mae: 1.5800 - val_loss: 14.5261 - val_mae: 2.8257\n",
      "Epoch 75/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.5521 - mae: 1.6064 - val_loss: 17.2186 - val_mae: 3.0090\n",
      "Epoch 76/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.7624 - mae: 1.6022 - val_loss: 15.4171 - val_mae: 2.8859\n",
      "Epoch 77/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.0623 - mae: 1.4893 - val_loss: 16.7663 - val_mae: 3.0481\n",
      "Epoch 78/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.2109 - mae: 1.5165 - val_loss: 15.1416 - val_mae: 2.7953\n",
      "Epoch 79/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.0150 - mae: 1.5016 - val_loss: 15.1305 - val_mae: 2.8656\n",
      "Epoch 80/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.7528 - mae: 1.6487 - val_loss: 13.9481 - val_mae: 2.8282\n",
      "Epoch 81/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.9366 - mae: 1.6659 - val_loss: 15.8455 - val_mae: 2.9025\n",
      "Epoch 82/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.9705 - mae: 1.6845 - val_loss: 18.3716 - val_mae: 3.1402\n",
      "Epoch 83/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.5123 - mae: 1.8539 - val_loss: 18.2510 - val_mae: 3.1754\n",
      "Epoch 84/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.6244 - mae: 1.6141 - val_loss: 14.5541 - val_mae: 2.7999\n",
      "Epoch 85/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.9486 - mae: 1.5276 - val_loss: 16.3340 - val_mae: 2.9255\n",
      "Epoch 86/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.4171 - mae: 1.7192 - val_loss: 15.0680 - val_mae: 2.8986\n",
      "Epoch 87/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.3521 - mae: 1.5191 - val_loss: 15.3142 - val_mae: 2.9318\n",
      "Epoch 88/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.0896 - mae: 1.5116 - val_loss: 15.1267 - val_mae: 2.8409\n",
      "Epoch 89/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.9053 - mae: 1.4287 - val_loss: 14.1815 - val_mae: 2.7749\n",
      "Epoch 90/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.0258 - mae: 1.5265 - val_loss: 15.0606 - val_mae: 2.8827\n",
      "Epoch 91/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.9604 - mae: 1.4974 - val_loss: 15.5358 - val_mae: 2.9032\n",
      "Epoch 92/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.7687 - mae: 1.4726 - val_loss: 15.4206 - val_mae: 2.9243\n",
      "Epoch 93/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.8821 - mae: 1.4796 - val_loss: 13.7150 - val_mae: 2.7699\n",
      "Epoch 94/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.8810 - mae: 1.4875 - val_loss: 15.6703 - val_mae: 2.9020\n",
      "Epoch 95/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.2471 - mae: 1.5727 - val_loss: 13.9101 - val_mae: 2.7945\n",
      "Epoch 96/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.6239 - mae: 1.4146 - val_loss: 13.9193 - val_mae: 2.7958\n",
      "Epoch 97/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.5094 - mae: 1.5442 - val_loss: 13.4503 - val_mae: 2.7578\n",
      "Epoch 98/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.3126 - mae: 1.5511 - val_loss: 12.9878 - val_mae: 2.6922\n",
      "Epoch 99/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.9739 - mae: 1.4827 - val_loss: 14.0269 - val_mae: 2.8102\n",
      "Epoch 100/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.5651 - mae: 1.3960 - val_loss: 13.8153 - val_mae: 2.7890\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60f8bd3",
   "metadata": {},
   "source": [
    "#### Evaluate The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12040685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final test loss (MSE): 12.5700\n",
      "Final test MAE: 2.3278\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on test data\n",
    "test_loss, test_mae = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\nFinal test loss (MSE): {test_loss:.4f}\")\n",
    "print(f\"Final test MAE: {test_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c69507",
   "metadata": {},
   "source": [
    "#### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bff4b45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Sample 1: Predicted price = 26.92, Actual price = 23.60\n",
      "Sample 2: Predicted price = 35.84, Actual price = 32.40\n",
      "Sample 3: Predicted price = 14.96, Actual price = 13.60\n",
      "Sample 4: Predicted price = 22.92, Actual price = 22.80\n",
      "Sample 5: Predicted price = 15.86, Actual price = 16.10\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on new data\n",
    "predictions = model.predict(X_test[:5])  # Predict on the first 5 test samples\n",
    "\n",
    "# Display predictions and ground truth\n",
    "for i in range(5):\n",
    "    print(f\"Sample {i + 1}: Predicted price = {predictions[i][0]:.2f}, Actual price = {y_test[i][0]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ef78a6",
   "metadata": {},
   "source": [
    "## Vanishing And Exploding Gradients\n",
    "\n",
    "As neural networks grow deeper (with more layers), training them can become unstable due to challenges such as vanishing gradients and exploding gradients. These challenges directly affect the performance of the network, causing either slow learning or unstable weight updates.\n",
    "\n",
    "#### Vanishing Gradients\n",
    "\n",
    "  During backpropagation, gradients are computed using the chain rule and propagated backward from the output layer to earlier layers. As the gradient flows backward, it is multiplied by small derivatives, especially if the network uses activation functions like **sigmoid** or **tanh**. This multiplication causes the gradients to become smaller and smaller as they move toward the input layer, effectively “vanishing.”\n",
    "\n",
    "- **Why is this a problem?**  \n",
    "  - Weights in the earlier layers are updated very slowly or not at all, making it difficult for the network to learn meaningful features.  \n",
    "  - This is particularly problematic for deep networks and recurrent neural networks (RNNs), where gradients may vanish before reaching the initial layers.\n",
    "\n",
    "- **Explaining the math:**  \n",
    "  Suppose you have a network with several layers, and the activation function is **sigmoid**. The derivative of the sigmoid function is given by:  \n",
    "  \n",
    "  $$\n",
    "  \\sigma'(z) = \\sigma(z) \\cdot (1 - \\sigma(z))\n",
    "  $$\n",
    "  \n",
    "  The maximum derivative of the sigmoid function is **0.25**. As the chain rule multiplies these small derivatives across layers, the gradients exponentially decrease, leading to very small updates.\n",
    "\n",
    "\n",
    "#### Exploding Gradients\n",
    "\n",
    "  The opposite of vanishing gradients occurs when the gradients grow exponentially large during backpropagation. This typically happens when the weights are initialized with large values or when the network is poorly tuned.\n",
    "\n",
    "- **Why is this a problem?**  \n",
    "  - Large gradients result in **unstable weight updates**, causing the loss to diverge rather than converge.  \n",
    "  - The model fails to learn, and the training process may halt due to numerical instability.\n",
    "\n",
    "- **Instability due to large gradients:**  \n",
    "  Consider a network using **ReLU** activation where the gradient does not saturate like in sigmoid or tanh. If the gradients of the loss with respect to the weights accumulate too quickly (due to large activations or poorly initialized weights), the updates during gradient descent can become excessively large, resulting in erratic or diverging loss values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46da70d",
   "metadata": {},
   "source": [
    "## Ways Of Correcting Vanishing And Exploding Gradients\n",
    "\n",
    "### Activation Function Choices\n",
    "\n",
    "The choice of activation functions can significantly affect how gradients propagate through the network.  \n",
    "\n",
    "- **Sigmoid and tanh** tend to cause vanishing gradients due to their saturating outputs.  \n",
    "- **ReLU (Rectified Linear Unit)** and its variants (e.g., **Leaky ReLU**, **Parametric ReLU**) are preferred because they do not saturate for positive inputs, helping alleviate vanishing gradient problems.\n",
    "\n",
    "**Example:**  \n",
    "- Using **ReLU**:  \n",
    "  $$\n",
    "  f(x) = \\max(0, x)\n",
    "  $$\n",
    "  \n",
    "  The derivative is either **1** (for positive inputs) or **0** (for negative inputs). This ensures that the gradient does not decay as rapidly as in sigmoid or tanh.\n",
    "\n",
    "### Batch Normalization\n",
    "\n",
    "Batch normalization normalizes the inputs to each layer, ensuring that the values lie within a stable range during training.  \n",
    "\n",
    "- **How it helps:** By normalizing the inputs, batch normalization prevents the gradients from becoming too small or too large.  \n",
    "- **Where it’s applied:** Typically applied before or after the activation function within each layer.\n",
    "- **Effect of batch normalization:**  \n",
    "    - Reduces internal covariate shift (where input distributions change during training). \n",
    "    - Helps accelerate training by making the gradients more stable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b4436d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, BatchNormalization, ReLU\n",
    "from tensorflow.keras import Sequential\n",
    "\n",
    "# Adding batch normalization in a neural network\n",
    "model = Sequential([\n",
    "    Dense(64, input_shape=(10,)),\n",
    "    BatchNormalization(),  # Normalizes the inputs to this layer\n",
    "    ReLU(),  # Activation function after normalization\n",
    "    Dense(1)  # Output layer\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282e9a27",
   "metadata": {},
   "source": [
    "### Weight Initialization Techniques\n",
    "\n",
    "Proper weight initialization can reduce both vanishing and exploding gradients by ensuring that the initial values of the weights do not lead to excessively large or small activations.\n",
    "\n",
    "- **Xavier Initialization (Glorot Initialization):**  \n",
    "  Used for layers with **sigmoid** or **tanh** activations.  \n",
    "  $$\n",
    "  W \\sim \\mathcal{U} \\left( -\\sqrt{\\frac{6}{n_{\\text{in}} + n_{\\text{out}}}}, \\sqrt{\\frac{6}{n_{\\text{in}} + n_{\\text{out}}}} \\right)\n",
    "  $$\n",
    "\n",
    "- **He Initialization:**  \n",
    "  Used for layers with **ReLU** activations.  \n",
    "  $$\n",
    "  W \\sim \\mathcal{N} \\left( 0, \\frac{2}{n_{\\text{in}}} \\right)\n",
    "  $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ab7918",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.initializers import HeNormal\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(64, input_shape=(10,), kernel_initializer=HeNormal()),  # He initialization for ReLU\n",
    "    tf.keras.layers.ReLU(),\n",
    "    Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb425cee",
   "metadata": {},
   "source": [
    "### Residual Connections (ResNets)\n",
    "\n",
    "Residual networks (ResNets) address vanishing gradients in very deep networks by allowing the gradient to flow more directly through the network. Instead of passing the input through many layers sequentially, **residual connections** skip layers by adding the input to the output of a layer.\n",
    "\n",
    "**How it helps:**  \n",
    "- Residual connections help maintain a strong gradient signal even in deep networks by allowing the network to learn **identity mappings**.\n",
    "\n",
    "**Residual connection formula:**  \n",
    "\n",
    "Given the input $x$ and a function $F(x)$ representing the transformation of the layer:  \n",
    "\n",
    "$$\n",
    "\\text{Output} = F(x) + x\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fafe34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class ResidualBlock(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(units, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(units)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        return x + inputs  # Residual connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c7736c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebc8bee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
