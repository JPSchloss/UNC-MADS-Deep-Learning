{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a74f39b",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f54a256",
   "metadata": {},
   "source": [
    "## Text Representations\n",
    "\n",
    "In machine learning (ML) and neural networks, the data often needs to be numerical since models cannot usually work directly with raw text. Text is inherently complex, filled with semantics, context, and structure that computers cannot easily interpret. To train models effectively on textual data, we need systematic ways to convert text into numerical representations while preserving as much relevant information as possible. Without this transformation, machine learning algorithms would be unable to derive patterns or make predictions from text data.\n",
    "\n",
    "Text is composed of characters and words, which hold meaning for humans but are not directly understandable by machines. To perform tasks like classification, clustering, or generation, machine learning models need features, or numerical inputs, that capture relevant patterns in the text. \n",
    "\n",
    "Effective text representations help models understand and make predictions by encoding:\n",
    "- Frequency and presence of important words\n",
    "- Context and relationships between words\n",
    "- Semantic meaning beyond the surface structure\n",
    "\n",
    "The quality of these representations directly impact model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1dda45",
   "metadata": {},
   "source": [
    "## Different Approaches To Text Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbe0a88",
   "metadata": {},
   "source": [
    "![](https://aiml.com/wp-content/uploads/2023/02/disadvantage-bow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc67bfb3",
   "metadata": {},
   "source": [
    "### Bag of Words (BoW)  \n",
    "\n",
    "The Bag of Words approach is one of the simplest and most widely used text representations. It involves creating a vocabulary from the text and representing each document as a vector indicating the presence or frequency of each word in the vocabulary.\n",
    "\n",
    "For example, consider two sentences:  \n",
    "- \"The cat sat on the mat.\"  \n",
    "- \"The dog sat on the mat.\"  \n",
    "\n",
    "The vocabulary from these sentences is: `['The', 'cat', 'dog', 'sat', 'on', 'mat']`. \n",
    "The first sentence can be represented as a vector: `[1, 1, 0, 1, 1, 1]`, where each entry corresponds to the count of a word from the vocabulary. \n",
    "\n",
    "While BoW is easy to implement, it has limitations. Primarily, it ignores word order and the context in which words appear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ca3be9",
   "metadata": {},
   "source": [
    "#### Example Using CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ab02b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words Representation:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>at</th>\n",
       "      <th>ball</th>\n",
       "      <th>barked</th>\n",
       "      <th>cat</th>\n",
       "      <th>chased</th>\n",
       "      <th>dog</th>\n",
       "      <th>mat</th>\n",
       "      <th>mouse</th>\n",
       "      <th>on</th>\n",
       "      <th>sat</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   at  ball  barked  cat  chased  dog  mat  mouse  on  sat  the\n",
       "0   0     0       0    1       0    0    1      0   1    1    2\n",
       "1   1     0       1    1       0    1    0      0   0    0    2\n",
       "2   0     0       0    1       1    0    0      1   0    0    2\n",
       "3   0     1       0    0       1    1    0      0   0    0    2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\n",
    "    \"The cat sat on the mat\",\n",
    "    \"The dog barked at the cat\",\n",
    "    \"The cat chased the mouse\",\n",
    "    \"The dog chased the ball\"\n",
    "]\n",
    "\n",
    "# Initialize the CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the corpus into BoW representation\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Convert the result to a DataFrame for readability\n",
    "bow_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display the Bag of Words representation\n",
    "print(\"Bag of Words Representation:\")\n",
    "bow_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65a14a6",
   "metadata": {},
   "source": [
    "### TF-IDF (Term Frequency-Inverse Document Frequency)  \n",
    "\n",
    "TF-IDF is a statistical measure used to evaluate how important a word (term) is to a document within a collection or corpus of documents. It builds upon the simple Bag of Words (BoW) representation by incorporating both the frequency of a term in a single document and how common (or rare) that term is across the entire corpus. In other words, TF-IDF helps us down-weight very common words (like “the”, “is”, “and”) that carry little distinguishing information and up-weight rarer and more informative words.\n",
    "\n",
    "#### Term Frequency (TF)\n",
    "\n",
    "The term frequency component quantifies how often a given term appears in a specific document. A higher TF value means the term appears more often in that document, suggesting it may be important within that document’s context. There are a few common ways to define TF:\n",
    "\n",
    "- **Raw Count**:  \n",
    "  $\\mathrm{TF}(t, d) = \\text{count of term } t \\text{ in document } d.$\n",
    "  \n",
    "\n",
    "- **Normalized Frequency** (accounts for document length):  \n",
    "  $\\mathrm{TF}(t, d) = \\frac{\\text{count of } t \\text{ in } d}{\\text{total words in } d}.$\n",
    "  \n",
    "\n",
    "- **Log-Scaled Frequency** (dampens large counts):  \n",
    "  $\\mathrm{TF}(t, d) =\n",
    "    \\begin{cases}\n",
    "      1 + \\log(\\text{count of } t \\text{ in } d), & \\text{if count} > 0,\\\\\n",
    "      0, & \\text{otherwise}.\n",
    "    \\end{cases}$\n",
    "    \n",
    "\n",
    "#### Inverse Document Frequency (IDF)\n",
    "\n",
    "While TF measures how often a term appears in one document, IDF measures how unique or rare that term is across the entire corpus. Intuitively:\n",
    "\n",
    "- If a term appears in almost every document (e.g., “the” or “and”), it carries less discriminative power.\n",
    "- If a term appears in very few documents (e.g., “photosynthesis” in a mixed corpus), it is more informative.\n",
    "\n",
    "A common definition of IDF is:\n",
    "\n",
    "$$\n",
    "\\mathrm{IDF}(t, D) \\;=\\; \\log\\!\\Bigl(\\frac{N}{\\,|\\{d \\in D: t \\in d\\}|\\,}\\Bigr),\n",
    "$$  \n",
    "\n",
    "where:\n",
    "\n",
    "- $N$ is the total number of documents in the corpus $D$.\n",
    "- $|\\{\\,d \\in D : t \\in d\\,\\}|$ is the number of documents in which term $t$ appears (the document frequency of $t$).\n",
    "\n",
    "\n",
    "Some variants add 1 to the denominator (or to the log) to avoid division by zero or negative values:  \n",
    "$$\n",
    "\\mathrm{IDF}(t, D) = \\log\\!\\Bigl(\\frac{1+N}{1 + |\\{d: t \\in d\\}|}\\Bigr) + 1.\n",
    "$$\n",
    "\n",
    "The higher the IDF, the rarer the term is across documents.\n",
    "\n",
    "#### Combining TF and IDF\n",
    "\n",
    "The core idea of TF-IDF is to multiply the two components:\n",
    "\n",
    "$$\n",
    "\\text{TF-IDF}(t, d, D) \\;=\\; \\text{TF}(t, d) \\times \\text{IDF}(t, D).\n",
    "$$\n",
    "\n",
    "- If a term $t$ occurs frequently in a document $d$ (high TF) but rarely in the rest of the corpus (high IDF), then TF-IDF is large.  \n",
    "- If a term is common in a document but also common across many documents (low IDF), then TF-IDF remains moderate.  \n",
    "- If a term is very rare in the specific document (low TF), its TF-IDF is low regardless of its IDF.  \n",
    "\n",
    "By weighting words this way, we produce a vector representation for each document where each dimension corresponds to a vocabulary term, and the value is the TF-IDF score. These vectors can then be used as features in downstream tasks (classification, clustering, information retrieval, etc.).\n",
    "\n",
    "#### Limitations of TF-IDF\n",
    "\n",
    "- **Ignores Word Order**: TF-IDF still treats each term independently and does not capture the sequence or context in which words appear. For example, “dog bites man” and “man bites dog” have the same TF-IDF representation even though their meanings differ.\n",
    "\n",
    "- **Ignores Word Relationships/Semantics**: Synonyms (“car” vs. “automobile”) will be treated as completely separate dimensions. TF-IDF does not capture semantic similarity between related words.\n",
    "\n",
    "- **High Dimensionality**: The resulting feature space is as large as the vocabulary size, which can be in the tens or hundreds of thousands. Sparse, high-dimensional feature vectors can be computationally expensive to store and process.\n",
    "\n",
    "- **Static Across Corpus**: Once you compute IDF, those values remain fixed unless you retrain. In dynamic corpora where documents are continually added, TF-IDF scores can become stale unless recalculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bad5643d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Representation:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>at</th>\n",
       "      <th>ball</th>\n",
       "      <th>barked</th>\n",
       "      <th>cat</th>\n",
       "      <th>chased</th>\n",
       "      <th>dog</th>\n",
       "      <th>mat</th>\n",
       "      <th>mouse</th>\n",
       "      <th>on</th>\n",
       "      <th>sat</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.301002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.471578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.471578</td>\n",
       "      <td>0.471578</td>\n",
       "      <td>0.492178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.492768</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.492768</td>\n",
       "      <td>0.314527</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.388504</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.514293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.361459</td>\n",
       "      <td>0.446473</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.566295</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.591032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.547794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.431887</td>\n",
       "      <td>0.431887</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.571724</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         at      ball    barked       cat    chased       dog       mat  \\\n",
       "0  0.000000  0.000000  0.000000  0.301002  0.000000  0.000000  0.471578   \n",
       "1  0.492768  0.000000  0.492768  0.314527  0.000000  0.388504  0.000000   \n",
       "2  0.000000  0.000000  0.000000  0.361459  0.446473  0.000000  0.000000   \n",
       "3  0.000000  0.547794  0.000000  0.000000  0.431887  0.431887  0.000000   \n",
       "\n",
       "      mouse        on       sat       the  \n",
       "0  0.000000  0.471578  0.471578  0.492178  \n",
       "1  0.000000  0.000000  0.000000  0.514293  \n",
       "2  0.566295  0.000000  0.000000  0.591032  \n",
       "3  0.000000  0.000000  0.000000  0.571724  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\n",
    "    \"The cat sat on the mat\",\n",
    "    \"The dog barked at the cat\",\n",
    "    \"The cat chased the mouse\",\n",
    "    \"The dog chased the ball\"\n",
    "]\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the corpus into TF-IDF representation\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Convert the result to a DataFrame for readability\n",
    "tfidf_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display the TF-IDF matrix\n",
    "print(\"TF-IDF Representation:\")\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd897da1",
   "metadata": {},
   "source": [
    "## Word Embeddings  \n",
    "\n",
    "Word embeddings are a type of dense vector representation where words with similar meanings have similar vector representations. Instead of treating words as discrete entities, embeddings place them in a continuous vector space, allowing models to capture semantic relationships and contextual similarity. \n",
    "\n",
    "For instance, in a well-trained word embedding model, the relationship \"king - man + woman\" could approximate the vector for \"queen,\" showing how embeddings can encode relational knowledge.\n",
    "\n",
    "Word embeddings are typically generated by training on large corpora of text. The model learns to place words in the vector space such that words that appear in similar contexts have similar vector representations. The training process focuses on capturing semantic relationships and dependencies between words.\n",
    "\n",
    "Popular methods for generating word embeddings include **Word2Vec**, **GloVe**, and **FastText**. These models learn from large corpora and capture relationships between words, such as analogies and semantic similarities.\n",
    "\n",
    "Word embeddings overcome many limitations of simpler methods like BoW and TF-IDF by capturing semantic meaning and some degree of context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bf2e4a",
   "metadata": {},
   "source": [
    "## Word2Vec  \n",
    "\n",
    "Word2Vec is a popular algorithm for generating word embeddings. Word2Vec uses neural networks to map words into a continuous vector space based on their context in a corpus of text. The key idea is that words appearing in similar contexts will have similar vector representations.  \n",
    "\n",
    "The training objective of Word2Vec is to predict either:\n",
    "- the surrounding words given a target word (Skip-Gram) \n",
    "- or predict the target word given its surrounding words (CBOW). \n",
    "  \n",
    "**Skip-Gram:**\n",
    "- The model looks at a single “center” word and tries to guess which words appear around it in the sentence. For example, if the center word is “dog,” it will learn to predict words like “barks,” “leash,” or “park” if those tend to appear near “dog.”\n",
    "\n",
    "**CBOW (Continuous Bag of Words):**\n",
    "- The model does the reverse. It takes a group of words around a blank spot (the context) and tries to predict which word belongs in that position. For instance, seeing “the ___ chased the ball,” it would learn to predict “dog.”\n",
    "\n",
    "Because it only needs to pay attention to a few words at a time rather than the entire vocabulary, Word2Vec can process really large amounts of text very quickly. The end result is that words with similar meanings or grammar roles (like “run” and “jog,” or “cat” and “kitty”) get mapped to vectors that sit close together in this new numerical space. These vectors then become convenient features for tasks like classifying documents, finding related words, or powering simple search systems.\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:1200/1*xC6wfTU_zpUlpRlXs5NZ4w.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ab6046",
   "metadata": {},
   "source": [
    "### Skip-Gram Model  \n",
    "\n",
    "The Skip-Gram model, a core component of Word2Vec, predicts the surrounding context words given a target word. For example, given the word \"cat,\" the model will try to predict nearby words like \"the,\" \"sat,\" or \"mat.\"  \n",
    "\n",
    "Mathematically, this looks like:  \n",
    "\n",
    "$$\n",
    "\\text{maximize} \\, \\sum \\log P(\\text{context} \\mid \\text{target})\n",
    "$$\n",
    "\n",
    "Skip-Gram is particularly useful for capturing meaningful relationships when the corpus is large, as it can effectively handle rare words. One way to understand Skip-Gram intuitively is to visualize a sliding window moving over the text and predicting the words within the window."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e215ab",
   "metadata": {},
   "source": [
    "### Continuous Bag of Words (CBOW)  \n",
    "\n",
    "CBOW is the opposite of the Skip-Gram model. Instead of predicting context words given a target word, it predicts the target word based on its surrounding context. Given a window of words surrounding a target word, the model aims to predict the target word.\n",
    "\n",
    "Mathematically, this looks like:\n",
    "\n",
    "$$\n",
    "\\text{maximize} \\, \\sum \\log P(\\text{target} \\mid \\text{context})\n",
    "$$  \n",
    "\n",
    "CBOW is generally faster to train than Skip-Gram because it averages the context word vectors to predict the target word. However, it may not perform as well on rare words since it relies heavily on the surrounding context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd39a662",
   "metadata": {},
   "source": [
    "### Coding Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20623998",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2b01926a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'cat':\n",
      "[-0.01753848  0.00737714  0.01058508  0.01203614  0.01473678 -0.01264185\n",
      "  0.00283708  0.01270612 -0.00632933 -0.01250195 -0.00059064 -0.01700857\n",
      " -0.01139127  0.01432026  0.00677256  0.01453993  0.01423641  0.01551222\n",
      " -0.00832898 -0.00157581  0.00498176 -0.00899076  0.01758184 -0.01981414\n",
      "  0.01403708  0.00569289 -0.00990202  0.00932641 -0.00395903  0.01334891\n",
      "  0.0198663  -0.00912277 -0.00077523 -0.01217308  0.00750499  0.00543842\n",
      "  0.01447675  0.01197266  0.01928601  0.01860449  0.0156456  -0.01382573\n",
      " -0.01894968 -0.00101606 -0.0058394   0.01620697  0.01174434 -0.00293944\n",
      "  0.00312232  0.00403298]\n",
      "\n",
      "Most similar words to 'cat':\n",
      "on: 0.18660055100917816\n",
      "the: 0.16965018212795258\n",
      "mat: 0.16849403083324432\n",
      "\n",
      "Similarity between 'cat' and 'dog': 0.03\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Sample corpus (list of tokenized sentences)\n",
    "corpus = [\n",
    "    [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"],\n",
    "    [\"the\", \"dog\", \"barked\", \"at\", \"the\", \"cat\"],\n",
    "    [\"the\", \"dog\", \"chased\", \"the\", \"ball\"],\n",
    "    [\"the\", \"cat\", \"chased\", \"the\", \"mouse\"],\n",
    "    [\"the\", \"dog\", \"sat\", \"on\", \"the\", \"sofa\"]\n",
    "]\n",
    "\n",
    "# Train Word2Vec model - Skip-Gram Model\n",
    "model = Word2Vec(sentences=corpus, vector_size=50, window=3, min_count=1, sg=1, epochs=100)\n",
    "\n",
    "# Train Word2Vec model - CBOW Model\n",
    "#model = Word2Vec(sentences=corpus, vector_size=50, window=3, min_count=1, sg=1, epochs=100)\n",
    "\n",
    "# Save the trained model\n",
    "# model.save(\"word2vec.model\")\n",
    "\n",
    "# Access vector for a word\n",
    "cat_vector = model.wv[\"cat\"]\n",
    "print(f\"Vector for 'cat':\\n{cat_vector}\")\n",
    "\n",
    "# Find the most similar words to 'cat'\n",
    "similar_words = model.wv.most_similar(\"cat\", topn=3)\n",
    "print(\"\\nMost similar words to 'cat':\")\n",
    "for word, score in similar_words:\n",
    "    print(f\"{word}: {score}\")\n",
    "\n",
    "# Find similarity between two words\n",
    "similarity_score = model.wv.similarity(\"cat\", \"dog\")\n",
    "print(f\"\\nSimilarity between 'cat' and 'dog': {similarity_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d212c962",
   "metadata": {},
   "source": [
    "#### Explanation of Parameters:\n",
    "\n",
    "- **`sentences`**: The corpus of tokenized sentences used for training.  \n",
    "- **`vector_size=50`**: The size (dimensionality) of the word embeddings.  \n",
    "- **`window=3`**: The context window size, meaning the model looks at 3 words before and after the target word.  \n",
    "- **`min_count=1`**: Minimum number of occurrences for a word to be included in the vocabulary.  \n",
    "- **`sg=1`**: Skip-Gram model (set `sg=0` for CBOW).  \n",
    "- **`epochs=100`**: Number of training epochs (iterations over the corpus).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7015817c",
   "metadata": {},
   "source": [
    "## Alternatives to Word2Vec  \n",
    "\n",
    "Although Word2Vec is widely used, several alternative methods exist:  \n",
    "\n",
    "- **GloVe (Global Vectors for Word Representation):** GloVe focuses on co-occurrence statistics across the entire corpus rather than relying on local context windows. It constructs a co-occurrence matrix and factorizes it to generate word embeddings. GloVe works well for capturing global context and semantic relationships.  \n",
    "- **FastText:** Developed by Facebook, FastText extends Word2Vec by incorporating subword information. It breaks words into character n-grams, making it better at handling rare words and morphologically rich languages.  \n",
    "- **BERT and Contextual Embeddings:** Unlike static embeddings like Word2Vec and GloVe, contextual embeddings (e.g., BERT, GPT) generate dynamic representations based on the context in which a word appears. These embeddings are ideal for tasks where word meaning depends on surrounding words.  \n",
    "\n",
    "### Contextual Embeddings  \n",
    "\n",
    "Contextual embeddings take word representations a step further by making them dependent on the surrounding text. Models like **BERT**, **GPT**, and **RoBERTa** generate embeddings dynamically, meaning the same word can have different representations depending on its usage. This is particularly useful for disambiguating polysemous words (e.g., the word \"bank\" has different meanings in \"river bank\" and \"financial bank\").\n",
    "\n",
    "### Character-Level Representations  \n",
    "\n",
    "Rather than representing words as whole units, some models use representations based on characters. This is particularly beneficial in cases where words are rare, misspelled, or morphologically complex. \n",
    "\n",
    "Character-level models can detect subword patterns and handle previously unseen words more effectively. Techniques like **Char-CNN** and **Char-RNN** implement this approach for languages with complex morphology.\n",
    "\n",
    "While character-level representations add robustness, they often require more computational resources and longer training times compared to word-based approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7ce7db",
   "metadata": {},
   "source": [
    "## Important Things to Consider  \n",
    "\n",
    "#### Window Size  \n",
    "The window size defines the number of words around a target word that the model should consider when learning embeddings. A shorter window (e.g., 2-3 words) typically captures **syntactic** information, such as word order or grammatical relationships. A longer window (e.g., 5-10 words) is better for capturing **semantic or topical relationships** between words.  \n",
    "\n",
    "For example:  \n",
    "- A shorter window size may learn that \"run\" and \"ran\" have similar grammatical functions. \n",
    "- A longer window size may capture that \"book\" and \"novel\" are topically related, even if they are not grammatically similar.  \n",
    "  \n",
    "#### Subword Information  \n",
    "Languages with complex morphology or compound words benefit from embeddings that capture subword information. Techniques like FastText and character-based embeddings decompose words into smaller units, enabling models to handle unseen or rare words more effectively.  \n",
    "#### Handling Out-of-Vocabulary (OOV) Words  \n",
    "Static embeddings, such as Word2Vec or GloVe, suffer from the issue of out-of-vocabulary (OOV) words. If a word is not in the training corpus, it will not have an embedding. Contextual embeddings and subword-based models provide solutions to this problem by dynamically generating embeddings or breaking words into subunits.  \n",
    "\n",
    "#### Semantic and Syntactic Trade-offs  \n",
    "Depending on the task, you may need embeddings that prioritize semantic relationships (e.g., grouping words by topic) or syntactic relationships (e.g., grammatical roles). Tuning parameters like window size, corpus size, and embedding dimensions allows for optimizing the model based on your needs.\n",
    "\n",
    "#### Dimensionality and Training Time  \n",
    "Choosing the dimensionality of the word vectors is important. Larger dimensions generally result in better performance but increase computational cost and risk overfitting. Practical implementations balance the quality of embeddings with the available computational resources.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2980262f",
   "metadata": {},
   "source": [
    "## Distance Functions\n",
    "\n",
    "When we represent text as vectors in an **n-dimensional space**, comparing the similarity between different text representations involves calculating the \"distance\" between them. \n",
    "\n",
    "These distances provide a quantitative measure of how similar (or different) two text samples are, which is crucial for tasks like document clustering, classification, and information retrieval.\n",
    "\n",
    "The choice of distance function can impact the performance of your model, as different tasks may require different types of similarity measures. \n",
    "\n",
    "![](https://www.maartengrootendorst.com/images/posts/2021-01-02-distances/header.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2d0661",
   "metadata": {},
   "source": [
    "### Euclidean Distance\n",
    "\n",
    "Euclidean distance is one of the most common and straightforward distance metrics. It measures the straight-line distance between two points in space. \n",
    "\n",
    "Given two vectors $ \\mathbf{A} $ and $ \\mathbf{B} $, the Euclidean distance $ d $ is defined as:\n",
    "\n",
    "$$\n",
    "d(\\mathbf{A}, \\mathbf{B}) = \\sqrt{\\sum_{i=1}^{n} (A_i - B_i)^2}\n",
    "$$\n",
    "\n",
    "Intuitively, this works well when comparing vectors based on magnitude or when word order does not matter. However, in high-dimensional spaces or sparse text data, Euclidean distance may not always capture meaningful similarities.\n",
    "\n",
    "#### Code Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0757ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean distance (manual): 3.605551275463989\n",
      "Euclidean distance (scipy): 3.605551275463989\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "# Define two vectors (representing text vectors or points in n-dimensional space)\n",
    "vector1 = np.array([1, 2, 3])\n",
    "vector2 = np.array([4, 0, 3])\n",
    "\n",
    "# Method 1: Manual computation of Euclidean distance\n",
    "euclidean_manual = np.sqrt(np.sum((vector1 - vector2)**2))\n",
    "print(f\"Euclidean distance (manual): {euclidean_manual}\")\n",
    "\n",
    "# Method 2: Using scipy's euclidean function\n",
    "euclidean_scipy = euclidean(vector1, vector2)\n",
    "print(f\"Euclidean distance (scipy): {euclidean_scipy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aea3df7",
   "metadata": {},
   "source": [
    "### Manhattan Distance (L1 Distance)\n",
    "\n",
    "Manhattan distance measures the distance by summing the absolute differences between the vector components. \n",
    "\n",
    "Mathematically, it is given by:\n",
    "\n",
    "$$\n",
    "d(\\mathbf{A}, \\mathbf{B}) = \\sum_{i=1}^{n} |A_i - B_i|\n",
    "$$\n",
    "\n",
    "This metric is useful when movement between dimensions is restricted. It is often used in text-related tasks where small deviations in feature values are meaningful.\n",
    "\n",
    "#### Code Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28e4443c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manhattan distance (manual): 5\n",
      "Manhattan distance (scipy): 5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cityblock  # Scipy function for Manhattan distance\n",
    "\n",
    "# Define two vectors (representing text vectors or points in n-dimensional space)\n",
    "vector1 = np.array([1, 2, 3])\n",
    "vector2 = np.array([4, 0, 3])\n",
    "\n",
    "# Method 1: Manual computation of Manhattan distance\n",
    "manhattan_manual = np.sum(np.abs(vector1 - vector2))\n",
    "print(f\"Manhattan distance (manual): {manhattan_manual}\")\n",
    "\n",
    "# Method 2: Using scipy's cityblock function\n",
    "manhattan_scipy = cityblock(vector1, vector2)\n",
    "print(f\"Manhattan distance (scipy): {manhattan_scipy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c9a9d8",
   "metadata": {},
   "source": [
    "### Minkowski Distance\n",
    "\n",
    "The **Minkowski distance** is a generalized form of distance that includes both the **Euclidean distance** and **Manhattan distance** as special cases. The formula for the Minkowski distance between two vectors $ \\mathbf{A} $ and $ \\mathbf{B} $ is:\n",
    "\n",
    "$$\n",
    "d(\\mathbf{A}, \\mathbf{B}) = \\left( \\sum_{i=1}^{n} |A_i - B_i|^p \\right)^{\\frac{1}{p}}\n",
    "$$\n",
    "\n",
    "- **p = 1:** The formula becomes the **Manhattan distance**.  \n",
    "- **p = 2:** The formula becomes the **Euclidean distance**.  \n",
    "- For larger values of $ p $, the metric places more emphasis on large differences between vector components.  \n",
    "\n",
    "#### Code Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfbe59f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minkowski distance (manual, p=3): 3.2710663101885897\n",
      "Minkowski distance (scipy, p=3): 3.2710663101885897\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import minkowski\n",
    "\n",
    "# Define two vectors (representing text vectors or points in n-dimensional space)\n",
    "vector1 = np.array([1, 2, 3])\n",
    "vector2 = np.array([4, 0, 3])\n",
    "\n",
    "# Method 1: Manual computation of Minkowski distance with p = 3\n",
    "p = 3\n",
    "minkowski_manual = np.power(np.sum(np.abs(vector1 - vector2)**p), 1/p)\n",
    "print(f\"Minkowski distance (manual, p=3): {minkowski_manual}\")\n",
    "\n",
    "# Method 2: Using scipy's minkowski function\n",
    "minkowski_scipy = minkowski(vector1, vector2, p=3)\n",
    "print(f\"Minkowski distance (scipy, p=3): {minkowski_scipy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7c5f93",
   "metadata": {},
   "source": [
    "### Cosine Similarity\n",
    "\n",
    "Cosine similarity is a popular measure for text representations because it compares the **angles between vectors** rather than their magnitudes. This makes it ideal for text data, where two documents with different lengths may still have high similarity if they share the same word distribution.\n",
    "\n",
    "The formula for cosine similarity between two vectors $ \\mathbf{A} $ and $ \\mathbf{B} $ is:\n",
    "\n",
    "$$\n",
    "\\text{cosine similarity}(\\mathbf{A}, \\mathbf{B}) = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\| \\|\\mathbf{B}\\|}\n",
    "$$\n",
    "\n",
    "Here, $ \\mathbf{A} \\cdot \\mathbf{B} $ represents the dot product of the two vectors, and $ \\|\\mathbf{A}\\| $ and $ \\|\\mathbf{B}\\| $ represent their magnitudes.\n",
    "\n",
    "Cosine similarity is widely used in **information retrieval** and **document clustering** because it focuses on the direction of vectors, making it less sensitive to differences in vector length.\n",
    "\n",
    "#### Code Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c61bd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity (manual): 0.6948792289723034\n",
      "Cosine similarity (scikit-learn): 0.6948792289723034\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Define two vectors (representing text vectors or points in n-dimensional space)\n",
    "vector1 = np.array([1, 2, 3])\n",
    "vector2 = np.array([4, 0, 3])\n",
    "\n",
    "# Method 1: Manual computation of cosine similarity\n",
    "dot_product = np.dot(vector1, vector2)\n",
    "magnitude1 = np.linalg.norm(vector1)\n",
    "magnitude2 = np.linalg.norm(vector2)\n",
    "cosine_similarity_manual = dot_product / (magnitude1 * magnitude2)\n",
    "print(f\"Cosine similarity (manual): {cosine_similarity_manual}\")\n",
    "\n",
    "# Method 2: Using scikit-learn's cosine_similarity function\n",
    "cosine_similarity_sklearn = cosine_similarity([vector1], [vector2])[0][0]\n",
    "print(f\"Cosine similarity (scikit-learn): {cosine_similarity_sklearn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab1bd10",
   "metadata": {},
   "source": [
    "### Other Distance Measures\n",
    "\n",
    "There are additional distance measures you may encounter depending on the specific problem:\n",
    "\n",
    "1. **Jaccard Similarity:** Measures the similarity between two sets by dividing the size of their intersection by the size of their union. This is particularly useful for comparing sparse vectors or binary representations of text.\n",
    "   $$\n",
    "   J(A, B) = \\frac{|A \\cap B|}{|A \\cup B|}\n",
    "   $$\n",
    "\n",
    "2. **Hamming Distance:** Measures the number of positions where corresponding components of two vectors differ. It is commonly used for comparing binary or categorical data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fff317",
   "metadata": {},
   "source": [
    "## Real World Example Using Newsgroup Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c111d651",
   "metadata": {},
   "source": [
    "#### Install Required Libraries (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f04f93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim scikit-learn nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93cf51f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e50da57",
   "metadata": {},
   "source": [
    "#### Load The Dataset\n",
    "\n",
    "We are using the 20 Newsgroups dataset, a common benchmark dataset in NLP. It contains around 18,000 newsgroup documents spread across 20 different categories, including topics like sports, politics, computers, and religion. This dataset is widely used for text classification, clustering, and word embedding demonstrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eea77383",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/jonathanschlosser/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "# Download NLTK tokenizer\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Load the 20 Newsgroups dataset\n",
    "newsgroups_data = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daea09a",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0047009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the documents into sentences of words\n",
    "sentences = [word_tokenize(doc.lower()) for doc in newsgroups_data.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "623fb7d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'am',\n",
       " 'sure',\n",
       " 'some',\n",
       " 'bashers',\n",
       " 'of',\n",
       " 'pens',\n",
       " 'fans',\n",
       " 'are',\n",
       " 'pretty',\n",
       " 'confused',\n",
       " 'about',\n",
       " 'the',\n",
       " 'lack',\n",
       " 'of',\n",
       " 'any',\n",
       " 'kind',\n",
       " 'of',\n",
       " 'posts',\n",
       " 'about',\n",
       " 'the',\n",
       " 'recent',\n",
       " 'pens',\n",
       " 'massacre',\n",
       " 'of',\n",
       " 'the',\n",
       " 'devils',\n",
       " '.',\n",
       " 'actually',\n",
       " ',',\n",
       " 'i',\n",
       " 'am',\n",
       " 'bit',\n",
       " 'puzzled',\n",
       " 'too',\n",
       " 'and',\n",
       " 'a',\n",
       " 'bit',\n",
       " 'relieved',\n",
       " '.',\n",
       " 'however',\n",
       " ',',\n",
       " 'i',\n",
       " 'am',\n",
       " 'going',\n",
       " 'to',\n",
       " 'put',\n",
       " 'an',\n",
       " 'end',\n",
       " 'to',\n",
       " 'non-pittsburghers',\n",
       " \"'\",\n",
       " 'relief',\n",
       " 'with',\n",
       " 'a',\n",
       " 'bit',\n",
       " 'of',\n",
       " 'praise',\n",
       " 'for',\n",
       " 'the',\n",
       " 'pens',\n",
       " '.',\n",
       " 'man',\n",
       " ',',\n",
       " 'they',\n",
       " 'are',\n",
       " 'killing',\n",
       " 'those',\n",
       " 'devils',\n",
       " 'worse',\n",
       " 'than',\n",
       " 'i',\n",
       " 'thought',\n",
       " '.',\n",
       " 'jagr',\n",
       " 'just',\n",
       " 'showed',\n",
       " 'you',\n",
       " 'why',\n",
       " 'he',\n",
       " 'is',\n",
       " 'much',\n",
       " 'better',\n",
       " 'than',\n",
       " 'his',\n",
       " 'regular',\n",
       " 'season',\n",
       " 'stats',\n",
       " '.',\n",
       " 'he',\n",
       " 'is',\n",
       " 'also',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'fo',\n",
       " 'fun',\n",
       " 'to',\n",
       " 'watch',\n",
       " 'in',\n",
       " 'the',\n",
       " 'playoffs',\n",
       " '.',\n",
       " 'bowman',\n",
       " 'should',\n",
       " 'let',\n",
       " 'jagr',\n",
       " 'have',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'of',\n",
       " 'fun',\n",
       " 'in',\n",
       " 'the',\n",
       " 'next',\n",
       " 'couple',\n",
       " 'of',\n",
       " 'games',\n",
       " 'since',\n",
       " 'the',\n",
       " 'pens',\n",
       " 'are',\n",
       " 'going',\n",
       " 'to',\n",
       " 'beat',\n",
       " 'the',\n",
       " 'pulp',\n",
       " 'out',\n",
       " 'of',\n",
       " 'jersey',\n",
       " 'anyway',\n",
       " '.',\n",
       " 'i',\n",
       " 'was',\n",
       " 'very',\n",
       " 'disappointed',\n",
       " 'not',\n",
       " 'to',\n",
       " 'see',\n",
       " 'the',\n",
       " 'islanders',\n",
       " 'lose',\n",
       " 'the',\n",
       " 'final',\n",
       " 'regular',\n",
       " 'season',\n",
       " 'game',\n",
       " '.',\n",
       " 'pens',\n",
       " 'rule',\n",
       " '!',\n",
       " '!',\n",
       " '!']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fee7db",
   "metadata": {},
   "source": [
    "#### Train Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52b42c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Word2Vec model on the tokenized sentences\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=5, sg=1, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b4777b",
   "metadata": {},
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b031285c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'computer':\n",
      "[-0.21225789  0.5404231   0.40459347  0.32228127  0.12945211 -0.3299833\n",
      " -0.09653363  0.20713666 -0.36554763  0.13997215 -0.37456098 -0.67297906\n",
      " -0.3721857   0.00911829 -0.20481554 -0.36107153  0.7215204   0.3608223\n",
      " -0.41200316 -0.7169939   0.38293985  0.86623466  0.09137827 -0.26366988\n",
      "  0.23594125 -0.17807429 -0.17314552 -0.07365485  0.2280913   0.200225\n",
      "  0.03429207 -0.2574355   0.28391963 -0.32798564 -0.03226035 -0.5596238\n",
      " -0.12443216  0.20995711  0.05276503 -0.3246789   0.9564842  -0.6072698\n",
      "  0.62925166  0.00243073  0.15336983  0.07485364 -0.31112248 -0.06012597\n",
      " -0.04015968  0.29611218 -0.20674986 -0.33931953 -0.40383413 -0.0755101\n",
      "  0.12370312  0.03053324 -0.13557634 -0.25636342 -0.26846898  0.263858\n",
      " -0.21728204  0.4428228   0.11869055  0.20396456 -0.00572924  0.39625686\n",
      " -0.02928978  0.41261107  0.07562852  0.1514651   0.10060371 -0.1340094\n",
      "  0.09531747 -0.27664697  0.58087593 -0.3628328  -0.00305403  0.30699235\n",
      " -0.08109504  0.03020633 -0.24945392  0.23120221 -0.13801555 -0.02945856\n",
      " -0.2450922  -0.3351114   0.04982343  0.5667882  -0.293193    0.30734646\n",
      "  0.33727098 -0.13578098  0.35929447  0.0221448  -0.13967526 -0.31036037\n",
      "  0.23958269 -0.74144065 -0.22419733 -0.24736337]\n"
     ]
    }
   ],
   "source": [
    "# Access the vector representation of a word\n",
    "word_vector = model.wv['computer']\n",
    "print(f\"Vector for 'computer':\\n{word_vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b4a429aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most similar words to 'computer':\n",
      "visualisation: 0.6896\n",
      "graphics: 0.6778\n",
      "shopper: 0.6759\n",
      "eckton: 0.6675\n",
      "computing: 0.6632\n"
     ]
    }
   ],
   "source": [
    "# Find the most similar words to a given word\n",
    "similar_words = model.wv.most_similar('computer', topn=5)\n",
    "print(\"\\nMost similar words to 'computer':\")\n",
    "for word, similarity in similar_words:\n",
    "    print(f\"{word}: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "854f9ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarity between 'computer' and 'software': 0.5553\n"
     ]
    }
   ],
   "source": [
    "# Calculate similarity between two words\n",
    "similarity_score = model.wv.similarity('computer', 'software')\n",
    "print(f\"\\nSimilarity between 'computer' and 'software': {similarity_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57f10aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
