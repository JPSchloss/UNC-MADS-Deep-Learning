{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d08e910",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7a7322",
   "metadata": {},
   "source": [
    "## Text Representations\n",
    "\n",
    "In machine learning (ML) and neural networks, the data often needs to be numerical since models cannot usually work directly with raw text. Text is inherently complex, filled with semantics, context, and structure that computers cannot easily interpret. To train models effectively on textual data, we need systematic ways to convert text into numerical representations while preserving as much relevant information as possible. Without this transformation, machine learning algorithms would be unable to derive patterns or make predictions from text data.\n",
    "\n",
    "Text is composed of characters and words, which hold meaning for humans but are not directly understandable by machines. To perform tasks like classification, clustering, or generation, machine learning models need features, or numerical inputs, that capture relevant patterns in the text. \n",
    "\n",
    "Effective text representations help models understand and make predictions by encoding:\n",
    "- Frequency and presence of important words\n",
    "- Context and relationships between words\n",
    "- Semantic meaning beyond the surface structure\n",
    "\n",
    "The quality of these representations directly impact model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2d6cb8",
   "metadata": {},
   "source": [
    "## Different Approaches To Text Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e128fec5",
   "metadata": {},
   "source": [
    "![](https://aiml.com/wp-content/uploads/2023/02/disadvantage-bow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfb1b80",
   "metadata": {},
   "source": [
    "### Bag of Words (BoW)  \n",
    "\n",
    "The Bag of Words approach is one of the simplest and most widely used text representations. It involves creating a vocabulary from the text and representing each document as a vector indicating the presence or frequency of each word in the vocabulary.\n",
    "\n",
    "For example, consider two sentences:  \n",
    "- \"The cat sat on the mat.\"  \n",
    "- \"The dog sat on the mat.\"  \n",
    "\n",
    "The vocabulary from these sentences is: `['The', 'cat', 'dog', 'sat', 'on', 'mat']`. The first sentence can be represented as a vector: `[1, 1, 0, 1, 1, 1]`, where each entry corresponds to the count of a word from the vocabulary. \n",
    "\n",
    "While BoW is easy to implement, it has limitations. Primarily, it ignores word order and the context in which words appear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f328e77",
   "metadata": {},
   "source": [
    "#### Example Using CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c650d4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words Representation:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>at</th>\n",
       "      <th>ball</th>\n",
       "      <th>barked</th>\n",
       "      <th>cat</th>\n",
       "      <th>chased</th>\n",
       "      <th>dog</th>\n",
       "      <th>mat</th>\n",
       "      <th>mouse</th>\n",
       "      <th>on</th>\n",
       "      <th>sat</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   at  ball  barked  cat  chased  dog  mat  mouse  on  sat  the\n",
       "0   0     0       0    1       0    0    1      0   1    1    2\n",
       "1   1     0       1    1       0    1    0      0   0    0    2\n",
       "2   0     0       0    1       1    0    0      1   0    0    2\n",
       "3   0     1       0    0       1    1    0      0   0    0    2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\n",
    "    \"The cat sat on the mat\",\n",
    "    \"The dog barked at the cat\",\n",
    "    \"The cat chased the mouse\",\n",
    "    \"The dog chased the ball\"\n",
    "]\n",
    "\n",
    "# Initialize the CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the corpus into BoW representation\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Convert the result to a DataFrame for readability\n",
    "bow_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display the Bag of Words representation\n",
    "print(\"Bag of Words Representation:\")\n",
    "bow_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef3f640",
   "metadata": {},
   "source": [
    "### TF-IDF (Term Frequency-Inverse Document Frequency)  \n",
    "\n",
    "TF-IDF improves upon the Bag of Words by accounting for the importance of words within a corpus. Words that frequently appear across many documents (such as \"the\" or \"and\") are down-weighted, while rare but informative terms are given more significance. This makes TF-IDF useful for distinguishing between documents in tasks like document classification.\n",
    "\n",
    "Although TF-IDF captures relevance better than BoW, it still does not consider the order of words or their relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d52e9766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Representation:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>at</th>\n",
       "      <th>ball</th>\n",
       "      <th>barked</th>\n",
       "      <th>cat</th>\n",
       "      <th>chased</th>\n",
       "      <th>dog</th>\n",
       "      <th>mat</th>\n",
       "      <th>mouse</th>\n",
       "      <th>on</th>\n",
       "      <th>sat</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.301002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.471578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.471578</td>\n",
       "      <td>0.471578</td>\n",
       "      <td>0.492178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.492768</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.492768</td>\n",
       "      <td>0.314527</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.388504</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.514293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.361459</td>\n",
       "      <td>0.446473</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.566295</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.591032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.547794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.431887</td>\n",
       "      <td>0.431887</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.571724</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         at      ball    barked       cat    chased       dog       mat  \\\n",
       "0  0.000000  0.000000  0.000000  0.301002  0.000000  0.000000  0.471578   \n",
       "1  0.492768  0.000000  0.492768  0.314527  0.000000  0.388504  0.000000   \n",
       "2  0.000000  0.000000  0.000000  0.361459  0.446473  0.000000  0.000000   \n",
       "3  0.000000  0.547794  0.000000  0.000000  0.431887  0.431887  0.000000   \n",
       "\n",
       "      mouse        on       sat       the  \n",
       "0  0.000000  0.471578  0.471578  0.492178  \n",
       "1  0.000000  0.000000  0.000000  0.514293  \n",
       "2  0.566295  0.000000  0.000000  0.591032  \n",
       "3  0.000000  0.000000  0.000000  0.571724  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\n",
    "    \"The cat sat on the mat\",\n",
    "    \"The dog barked at the cat\",\n",
    "    \"The cat chased the mouse\",\n",
    "    \"The dog chased the ball\"\n",
    "]\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the corpus into TF-IDF representation\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Convert the result to a DataFrame for readability\n",
    "tfidf_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display the TF-IDF matrix\n",
    "print(\"TF-IDF Representation:\")\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dee961d",
   "metadata": {},
   "source": [
    "### Word Embeddings  \n",
    "\n",
    "Word embeddings provide dense vector representations of words, with each word mapped to a continuous vector space where semantically similar words have similar vectors. Popular methods for generating word embeddings include **Word2Vec**, **GloVe**, and **FastText**. These models learn from large corpora and capture relationships between words, such as analogies and semantic similarities.\n",
    "\n",
    "For instance, in a well-trained word embedding model, the relationship \"king - man + woman\" could approximate the vector for \"queen,\" showing how embeddings can encode relational knowledge. \n",
    "\n",
    "Word embeddings overcome many limitations of simpler methods like BoW by capturing semantic meaning and some degree of context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1083952",
   "metadata": {},
   "source": [
    "### Contextual Embeddings  \n",
    "\n",
    "Contextual embeddings take word representations a step further by making them dependent on the surrounding text. Models like **BERT**, **GPT**, and **RoBERTa** generate embeddings dynamically, meaning the same word can have different representations depending on its usage. This is particularly useful for disambiguating polysemous words (e.g., the word \"bank\" has different meanings in \"river bank\" and \"financial bank\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f2ba1e",
   "metadata": {},
   "source": [
    "### Character-Level Representations  \n",
    "\n",
    "Rather than representing words as whole units, some models use representations based on characters. This is particularly beneficial in cases where words are rare, misspelled, or morphologically complex. \n",
    "\n",
    "Character-level models can detect subword patterns and handle previously unseen words more effectively. Techniques like **Char-CNN** and **Char-RNN** implement this approach for languages with complex morphology.\n",
    "\n",
    "While character-level representations add robustness, they often require more computational resources and longer training times compared to word-based approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc33ba4",
   "metadata": {},
   "source": [
    "## Distance Functions\n",
    "\n",
    "When we represent text as vectors in an **n-dimensional space**, comparing the similarity between different text representations involves calculating the \"distance\" between them. \n",
    "\n",
    "These distances provide a quantitative measure of how similar (or different) two text samples are, which is crucial for tasks like document clustering, classification, and information retrieval.\n",
    "\n",
    "The choice of distance function can impact the performance of your model, as different tasks may require different types of similarity measures. \n",
    "\n",
    "![](https://www.maartengrootendorst.com/images/posts/2021-01-02-distances/header.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913b9012",
   "metadata": {},
   "source": [
    "### Euclidean Distance\n",
    "\n",
    "Euclidean distance is one of the most common and straightforward distance metrics. It measures the straight-line distance between two points in space. \n",
    "\n",
    "Given two vectors $ \\mathbf{A} $ and $ \\mathbf{B} $, the Euclidean distance $ d $ is defined as:\n",
    "\n",
    "$$\n",
    "d(\\mathbf{A}, \\mathbf{B}) = \\sqrt{\\sum_{i=1}^{n} (A_i - B_i)^2}\n",
    "$$\n",
    "\n",
    "Intuitively, this works well when comparing vectors based on magnitude or when word order does not matter. However, in high-dimensional spaces or sparse text data, Euclidean distance may not always capture meaningful similarities.\n",
    "\n",
    "#### Code Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc0ca349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean distance (manual): 3.605551275463989\n",
      "Euclidean distance (scipy): 3.605551275463989\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "# Define two vectors (representing text vectors or points in n-dimensional space)\n",
    "vector1 = np.array([1, 2, 3])\n",
    "vector2 = np.array([4, 0, 3])\n",
    "\n",
    "# Method 1: Manual computation of Euclidean distance\n",
    "euclidean_manual = np.sqrt(np.sum((vector1 - vector2)**2))\n",
    "print(f\"Euclidean distance (manual): {euclidean_manual}\")\n",
    "\n",
    "# Method 2: Using scipy's euclidean function\n",
    "euclidean_scipy = euclidean(vector1, vector2)\n",
    "print(f\"Euclidean distance (scipy): {euclidean_scipy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca40ab0",
   "metadata": {},
   "source": [
    "### Manhattan Distance (L1 Distance)\n",
    "\n",
    "Manhattan distance measures the distance by summing the absolute differences between the vector components. \n",
    "\n",
    "Mathematically, it is given by:\n",
    "\n",
    "$$\n",
    "d(\\mathbf{A}, \\mathbf{B}) = \\sum_{i=1}^{n} |A_i - B_i|\n",
    "$$\n",
    "\n",
    "This metric is useful when movement between dimensions is restricted. It is often used in text-related tasks where small deviations in feature values are meaningful.\n",
    "\n",
    "#### Code Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86f74630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manhattan distance (manual): 5\n",
      "Manhattan distance (scipy): 5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cityblock  # Scipy function for Manhattan distance\n",
    "\n",
    "# Define two vectors (representing text vectors or points in n-dimensional space)\n",
    "vector1 = np.array([1, 2, 3])\n",
    "vector2 = np.array([4, 0, 3])\n",
    "\n",
    "# Method 1: Manual computation of Manhattan distance\n",
    "manhattan_manual = np.sum(np.abs(vector1 - vector2))\n",
    "print(f\"Manhattan distance (manual): {manhattan_manual}\")\n",
    "\n",
    "# Method 2: Using scipy's cityblock function\n",
    "manhattan_scipy = cityblock(vector1, vector2)\n",
    "print(f\"Manhattan distance (scipy): {manhattan_scipy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db436ce0",
   "metadata": {},
   "source": [
    "### Minkowski Distance\n",
    "\n",
    "The **Minkowski distance** is a generalized form of distance that includes both the **Euclidean distance** and **Manhattan distance** as special cases. The formula for the Minkowski distance between two vectors $ \\mathbf{A} $ and $ \\mathbf{B} $ is:\n",
    "\n",
    "$$\n",
    "d(\\mathbf{A}, \\mathbf{B}) = \\left( \\sum_{i=1}^{n} |A_i - B_i|^p \\right)^{\\frac{1}{p}}\n",
    "$$\n",
    "\n",
    "- **p = 1:** The formula becomes the **Manhattan distance**.  \n",
    "- **p = 2:** The formula becomes the **Euclidean distance**.  \n",
    "- For larger values of $ p $, the metric places more emphasis on large differences between vector components.  \n",
    "\n",
    "#### Code Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aab4f507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minkowski distance (manual, p=3): 3.2710663101885897\n",
      "Minkowski distance (scipy, p=3): 3.2710663101885897\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import minkowski\n",
    "\n",
    "# Define two vectors (representing text vectors or points in n-dimensional space)\n",
    "vector1 = np.array([1, 2, 3])\n",
    "vector2 = np.array([4, 0, 3])\n",
    "\n",
    "# Method 1: Manual computation of Minkowski distance with p = 3\n",
    "p = 3\n",
    "minkowski_manual = np.power(np.sum(np.abs(vector1 - vector2)**p), 1/p)\n",
    "print(f\"Minkowski distance (manual, p=3): {minkowski_manual}\")\n",
    "\n",
    "# Method 2: Using scipy's minkowski function\n",
    "minkowski_scipy = minkowski(vector1, vector2, p=3)\n",
    "print(f\"Minkowski distance (scipy, p=3): {minkowski_scipy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb66bfc",
   "metadata": {},
   "source": [
    "### Cosine Similarity\n",
    "\n",
    "Cosine similarity is a popular measure for text representations because it compares the **angles between vectors** rather than their magnitudes. This makes it ideal for text data, where two documents with different lengths may still have high similarity if they share the same word distribution.\n",
    "\n",
    "The formula for cosine similarity between two vectors $ \\mathbf{A} $ and $ \\mathbf{B} $ is:\n",
    "\n",
    "$$\n",
    "\\text{cosine similarity}(\\mathbf{A}, \\mathbf{B}) = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\| \\|\\mathbf{B}\\|}\n",
    "$$\n",
    "\n",
    "Here, $ \\mathbf{A} \\cdot \\mathbf{B} $ represents the dot product of the two vectors, and $ \\|\\mathbf{A}\\| $ and $ \\|\\mathbf{B}\\| $ represent their magnitudes.\n",
    "\n",
    "Cosine similarity is widely used in **information retrieval** and **document clustering** because it focuses on the direction of vectors, making it less sensitive to differences in vector length.\n",
    "\n",
    "#### Code Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d4cbeb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity (manual): 0.6948792289723034\n",
      "Cosine similarity (scikit-learn): 0.6948792289723034\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Define two vectors (representing text vectors or points in n-dimensional space)\n",
    "vector1 = np.array([1, 2, 3])\n",
    "vector2 = np.array([4, 0, 3])\n",
    "\n",
    "# Method 1: Manual computation of cosine similarity\n",
    "dot_product = np.dot(vector1, vector2)\n",
    "magnitude1 = np.linalg.norm(vector1)\n",
    "magnitude2 = np.linalg.norm(vector2)\n",
    "cosine_similarity_manual = dot_product / (magnitude1 * magnitude2)\n",
    "print(f\"Cosine similarity (manual): {cosine_similarity_manual}\")\n",
    "\n",
    "# Method 2: Using scikit-learn's cosine_similarity function\n",
    "cosine_similarity_sklearn = cosine_similarity([vector1], [vector2])[0][0]\n",
    "print(f\"Cosine similarity (scikit-learn): {cosine_similarity_sklearn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb9380d",
   "metadata": {},
   "source": [
    "### Other Distance Measures\n",
    "\n",
    "There are additional distance measures you may encounter depending on the specific problem:\n",
    "\n",
    "1. **Jaccard Similarity:** Measures the similarity between two sets by dividing the size of their intersection by the size of their union. This is particularly useful for comparing sparse vectors or binary representations of text.\n",
    "   $$\n",
    "   J(A, B) = \\frac{|A \\cap B|}{|A \\cup B|}\n",
    "   $$\n",
    "\n",
    "2. **Hamming Distance:** Measures the number of positions where corresponding components of two vectors differ. It is commonly used for comparing binary or categorical data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa6c8c2",
   "metadata": {},
   "source": [
    "## Word Embeddings  \n",
    "\n",
    "Word embeddings are a type of dense vector representation where words with similar meanings have similar vector representations. Instead of treating words as discrete entities, embeddings place them in a continuous vector space, allowing models to capture semantic relationships and contextual similarity. \n",
    "\n",
    "Word embeddings are typically generated by training on large corpora of text. The model learns to place words in the vector space such that words that appear in similar contexts have similar vector representations. The training process focuses on capturing semantic relationships and dependencies between words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b977d6",
   "metadata": {},
   "source": [
    "## Word2Vec  \n",
    "\n",
    "Word2Vec is a popular algorithm for generating word embeddings. Word2Vec uses neural networks to map words into a continuous vector space based on their context in a corpus of text. The key idea is that words appearing in similar contexts will have similar vector representations.  \n",
    "\n",
    "The training objective of Word2Vec is to predict either the surrounding words given a target word (CBOW model) or predict the target word given its surrounding words (Skip-Gram model). Word2Vec can be trained efficiently even on large datasets and produces embeddings that capture both syntactic and semantic properties of words.  \n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:1200/1*xC6wfTU_zpUlpRlXs5NZ4w.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e167d1c",
   "metadata": {},
   "source": [
    "### Skip-Gram Model  \n",
    "\n",
    "The Skip-Gram model, a core component of Word2Vec, predicts the surrounding context words given a target word. For example, given the word \"cat,\" the model will try to predict nearby words like \"the,\" \"sat,\" or \"mat.\"  \n",
    "\n",
    "Mathematically, the model maximizes the probability of observing a context word given a target word by minimizing the following objective function:  \n",
    "\n",
    "$$\n",
    "\\text{maximize} \\, \\sum \\log P(\\text{context} \\mid \\text{target})\n",
    "$$\n",
    "\n",
    "Skip-Gram is particularly useful for capturing meaningful relationships when the corpus is large, as it can effectively handle rare words. One way to understand Skip-Gram intuitively is to visualize a sliding window moving over the text and predicting the words within the window."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece7872f",
   "metadata": {},
   "source": [
    "### Continuous Bag of Words (CBOW)  \n",
    "\n",
    "CBOW is the opposite of the Skip-Gram model. Instead of predicting context words given a target word, it predicts the target word based on its surrounding context. Given a window of words surrounding a target word, the model aims to predict the target word by maximizing the following objective function:  \n",
    "\n",
    "$$\n",
    "\\text{maximize} \\, \\sum \\log P(\\text{target} \\mid \\text{context})\n",
    "$$  \n",
    "\n",
    "CBOW is generally faster to train than Skip-Gram because it averages the context word vectors to predict the target word. However, it may not perform as well on rare words since it relies heavily on the surrounding context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c55b77",
   "metadata": {},
   "source": [
    "### Coding Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca910054",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f8ab491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'cat':\n",
      "[-0.017551    0.00739897  0.01054541  0.01177877  0.0147996  -0.01261895\n",
      "  0.00260913  0.01238323 -0.00613262 -0.01253047 -0.00066736 -0.01696945\n",
      " -0.01137851  0.01422109  0.00669504  0.01454292  0.01396383  0.01539372\n",
      " -0.00807038 -0.00138335  0.00487858 -0.00894551  0.01735162 -0.01978637\n",
      "  0.01381388  0.00584623 -0.00989498  0.00910192 -0.00375495  0.01340912\n",
      "  0.02001455 -0.00904609 -0.00094419 -0.01190493  0.0076113   0.00550929\n",
      "  0.0142548   0.01216169  0.01925049  0.01866803  0.01587015 -0.01396381\n",
      " -0.01876468 -0.00090628 -0.00592141  0.01605945  0.01181045 -0.00301812\n",
      "  0.00315656  0.00381467]\n",
      "\n",
      "Most similar words to 'cat':\n",
      "on: 0.17365525662899017\n",
      "mat: 0.17027075588703156\n",
      "the: 0.16929367184638977\n",
      "\n",
      "Similarity between 'cat' and 'dog': 0.04\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Sample corpus (list of tokenized sentences)\n",
    "corpus = [\n",
    "    [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"],\n",
    "    [\"the\", \"dog\", \"barked\", \"at\", \"the\", \"cat\"],\n",
    "    [\"the\", \"dog\", \"chased\", \"the\", \"ball\"],\n",
    "    [\"the\", \"cat\", \"chased\", \"the\", \"mouse\"],\n",
    "    [\"the\", \"dog\", \"sat\", \"on\", \"the\", \"sofa\"]\n",
    "]\n",
    "\n",
    "# Train Word2Vec model - Skip-Gram Model\n",
    "model = Word2Vec(sentences=corpus, vector_size=50, window=3, min_count=1, sg=1, epochs=100)\n",
    "\n",
    "# Train Word2Vec model - CBOW Model\n",
    "# model = Word2Vec(sentences=corpus, vector_size=50, window=3, min_count=1, sg=0, epochs=100)\n",
    "\n",
    "# Save the trained model\n",
    "# model.save(\"word2vec.model\")\n",
    "\n",
    "# Access vector for a word\n",
    "cat_vector = model.wv[\"cat\"]\n",
    "print(f\"Vector for 'cat':\\n{cat_vector}\")\n",
    "\n",
    "# Find the most similar words to 'cat'\n",
    "similar_words = model.wv.most_similar(\"cat\", topn=3)\n",
    "print(\"\\nMost similar words to 'cat':\")\n",
    "for word, score in similar_words:\n",
    "    print(f\"{word}: {score}\")\n",
    "\n",
    "# Find similarity between two words\n",
    "similarity_score = model.wv.similarity(\"cat\", \"dog\")\n",
    "print(f\"\\nSimilarity between 'cat' and 'dog': {similarity_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ed26d7",
   "metadata": {},
   "source": [
    "#### Explanation of Parameters:\n",
    "\n",
    "- **`sentences`**: The corpus of tokenized sentences used for training.  \n",
    "- **`vector_size=50`**: The size (dimensionality) of the word embeddings.  \n",
    "- **`window=3`**: The context window size, meaning the model looks at 3 words before and after the target word.  \n",
    "- **`min_count=1`**: Minimum number of occurrences for a word to be included in the vocabulary.  \n",
    "- **`sg=1`**: Skip-Gram model (set `sg=0` for CBOW).  \n",
    "- **`epochs=100`**: Number of training epochs (iterations over the corpus).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c3d5b3",
   "metadata": {},
   "source": [
    "## Alternatives to Word2Vec  \n",
    "\n",
    "Although Word2Vec is widely used, several alternative methods exist:  \n",
    "\n",
    "- **GloVe (Global Vectors for Word Representation):** GloVe focuses on co-occurrence statistics across the entire corpus rather than relying on local context windows. It constructs a co-occurrence matrix and factorizes it to generate word embeddings. GloVe works well for capturing global context and semantic relationships.  \n",
    "- **FastText:** Developed by Facebook, FastText extends Word2Vec by incorporating subword information. It breaks words into character n-grams, making it better at handling rare words and morphologically rich languages.  \n",
    "- **BERT and Contextual Embeddings:** Unlike static embeddings like Word2Vec and GloVe, contextual embeddings (e.g., BERT, GPT) generate dynamic representations based on the context in which a word appears. These embeddings are ideal for tasks where word meaning depends on surrounding words.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d618889",
   "metadata": {},
   "source": [
    "## Important Things to Consider  \n",
    "\n",
    "#### Window Size  \n",
    "The window size defines the number of words around a target word that the model should consider when learning embeddings. A shorter window (e.g., 2-3 words) typically captures **syntactic** information, such as word order or grammatical relationships. A longer window (e.g., 5-10 words) is better for capturing **semantic or topical relationships** between words.  \n",
    "\n",
    "For example:  \n",
    "- A shorter window size may learn that \"run\" and \"ran\" have similar grammatical functions. \n",
    "- A longer window size may capture that \"book\" and \"novel\" are topically related, even if they are not grammatically similar.  \n",
    "  \n",
    "#### Subword Information  \n",
    "Languages with complex morphology or compound words benefit from embeddings that capture subword information. Techniques like FastText and character-based embeddings decompose words into smaller units, enabling models to handle unseen or rare words more effectively.  \n",
    "#### Handling Out-of-Vocabulary (OOV) Words  \n",
    "Static embeddings, such as Word2Vec or GloVe, suffer from the issue of out-of-vocabulary (OOV) words. If a word is not in the training corpus, it will not have an embedding. Contextual embeddings and subword-based models provide solutions to this problem by dynamically generating embeddings or breaking words into subunits.  \n",
    "\n",
    "#### Semantic and Syntactic Trade-offs  \n",
    "Depending on the task, you may need embeddings that prioritize semantic relationships (e.g., grouping words by topic) or syntactic relationships (e.g., grammatical roles). Tuning parameters like window size, corpus size, and embedding dimensions allows for optimizing the model based on your needs.\n",
    "\n",
    "#### Dimensionality and Training Time  \n",
    "Choosing the dimensionality of the word vectors is important. Larger dimensions generally result in better performance but increase computational cost and risk overfitting. Practical implementations balance the quality of embeddings with the available computational resources.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31481b6e",
   "metadata": {},
   "source": [
    "## Real World Example Using Newsgroup Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6514f15c",
   "metadata": {},
   "source": [
    "#### Install Required Libraries (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54609e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim scikit-learn nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b17c6a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b0452c",
   "metadata": {},
   "source": [
    "#### Load The Dataset\n",
    "\n",
    "We are using the 20 Newsgroups dataset, a common benchmark dataset in NLP. It contains around 18,000 newsgroup documents spread across 20 different categories, including topics like sports, politics, computers, and religion. This dataset is widely used for text classification, clustering, and word embedding demonstrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9182e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/jonathanschlosser/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "# Download NLTK tokenizer\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Load the 20 Newsgroups dataset\n",
    "newsgroups_data = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047d2a98",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96b83be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the documents into sentences of words\n",
    "sentences = [word_tokenize(doc.lower()) for doc in newsgroups_data.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2053c3b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'am',\n",
       " 'sure',\n",
       " 'some',\n",
       " 'bashers',\n",
       " 'of',\n",
       " 'pens',\n",
       " 'fans',\n",
       " 'are',\n",
       " 'pretty',\n",
       " 'confused',\n",
       " 'about',\n",
       " 'the',\n",
       " 'lack',\n",
       " 'of',\n",
       " 'any',\n",
       " 'kind',\n",
       " 'of',\n",
       " 'posts',\n",
       " 'about',\n",
       " 'the',\n",
       " 'recent',\n",
       " 'pens',\n",
       " 'massacre',\n",
       " 'of',\n",
       " 'the',\n",
       " 'devils',\n",
       " '.',\n",
       " 'actually',\n",
       " ',',\n",
       " 'i',\n",
       " 'am',\n",
       " 'bit',\n",
       " 'puzzled',\n",
       " 'too',\n",
       " 'and',\n",
       " 'a',\n",
       " 'bit',\n",
       " 'relieved',\n",
       " '.',\n",
       " 'however',\n",
       " ',',\n",
       " 'i',\n",
       " 'am',\n",
       " 'going',\n",
       " 'to',\n",
       " 'put',\n",
       " 'an',\n",
       " 'end',\n",
       " 'to',\n",
       " 'non-pittsburghers',\n",
       " \"'\",\n",
       " 'relief',\n",
       " 'with',\n",
       " 'a',\n",
       " 'bit',\n",
       " 'of',\n",
       " 'praise',\n",
       " 'for',\n",
       " 'the',\n",
       " 'pens',\n",
       " '.',\n",
       " 'man',\n",
       " ',',\n",
       " 'they',\n",
       " 'are',\n",
       " 'killing',\n",
       " 'those',\n",
       " 'devils',\n",
       " 'worse',\n",
       " 'than',\n",
       " 'i',\n",
       " 'thought',\n",
       " '.',\n",
       " 'jagr',\n",
       " 'just',\n",
       " 'showed',\n",
       " 'you',\n",
       " 'why',\n",
       " 'he',\n",
       " 'is',\n",
       " 'much',\n",
       " 'better',\n",
       " 'than',\n",
       " 'his',\n",
       " 'regular',\n",
       " 'season',\n",
       " 'stats',\n",
       " '.',\n",
       " 'he',\n",
       " 'is',\n",
       " 'also',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'fo',\n",
       " 'fun',\n",
       " 'to',\n",
       " 'watch',\n",
       " 'in',\n",
       " 'the',\n",
       " 'playoffs',\n",
       " '.',\n",
       " 'bowman',\n",
       " 'should',\n",
       " 'let',\n",
       " 'jagr',\n",
       " 'have',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'of',\n",
       " 'fun',\n",
       " 'in',\n",
       " 'the',\n",
       " 'next',\n",
       " 'couple',\n",
       " 'of',\n",
       " 'games',\n",
       " 'since',\n",
       " 'the',\n",
       " 'pens',\n",
       " 'are',\n",
       " 'going',\n",
       " 'to',\n",
       " 'beat',\n",
       " 'the',\n",
       " 'pulp',\n",
       " 'out',\n",
       " 'of',\n",
       " 'jersey',\n",
       " 'anyway',\n",
       " '.',\n",
       " 'i',\n",
       " 'was',\n",
       " 'very',\n",
       " 'disappointed',\n",
       " 'not',\n",
       " 'to',\n",
       " 'see',\n",
       " 'the',\n",
       " 'islanders',\n",
       " 'lose',\n",
       " 'the',\n",
       " 'final',\n",
       " 'regular',\n",
       " 'season',\n",
       " 'game',\n",
       " '.',\n",
       " 'pens',\n",
       " 'rule',\n",
       " '!',\n",
       " '!',\n",
       " '!']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271847e6",
   "metadata": {},
   "source": [
    "#### Train Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "43bf8450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Word2Vec model on the tokenized sentences\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=5, sg=1, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88721641",
   "metadata": {},
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1bd4c22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'computer':\n",
      "[-0.21225789  0.5404231   0.40459347  0.32228127  0.12945211 -0.3299833\n",
      " -0.09653363  0.20713666 -0.36554763  0.13997215 -0.37456098 -0.67297906\n",
      " -0.3721857   0.00911829 -0.20481554 -0.36107153  0.7215204   0.3608223\n",
      " -0.41200316 -0.7169939   0.38293985  0.86623466  0.09137827 -0.26366988\n",
      "  0.23594125 -0.17807429 -0.17314552 -0.07365485  0.2280913   0.200225\n",
      "  0.03429207 -0.2574355   0.28391963 -0.32798564 -0.03226035 -0.5596238\n",
      " -0.12443216  0.20995711  0.05276503 -0.3246789   0.9564842  -0.6072698\n",
      "  0.62925166  0.00243073  0.15336983  0.07485364 -0.31112248 -0.06012597\n",
      " -0.04015968  0.29611218 -0.20674986 -0.33931953 -0.40383413 -0.0755101\n",
      "  0.12370312  0.03053324 -0.13557634 -0.25636342 -0.26846898  0.263858\n",
      " -0.21728204  0.4428228   0.11869055  0.20396456 -0.00572924  0.39625686\n",
      " -0.02928978  0.41261107  0.07562852  0.1514651   0.10060371 -0.1340094\n",
      "  0.09531747 -0.27664697  0.58087593 -0.3628328  -0.00305403  0.30699235\n",
      " -0.08109504  0.03020633 -0.24945392  0.23120221 -0.13801555 -0.02945856\n",
      " -0.2450922  -0.3351114   0.04982343  0.5667882  -0.293193    0.30734646\n",
      "  0.33727098 -0.13578098  0.35929447  0.0221448  -0.13967526 -0.31036037\n",
      "  0.23958269 -0.74144065 -0.22419733 -0.24736337]\n"
     ]
    }
   ],
   "source": [
    "# Access the vector representation of a word\n",
    "word_vector = model.wv['computer']\n",
    "print(f\"Vector for 'computer':\\n{word_vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d224e68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most similar words to 'computer':\n",
      "visualisation: 0.6896\n",
      "graphics: 0.6778\n",
      "shopper: 0.6759\n",
      "eckton: 0.6675\n",
      "computing: 0.6632\n"
     ]
    }
   ],
   "source": [
    "# Find the most similar words to a given word\n",
    "similar_words = model.wv.most_similar('computer', topn=5)\n",
    "print(\"\\nMost similar words to 'computer':\")\n",
    "for word, similarity in similar_words:\n",
    "    print(f\"{word}: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb3f48c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarity between 'computer' and 'software': 0.5553\n"
     ]
    }
   ],
   "source": [
    "# Calculate similarity between two words\n",
    "similarity_score = model.wv.similarity('computer', 'software')\n",
    "print(f\"\\nSimilarity between 'computer' and 'software': {similarity_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cadc810",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
